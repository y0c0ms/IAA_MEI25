# Introdu√ß√£o √† Aprendizagem Autom√°tica - MEI 2025

## Sobre este Reposit√≥rio

Este reposit√≥rio foi criado para documentar o trabalho desenvolvido ao longo da unidade curricular de **Introdu√ß√£o √† Aprendizagem Autom√°tica** (IAA) do Mestrado em Engenharia Inform√°tica, no ano letivo 2025/2026.

## Autores

- **Alexandre Mendes** - N¬∫ 111026
- **Manuel Santos** - N¬∫ 111087

## Descri√ß√£o da Cadeira

A unidade curricular de Introdu√ß√£o √† Aprendizagem Autom√°tica aborda os conceitos fundamentais e t√©cnicas essenciais da √°rea de Machine Learning, explorando tanto m√©todos supervisionados como n√£o supervisionados. Os alunos s√£o expostos a diferentes algoritmos e ferramentas que permitem desenvolver solu√ß√µes inteligentes para problemas reais.

### Objetivos de Aprendizagem

Ao longo desta cadeira, pretendemos:
- Compreender os conceitos base de aprendizagem autom√°tica
- Aplicar algoritmos de classifica√ß√£o e regress√£o
- Explorar t√©cnicas de clustering e redu√ß√£o de dimensionalidade
- Desenvolver compet√™ncias pr√°ticas em Python e bibliotecas como scikit-learn
- Avaliar e otimizar modelos de machine learning

## Estrutura do Reposit√≥rio

O reposit√≥rio est√° organizado da seguinte forma:

```
IAA_MEI25/
‚îú‚îÄ‚îÄ Exercicio1/                                                    # Clustering N√£o Supervisionado
‚îÇ   ‚îú‚îÄ‚îÄ Gera√ß√£o_e_Visualiza√ß√£o_de_Distribui√ß√µes_Gaussianas.ipynb # Dataset: 2 distribui√ß√µes Gaussianas
‚îÇ   ‚îú‚îÄ‚îÄ Implementa√ß√£o_K-Means.ipynb                               # K-Means Online e Batch
‚îÇ   ‚îú‚îÄ‚îÄ Clustering_Hier√°rquico_Aglomerativo.ipynb                # Hier√°rquico Bottom-Up
‚îÇ   ‚îú‚îÄ‚îÄ DBSCAN_Clustering.ipynb                                   # DBSCAN com Densidade
‚îÇ   ‚îî‚îÄ‚îÄ dados_gaussianos.csv                                       # Dataset gerado (1000 pontos)
‚îú‚îÄ‚îÄ Exercicio2/                                                    # (a ser adicionado)
‚îú‚îÄ‚îÄ Exercicio3/                                                    # (a ser adicionado)
‚îî‚îÄ‚îÄ README.md
```

## Exerc√≠cio 1: Clustering N√£o Supervisionado

O primeiro exerc√≠cio pr√°tico foca-se em **algoritmos de clustering n√£o supervisionado**, implementando do zero tr√™s diferentes abordagens para descobrir estruturas em dados sem etiquetas pr√©-definidas. Este trabalho demonstra como algoritmos conseguem aprender a distinguir pontos gerados por diferentes distribui√ß√µes sem informa√ß√£o pr√©via sobre os grupos.

### üìä Dataset: Distribui√ß√µes Gaussianas Multivariadas

**Notebook:** `Gera√ß√£o_e_Visualiza√ß√£o_de_Distribui√ß√µes_Gaussianas.ipynb`

Gera√ß√£o de um dataset sint√©tico com as seguintes caracter√≠sticas:
- **1000 pontos** divididos em 2 conjuntos de 500 pontos cada
- **Conjunto 1**: Distribui√ß√£o Gaussiana com m√©dia [3, 3] e covari√¢ncia [[1, 0], [0, 1]]
- **Conjunto 2**: Distribui√ß√£o Gaussiana com m√©dia [-3, -3] e covari√¢ncia [[2, 0], [0, 5]]
- Centros diferentes para criar separa√ß√£o clara
- Vari√¢ncias diferentes para simular sobreposi√ß√£o realista
- Dados baralhados e salvos em `dados_gaussianos.csv`

**Caracter√≠sticas implementadas:**
- Gera√ß√£o de distribui√ß√µes Gaussianas multivariadas com NumPy
- Adi√ß√£o de etiquetas (labels) para valida√ß√£o posterior
- Visualiza√ß√£o com separa√ß√£o por cores
- An√°lise estat√≠stica das distribui√ß√µes geradas
- Discuss√£o sobre adequa√ß√£o do dataset para clustering

---

### üéØ Al√≠nea A: K-Means com Atualiza√ß√£o Incremental (Online)

**Notebook:** `Implementa√ß√£o_K-Means.ipynb`

Implementa√ß√£o de uma vers√£o simplificada do **K-Means** com atualiza√ß√£o incremental, onde cada ponto atualiza imediatamente o centroide mais pr√≥ximo.

**Algoritmo Implementado:**
```
Para cada √©poca:
    Para cada ponto x no dataset:
        Se x est√° mais pr√≥ximo de r1:
            r1 ‚Üê (1 - Œ±) √ó r1 + Œ± √ó x
        Sen√£o se x est√° mais pr√≥ximo de r2:
            r2 ‚Üê (1 - Œ±) √ó r2 + Œ± √ó x
```

**Experimentos Realizados:**

1. **Configura√ß√£o inicial:** Œ± = 10E-5, 10 √©pocas
2. **Varia√ß√£o de Œ±:** Testes com Œ± = {10E-5, 0.01, 0.1}
3. **Varia√ß√£o de √©pocas:** Diferentes n√∫meros de itera√ß√µes
4. **K-Means Batch:** Implementa√ß√£o alternativa com atualiza√ß√£o acumulada

**Visualiza√ß√µes:**
- Trajet√≥rias completas dos representantes (todas as atualiza√ß√µes)
- Posi√ß√µes no fim de cada √©poca
- Compara√ß√£o Online vs Batch
- An√°lise por etiquetas (4 grupos: r1/r2 √ó label1/label2)
- 30 repeti√ß√µes para an√°lise de estabilidade

**Resultados e Discuss√£o:**
- Efeito do par√¢metro Œ± na velocidade de converg√™ncia
- Rela√ß√£o entre representantes finais e centros originais [3,3] e [-3,-3]
- Compara√ß√£o de acur√°cia entre m√©todos
- An√°lise da variabilidade com m√∫ltiplas inicializa√ß√µes

---

### üå≥ Al√≠nea B: Clustering Hier√°rquico Aglomerativo

**Notebook:** `Clustering_Hier√°rquico_Aglomerativo.ipynb`

Implementa√ß√£o simplificada de **clustering hier√°rquico aglomerativo** que funde iterativamente os pontos mais pr√≥ximos.

**Algoritmo Implementado:**
```
Enquanto houver mais de 2 pontos:
    Encontrar os dois pontos mais pr√≥ximos
    Substituir ambos pela sua m√©dia
```

**Caracter√≠sticas:**
- Processo determin√≠stico (sem aleatoriedade)
- Constru√ß√£o bottom-up (dos pontos individuais at√© 2 clusters)
- Armazenamento completo da hierarquia de fus√µes
- Registro de dist√¢ncias em cada fus√£o

**Visualiza√ß√µes:**
- Evolu√ß√£o do processo em 6 snapshots
- Gr√°fico da evolu√ß√£o das dist√¢ncias m√≠nimas
- Redu√ß√£o do n√∫mero de clusters ao longo do tempo
- Dendrograma simplificado (√∫ltimas 20 fus√µes)
- Compara√ß√£o lado a lado com K-Means

**An√°lises:**
- Matriz de confus√£o comparativa
- C√°lculo de acur√°cia vs etiquetas verdadeiras
- Visualiza√ß√£o de erros e acertos
- Compara√ß√£o de centroides finais com centros originais

**Discuss√£o:**
- Vantagens: Determin√≠stico, fornece hierarquia completa
- Desvantagens: Complexidade O(n¬≤), decis√µes irrevog√°veis
- Casos de uso apropriados

---

### üîç Al√≠nea C: DBSCAN - Clustering Baseado em Densidade

**Notebook:** `DBSCAN_Clustering.ipynb`

Implementa√ß√£o completa do **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise), que identifica clusters como regi√µes densas de pontos.

**Conceitos Fundamentais:**

- **Epsilon (Œµ):** Raio de busca ao redor de cada ponto (epsilon ball)
- **MinPoints:** N√∫mero m√≠nimo de vizinhos para formar um cluster
- **Core Point:** Ponto com ‚â• minPts vizinhos dentro de Œµ
- **Border Point:** Dentro do Œµ de um core point, mas sem minPts vizinhos
- **Noise Point:** N√£o pertence a nenhum cluster

**Algoritmo Implementado:**
```
Para cada ponto n√£o visitado:
    Se √© core point:
        Criar novo cluster
        Expandir recursivamente adicionando vizinhos
    Sen√£o:
        Marcar como ru√≠do (pode ser reclassificado depois)
```

**Experimentos:**
- Configura√ß√£o base: Œµ = 0.5, minPts = 5
- Teste com 6 combina√ß√µes diferentes de par√¢metros
- An√°lise do efeito de Œµ (pequeno, m√©dio, grande)
- An√°lise do efeito de minPts (3, 5, 10)

**Visualiza√ß√µes Detalhadas:**

1. **Epsilon Balls:** C√≠rculos de raio Œµ mostrando core points vs non-core
2. **Snapshots do processo:**
   - In√≠cio do primeiro cluster (ponto inicial verde)
   - Expans√£o com core point (pontos azuis)
   - Adi√ß√£o de border points (pontos amarelos)
   - Cluster finalizado
   - Detec√ß√£o de pontos de ru√≠do (cruzes pretas)
3. **Compara√ß√£o tripla:** DBSCAN vs K-Means vs Etiquetas Verdadeiras

**Resultados:**
- Identifica√ß√£o autom√°tica de clusters
- Detec√ß√£o de outliers/ru√≠do
- Clusters de forma arbitr√°ria (n√£o apenas esf√©ricos)
- Tabela comparativa dos tr√™s algoritmos

---

### üìà Compara√ß√£o dos Tr√™s Algoritmos (Resultados Obtidos)

| Caracter√≠stica | K-Means | Hier√°rquico | DBSCAN |
|---------------|---------|-------------|---------|
| **Acur√°cia** | 92.00% | **99.50%** ‚úì | 98%+ |
| **Centroides Finais** | r1:[2.99,4.00]<br>r2:[-2.84,-3.24] | [2.29,4.01]<br>[-2.85,-3.24] | N/A |
| **Erro vs Real** | M√©dio | Baixo | N/A |
| **Forma dos clusters** | Esf√©rica | Arbitr√°ria | Arbitr√°ria |
| **N¬∫ de clusters** | Deve especificar k=2 | Flex√≠vel | Autom√°tico |
| **Identifica ru√≠do** | N√£o | N√£o | **Sim** ‚úì |
| **Determin√≠stico** | N√£o (œÉ‚âà3.0) | **Sim** ‚úì | Quase* |
| **Complexidade** | O(n¬∑k¬∑i) | O(n¬≤) | O(n log n) |
| **Tempo Execu√ß√£o** | **R√°pido** ‚úì | Lento (998 iter) | M√©dio |
| **Par√¢metros** | k, Œ±, √©pocas | Nenhum‚Ä† | Œµ, minPts |
| **Vantagens** | R√°pido, simples | **Determin√≠stico<br>Hierarquia completa<br>Melhor acur√°cia** | Detecta ru√≠do<br>Formas complexas |
| **Desvantagens** | S√≥ esferas<br>Precisa k<br>Variabilidade alta | Lento O(n¬≤)<br>Decis√µes permanentes | Sens√≠vel a par√¢metros |
| **Pontos Mal Classificados** | 80 (8.0%) | **5 (0.5%)** ‚úì | ~20 (2.0%) |
| **Repeti√ß√µes Necess√°rias** | 30+ | **1** ‚úì | 1 |

*DBSCAN: Border points podem ser atribu√≠dos arbitrariamente  
‚Ä†Hier√°rquico: Pode escolher n√∫mero de clusters cortando a √°rvore

**Conclus√£o dos Testes:** Para este dataset de 1000 pontos com 2 distribui√ß√µes Gaussianas:
- **Hier√°rquico**: Melhor acur√°cia (99.50%) e determinismo, ideal quando tempo n√£o √© cr√≠tico
- **K-Means**: Melhor velocidade mas menor acur√°cia (92.00%), ideal para prototipagem r√°pida
- **DBSCAN**: √önico a identificar outliers, ideal quando ru√≠do √© esperado

---

### üéì Aprendizagens e Conclus√µes (Baseadas em Resultados Reais)

1. **K-Means Online/Batch:**
   - Taxa de aprendizagem Œ± cr√≠tica para converg√™ncia
     - Œ± = 10E-5: Converg√™ncia muito lenta mas est√°vel
     - Œ± = 0.01: Equil√≠brio ideal entre velocidade e estabilidade
     - Œ± = 0.1: Converg√™ncia r√°pida mas com oscila√ß√µes
   - M√©todo batch consistentemente mais est√°vel que online
   - Representantes convergem para centros pr√≥ximos dos verdadeiros [3,3] e [-3,-3]
   - **Resultados obtidos (30 repeti√ß√µes, Œ±=0.01):**
     - r1 m√©dio: [2.996, 3.442] (centro real: [3, 3])
     - r2 m√©dio: [-0.627, -0.616] (centro real: [-3, -3])
     - Desvio padr√£o significativo (‚âà3.0) mostra sensibilidade √† inicializa√ß√£o
   - Acur√°cia t√≠pica: **92.00%** (920/1000 pontos corretamente classificados)

2. **Clustering Hier√°rquico:**
   - Fornece vis√£o completa da estrutura de dados atrav√©s de 998 fus√µes
   - √ötil quando n√∫mero de clusters √© incerto
   - Dendrograma revela rela√ß√µes hier√°rquicas progressivas
   - Decis√µes de fus√£o s√£o permanentes (n√£o h√° backtracking)
   - **Resultados obtidos:**
     - Cluster 1: [2.295, 4.013] (centro real: [3, 3])
     - Cluster 2: [-2.845, -3.239] (centro real: [-3, -3])
     - Acur√°cia: **99.50%** (995/1000 pontos)
     - Apenas 5 pontos mal classificados na regi√£o de sobreposi√ß√£o
   - Dist√¢ncias de fus√£o aumentam gradualmente, com salto final ao unir os 2 clusters principais
   - **Mais determin√≠stico e preciso que K-Means neste dataset**

3. **DBSCAN:**
   - Excelente para detectar outliers automaticamente
   - N√£o for√ßa pontos amb√≠guos em clusters (marca como ru√≠do)
   - Œµ e minPts devem ser ajustados ao dataset
     - Œµ = 0.5: Demasiado restritivo, fragmenta clusters
     - Œµ = 1.5: Identifica corretamente 2 clusters principais
   - Ideal para clusters de densidade e forma irregular
   - **Resultados obtidos (Œµ=1.5, minPts=5):**
     - Identificou corretamente 2 clusters principais
     - Detetou pontos de ru√≠do na regi√£o de transi√ß√£o
     - Mais robusto a outliers que K-Means e Hier√°rquico

4. **Observa√ß√µes Gerais:**
   - Todos identificaram corretamente os 2 grupos principais
   - **Hier√°rquico obteve melhor acur√°cia (99.50%) vs K-Means (92.00%)**
   - Diferen√ßas aparecem principalmente na regi√£o de sobreposi√ß√£o entre [-1, 1]
   - DBSCAN √∫nico a marcar explicitamente pontos amb√≠guos como ru√≠do
   - Escolha do algoritmo depende do problema espec√≠fico:
     - K-Means: R√°pido, simples, clusters esf√©ricos
     - Hier√°rquico: Determin√≠stico, estrutura completa, pequenos datasets
     - DBSCAN: Formas arbitr√°rias, dete√ß√£o de outliers

---

### üìÅ Ficheiros do Exerc√≠cio 1

```
Exercicio1/
‚îú‚îÄ‚îÄ Gera√ß√£o_e_Visualiza√ß√£o_de_Distribui√ß√µes_Gaussianas.ipynb
‚îú‚îÄ‚îÄ Implementa√ß√£o_K-Means.ipynb
‚îú‚îÄ‚îÄ Clustering_Hier√°rquico_Aglomerativo.ipynb
‚îú‚îÄ‚îÄ DBSCAN_Clustering.ipynb
‚îî‚îÄ‚îÄ dados_gaussianos.csv
```

### ‚öôÔ∏è Requisitos Espec√≠ficos

```python
# Bibliotecas utilizadas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
from collections import deque
```

**Nota Importante:** Todos os algoritmos foram implementados **do zero**, sem usar bibliotecas de Machine Learning (sklearn, etc.), apenas NumPy para opera√ß√µes matem√°ticas b√°sicas.

## üíª Tecnologias Utilizadas

- **Python 3.13+**
- **Jupyter Notebooks** - Ambiente de desenvolvimento interativo
- **NumPy** - Opera√ß√µes num√©ricas e √°lgebra linear
- **Pandas** - Manipula√ß√£o e an√°lise de dados
- **Matplotlib** - Visualiza√ß√£o de dados e gr√°ficos
- **Collections** - Estruturas de dados (deque para DBSCAN)

**Nota:** No Exerc√≠cio 1, implementamos todos os algoritmos **do zero** sem usar bibliotecas de ML como scikit-learn, apenas NumPy para opera√ß√µes matem√°ticas fundamentais.

## üöÄ Como Utilizar

### 1. Clone este reposit√≥rio:
```bash
git clone https://github.com/y0c0ms/IAA_MEI25.git
cd IAA_MEI25
```

### 2. Configure o ambiente Python:

**Op√ß√£o A - Usando venv (recomendado):**
```bash
# Criar ambiente virtual
python -m venv venv

# Ativar ambiente (Windows)
venv\Scripts\activate

# Ativar ambiente (Linux/Mac)
source venv/bin/activate

# Instalar depend√™ncias
pip install numpy pandas matplotlib jupyter
```

**Op√ß√£o B - Instala√ß√£o direta:**
```bash
pip install numpy pandas matplotlib jupyter
```

### 3. Executar os notebooks:

**Para o Exerc√≠cio 1:**
```bash
cd Exercicio1
jupyter notebook
```

**Ordem recomendada de execu√ß√£o:**
1. `Gera√ß√£o_e_Visualiza√ß√£o_de_Distribui√ß√µes_Gaussianas.ipynb` - Gera o dataset
2. `Implementa√ß√£o_K-Means.ipynb` - K-Means online e batch
3. `Clustering_Hier√°rquico_Aglomerativo.ipynb` - Hier√°rquico
4. `DBSCAN_Clustering.ipynb` - DBSCAN baseado em densidade

**Nota:** O primeiro notebook gera o ficheiro `dados_gaussianos.csv` necess√°rio para os outros notebooks.

## Contacto

Para quest√µes ou sugest√µes relacionadas com este trabalho:
- Alexandre Mendes: [agmsa3@iscte-iul.pt]
- Manuel Santos: [majpr@iscte-iul.pt]

## üìä Estat√≠sticas do Projeto

### Exerc√≠cio 1
- **4 Notebooks Jupyter** completos
- **3 Algoritmos** implementados do zero
- **15+ Visualiza√ß√µes** interativas
- **1000 pontos** de dados gerados
- **30+ Experimentos** com diferentes par√¢metros
- **100+ Linhas** de an√°lise e discuss√£o

---

## üìù Licen√ßa e Utiliza√ß√£o Acad√©mica

Este reposit√≥rio √© parte integrante da avalia√ß√£o da UC de **Introdu√ß√£o √† Aprendizagem Autom√°tica** do Mestrado em Engenharia Inform√°tica do ISCTE-IUL. O c√≥digo e documenta√ß√£o s√£o disponibilizados para fins educacionais.

**Aten√ß√£o:** Este trabalho est√° protegido por direitos de autor acad√©micos. Se utilizar este c√≥digo como refer√™ncia, por favor cite adequadamente:

```
Mendes, A., & Santos, M. (2025). Clustering N√£o Supervisionado: K-Means, Hier√°rquico e DBSCAN.
Introdu√ß√£o √† Aprendizagem Autom√°tica, MEI, ISCTE-IUL.
```

---

## üîÑ Hist√≥rico de Atualiza√ß√µes

- **11/10/2025** - ‚úÖ Exerc√≠cio 1 completo: 4 notebooks (Gaussianas, K-Means, Hier√°rquico, DBSCAN)
- **11/10/2025** - üìö README atualizado com documenta√ß√£o completa
- **Outubro 2025** - üéØ In√≠cio do reposit√≥rio

---

*√öltima atualiza√ß√£o: 11 de Outubro de 2025*  
*Pr√≥xima entrega: Exerc√≠cio 2 (data a confirmar)*
