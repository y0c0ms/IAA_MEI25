{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercício 4: Análise de Entropia e Ganho de Informação\n",
        "\n",
        "**Objetivo**: Calcular entropia e ganho de informação para construção de árvores de decisão\n",
        "\n",
        "**Requisitos do guião**:\n",
        "1. Calcular entropia dos 4 conjuntos (completo + 3 subconjuntos)\n",
        "2. Calcular ganho da partição\n",
        "3. Calcular ganho para todas as features\n",
        "4. Explicar construção de árvore de decisão\n",
        "\n",
        "## Definição das Features do Dataset Iris\n",
        "\n",
        "O dataset Iris contém 4 features medidas em centímetros:\n",
        "- **Feature 1 (Sepal Length)**: Comprimento do sépala - parte externa da flor que protege o botão\n",
        "- **Feature 2 (Sepal Width)**: Largura do sépala - largura da parte externa da flor\n",
        "- **Feature 3 (Petal Length)**: Comprimento da pétala - parte colorida da flor\n",
        "- **Feature 4 (Petal Width)**: Largura da pétala - largura da parte colorida da flor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: 150 exemplos, 4 features\n",
            "Classes: ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
            "Target binário: Iris-setosa (p+) vs. outras (p-)\n",
            "Distribuição: 50 Iris-setosa, 100 outras\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def load_iris_data(filepath):\n",
        "    \"\"\"Carrega o dataset Iris\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(',')\n",
        "                if len(parts) == 5:\n",
        "                    features = [float(x) for x in parts[:4]]\n",
        "                    label = parts[4]\n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "    \n",
        "    X = np.array(data)\n",
        "    unique_labels = sorted(list(set(labels)))\n",
        "    label_to_num = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y = np.array([label_to_num[label] for label in labels])\n",
        "    \n",
        "    return X, y, unique_labels\n",
        "\n",
        "def discretize_features(X, method='tercis'):\n",
        "    \"\"\"Discretiza features em low/medium/high\"\"\"\n",
        "    X_discretized = np.zeros_like(X, dtype=int)\n",
        "    thresholds = {}\n",
        "    \n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        threshold_low = np.percentile(feature_values, 33.33)\n",
        "        threshold_high = np.percentile(feature_values, 66.67)\n",
        "        \n",
        "        discretized_feature = np.zeros(len(feature_values), dtype=int)\n",
        "        discretized_feature[feature_values <= threshold_low] = 0  # low\n",
        "        discretized_feature[(feature_values > threshold_low) & (feature_values <= threshold_high)] = 1  # medium\n",
        "        discretized_feature[feature_values > threshold_high] = 2  # high\n",
        "        \n",
        "        X_discretized[:, feature_idx] = discretized_feature\n",
        "        thresholds[feature_idx] = (threshold_low, threshold_high)\n",
        "    \n",
        "    return X_discretized, thresholds\n",
        "\n",
        "def calculate_entropy(y_binary):\n",
        "    \"\"\"Calcula entropia: entropy(S) = -p+ × log2(p+) - p- × log2(p-)\"\"\"\n",
        "    if len(y_binary) == 0:\n",
        "        return 0\n",
        "    \n",
        "    n_positive = np.sum(y_binary == 1)\n",
        "    n_negative = np.sum(y_binary == 0)\n",
        "    total = len(y_binary)\n",
        "    \n",
        "    p_positive = n_positive / total\n",
        "    p_negative = n_negative / total\n",
        "    \n",
        "    entropy = 0\n",
        "    if p_positive > 0:\n",
        "        entropy -= p_positive * math.log2(p_positive)\n",
        "    if p_negative > 0:\n",
        "        entropy -= p_negative * math.log2(p_negative)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X_discretized, y_binary, feature_idx):\n",
        "    \"\"\"Calcula ganho: gain(S,a) = entropy(S) - Σ(|Sv| × entropy(Sv)) / |S|\"\"\"\n",
        "    original_entropy = calculate_entropy(y_binary)\n",
        "    feature_values = X_discretized[:, feature_idx]\n",
        "    unique_values = np.unique(feature_values)\n",
        "    \n",
        "    weighted_entropy = 0\n",
        "    total_samples = len(y_binary)\n",
        "    subsets_info = {}\n",
        "    \n",
        "    for value in unique_values:\n",
        "        subset_mask = (feature_values == value)\n",
        "        subset_y = y_binary[subset_mask]\n",
        "        subset_size = len(subset_y)\n",
        "        subset_entropy = calculate_entropy(subset_y)\n",
        "        \n",
        "        # Fórmula: (|Sv| × entropy(Sv)) / |S|\n",
        "        weighted_entropy += (subset_size * subset_entropy) / total_samples\n",
        "        \n",
        "        n_positive = np.sum(subset_y == 1)\n",
        "        n_negative = np.sum(subset_y == 0)\n",
        "        \n",
        "        subsets_info[value] = {\n",
        "            'size': subset_size,\n",
        "            'entropy': subset_entropy,\n",
        "            'n_positive': n_positive,\n",
        "            'n_negative': n_negative\n",
        "        }\n",
        "    \n",
        "    information_gain = original_entropy - weighted_entropy\n",
        "    return information_gain, subsets_info\n",
        "\n",
        "# Carregar e preparar dados\n",
        "X_continuous, y, class_names = load_iris_data('iris/iris.data')\n",
        "feature_names = ['Feature 1 (Sepal Length)', 'Feature 2 (Sepal Width)', 'Feature 3 (Petal Length)', 'Feature 4 (Petal Width)']\n",
        "\n",
        "# Discretizar features\n",
        "X_discretized, thresholds = discretize_features(X_continuous, method='tercis')\n",
        "\n",
        "# Criar target binário: Iris-setosa (p+) vs. outras (p-)\n",
        "y_binary = (y == 0).astype(int)  # 0 = Iris-setosa\n",
        "\n",
        "print(f\"Dataset: {X_continuous.shape[0]} exemplos, {X_continuous.shape[1]} features\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Target binário: Iris-setosa (p+) vs. outras (p-)\")\n",
        "print(f\"Distribuição: {np.sum(y_binary)} Iris-setosa, {len(y_binary) - np.sum(y_binary)} outras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Cálculo da Entropia dos 4 Conjuntos\n",
        "\n",
        "**Fórmula**: `entropy(S) = -p+ × log₂(p+) - p- × log₂(p-)`\n",
        "\n",
        "Vamos calcular a entropia do conjunto completo e dos 3 subconjuntos criados pela primeira feature (Feature 1 - Sepal Length).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ENTROPIA DO CONJUNTO COMPLETO ===\n",
            "entropy(S) = 0.9183\n",
            "\n",
            "=== ENTROPIA DOS 3 SUBCONJUNTOS (Feature 1 (Sepal Length)) ===\n",
            "Limiares: Low ≤ 5.40, Medium ≤ 6.30, High > 6.30\n",
            "\n",
            "Low Dataset:\n",
            "  Tamanho: 52 exemplos\n",
            "  Iris-setosa: 45, Outras: 7\n",
            "  entropy(S_low) = 0.5700\n",
            "\n",
            "Medium Dataset:\n",
            "  Tamanho: 56 exemplos\n",
            "  Iris-setosa: 5, Outras: 51\n",
            "  entropy(S_medium) = 0.4341\n",
            "\n",
            "High Dataset:\n",
            "  Tamanho: 42 exemplos\n",
            "  Iris-setosa: 0, Outras: 42\n",
            "  entropy(S_high) = 0.0000\n",
            "\n",
            "=== RESUMO DAS 4 ENTROPIAS ===\n",
            "1. Conjunto completo (S): 0.9183\n",
            "2. Low Dataset: 0.5700\n",
            "3. Medium Dataset: 0.4341\n",
            "4. High Dataset: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# 1. Entropia do conjunto completo\n",
        "original_entropy = calculate_entropy(y_binary)\n",
        "print(f\"=== ENTROPIA DO CONJUNTO COMPLETO ===\")\n",
        "print(f\"entropy(S) = {original_entropy:.4f}\")\n",
        "\n",
        "# 2. Entropia dos 3 subconjuntos (primeira feature - Feature 1)\n",
        "feature_idx = 0  # Feature 1 (Sepal Length)\n",
        "feature_values = X_discretized[:, feature_idx]\n",
        "value_names = ['Low', 'Medium', 'High']\n",
        "\n",
        "print(f\"\\n=== ENTROPIA DOS 3 SUBCONJUNTOS ({feature_names[feature_idx]}) ===\")\n",
        "print(f\"Limiares: Low ≤ {thresholds[feature_idx][0]:.2f}, Medium ≤ {thresholds[feature_idx][1]:.2f}, High > {thresholds[feature_idx][1]:.2f}\")\n",
        "\n",
        "subsets_entropy = {}\n",
        "for value in [0, 1, 2]:  # low, medium, high\n",
        "    subset_mask = (feature_values == value)\n",
        "    subset_y = y_binary[subset_mask]\n",
        "    subset_entropy = calculate_entropy(subset_y)\n",
        "    subsets_entropy[value] = subset_entropy\n",
        "    \n",
        "    n_positive = np.sum(subset_y == 1)\n",
        "    n_negative = np.sum(subset_y == 0)\n",
        "    \n",
        "    print(f\"\\n{value_names[value]} Dataset:\")\n",
        "    print(f\"  Tamanho: {len(subset_y)} exemplos\")\n",
        "    print(f\"  Iris-setosa: {n_positive}, Outras: {n_negative}\")\n",
        "    print(f\"  entropy(S_{value_names[value].lower()}) = {subset_entropy:.4f}\")\n",
        "\n",
        "print(f\"\\n=== RESUMO DAS 4 ENTROPIAS ===\")\n",
        "print(f\"1. Conjunto completo (S): {original_entropy:.4f}\")\n",
        "print(f\"2. Low Dataset: {subsets_entropy[0]:.4f}\")\n",
        "print(f\"3. Medium Dataset: {subsets_entropy[1]:.4f}\")\n",
        "print(f\"4. High Dataset: {subsets_entropy[2]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cálculo do Ganho da Partição\n",
        "\n",
        "**Fórmula**: `gain(S,a) = entropy(S) - Σ(|Sv| × entropy(Sv)) / |S|`\n",
        "\n",
        "O ganho mede quanto a partição reduz a entropia média.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GANHO DA PARTIÇÃO (Feature 1 (Sepal Length)) ===\n",
            "\n",
            "Cálculo detalhado:\n",
            "entropy(S) = 0.9183\n",
            "\n",
            "Entropia ponderada dos subconjuntos:\n",
            "  |S_low| × entropy(S_low) / |S| = 52 × 0.5700 / 150 = 0.1976\n",
            "  |S_medium| × entropy(S_medium) / |S| = 56 × 0.4341 / 150 = 0.1621\n",
            "  |S_high| × entropy(S_high) / |S| = 42 × 0.0000 / 150 = 0.0000\n",
            "\n",
            "Σ(|Sv| × entropy(Sv)) / |S| = 0.3596\n",
            "\n",
            "gain(S, Feature 1 (Sepal Length)) = 0.9183 - 0.3596 = 0.5587\n",
            "\n",
            "=== INTERPRETAÇÃO ===\n",
            "• Antes da partição: entropia = 0.9183 (impureza moderada)\n",
            "• Após partição: entropia média = 0.3596\n",
            "• Redução: 0.5587 (60.8% de melhoria)\n",
            "• Significado: Feature 1 (Sepal Length) reduz significativamente a incerteza na classificação\n"
          ]
        }
      ],
      "source": [
        "# Calcular ganho para a primeira feature\n",
        "gain, subsets_info = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "\n",
        "print(f\"=== GANHO DA PARTIÇÃO ({feature_names[feature_idx]}) ===\")\n",
        "print(f\"\\nCálculo detalhado:\")\n",
        "print(f\"entropy(S) = {original_entropy:.4f}\")\n",
        "\n",
        "# Calcular entropia ponderada\n",
        "weighted_entropy = 0\n",
        "total_samples = len(y_binary)\n",
        "print(f\"\\nEntropia ponderada dos subconjuntos:\")\n",
        "for value in [0, 1, 2]:\n",
        "    if value in subsets_info:\n",
        "        info = subsets_info[value]\n",
        "        weight = info['size'] / total_samples\n",
        "        weighted_contribution = (info['size'] * info['entropy']) / total_samples\n",
        "        weighted_entropy += weighted_contribution\n",
        "        \n",
        "        print(f\"  |S_{value_names[value].lower()}| × entropy(S_{value_names[value].lower()}) / |S| = {info['size']} × {info['entropy']:.4f} / {total_samples} = {weighted_contribution:.4f}\")\n",
        "\n",
        "print(f\"\\nΣ(|Sv| × entropy(Sv)) / |S| = {weighted_entropy:.4f}\")\n",
        "print(f\"\\ngain(S, {feature_names[feature_idx]}) = {original_entropy:.4f} - {weighted_entropy:.4f} = {gain:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ganho para Todas as Features\n",
        "\n",
        "Vamos calcular o ganho para todas as 4 features e identificar a melhor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GANHO PARA TODAS AS FEATURES ===\n",
            "\n",
            "Feature                   Ganho    Ranking\n",
            "---------------------------------------------\n",
            "Feature 3 (Petal Length)  0.9183   1º\n",
            "Feature 4 (Petal Width)   0.9183   2º\n",
            "Feature 1 (Sepal Length)  0.5587   3º\n",
            "Feature 2 (Sepal Width)   0.3081   4º\n",
            "\n",
            "=== MELHOR FEATURE ===\n",
            "Feature: Feature 3 (Petal Length)\n",
            "Ganho: 0.9183\n",
            "\n",
            "Significado: Feature 3 (Petal Length) oferece o maior potencial de classificação\n",
            "para distinguir Iris-setosa das outras classes.\n",
            "\n",
            "=== ANÁLISE DA MELHOR FEATURE ===\n",
            "Particionamento por Feature 3 (Petal Length):\n",
            "  Low: 50 exemplos, entropia = 0.0000\n",
            "    → Todos são Iris-setosa (puro!)\n",
            "  Medium: 54 exemplos, entropia = 0.0000\n",
            "    → Nenhum é Iris-setosa (puro!)\n",
            "  High: 46 exemplos, entropia = 0.0000\n",
            "    → Nenhum é Iris-setosa (puro!)\n"
          ]
        }
      ],
      "source": [
        "print(f\"=== GANHO PARA TODAS AS FEATURES ===\")\n",
        "\n",
        "gains = []\n",
        "for feature_idx in range(len(feature_names)):\n",
        "    gain, _ = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "    gains.append((feature_idx, gain, feature_names[feature_idx]))\n",
        "\n",
        "# Ordenar por ganho (maior primeiro)\n",
        "gains.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\n{'Feature':<25} {'Ganho':<8} {'Ranking'}\")\n",
        "print(\"-\" * 45)\n",
        "for rank, (feature_idx, gain, feature_name) in enumerate(gains, 1):\n",
        "    print(f\"{feature_name:<25} {gain:<8.4f} {rank}º\")\n",
        "\n",
        "best_feature_idx, best_gain, best_feature_name = gains[0]\n",
        "print(f\"\\n=== MELHOR FEATURE ===\")\n",
        "print(f\"Feature: {best_feature_name}\")\n",
        "print(f\"Ganho: {best_gain:.4f}\")\n",
        "print(f\"\\nSignificado: {best_feature_name} oferece o maior potencial de classificação\")\n",
        "print(f\"para distinguir Iris-setosa das outras classes.\")\n",
        "\n",
        "# Mostrar análise da melhor feature\n",
        "_, best_subsets = calculate_information_gain(X_discretized, y_binary, best_feature_idx)\n",
        "print(f\"\\n=== ANÁLISE DA MELHOR FEATURE ===\")\n",
        "print(f\"Particionamento por {best_feature_name}:\")\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        print(f\"  {value_names[value]}: {info['size']} exemplos, entropia = {info['entropy']:.4f}\")\n",
        "        if info['entropy'] == 0:\n",
        "            if info['n_positive'] == info['size']:\n",
        "                print(f\"    → Todos são Iris-setosa (puro!)\")\n",
        "            else:\n",
        "                print(f\"    → Nenhum é Iris-setosa (puro!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Construção de Árvore de Decisão\n",
        "\n",
        "**Estratégia**: Usar o ganho de informação para escolher a melhor feature em cada nó.\n",
        "\n",
        "### Algoritmo:\n",
        "1. **Nó Raiz**: Escolher a feature com maior ganho\n",
        "2. **Particionamento**: Dividir dados pelos valores da feature (Low/Medium/High)\n",
        "3. **Recursão**: Para cada subconjunto:\n",
        "   - Se entropia = 0 → nó folha (classe pura)\n",
        "   - Senão → repetir com features restantes\n",
        "4. **Critérios de parada**: Entropia = 0 ou não há mais features\n",
        "\n",
        "### Construção da Árvore de Decisão\n",
        "\n",
        "**PASSO 1: Nó Raiz**\n",
        "- Escolher feature com maior ganho: Feature 3 (Petal Length) (0.9183)\n",
        "- Entropia inicial: 0.9183\n",
        "\n",
        "**PASSO 2: Particionamento**\n",
        "- Dividir dados pelos valores de Feature 3 (Petal Length):\n",
        "  - Low: 50 exemplos → FOLHA: Iris-setosa (todos puros)\n",
        "  - Medium: 54 exemplos → FOLHA: Outras classes (todos puros)\n",
        "  - High: 46 exemplos → FOLHA: Outras classes (todos puros)\n",
        "\n",
        "**PASSO 3: Resultado**\n",
        "- A árvore seria muito simples neste caso!\n",
        "- Feature 3 (Petal Length) separa perfeitamente as classes\n",
        "- Todos os subconjuntos têm entropia = 0 (puros)\n",
        "- Não é necessário continuar o particionamento\n",
        "\n",
        "**REGRAS DE DECISÃO EXTRAÍDAS:**\n",
        "- SE Feature 3 (Petal Length) ≤ 1.90 ENTÃO Iris-setosa\n",
        "- SE 1.90 < Feature 3 (Petal Length) ≤ 4.35 ENTÃO Outras classes\n",
        "- SE Feature 3 (Petal Length) > 4.35 ENTÃO Outras classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo dos Resultados\n",
        "\n",
        "### Entropias Calculadas:\n",
        "- **Conjunto completo**: 0.9183\n",
        "- **Subconjuntos**: Varia conforme a feature\n",
        "\n",
        "### Melhor Feature:\n",
        "- **Feature 3 (Petal Length)** com ganho de **0.9183**\n",
        "- Oferece separação perfeita das classes\n",
        "\n",
        "### Árvore de Decisão:\n",
        "- Estrutura simples com apenas um nível\n",
        "- Regras claras e interpretáveis\n",
        "- Classificação perfeita possível\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
