{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercício 4: Análise de Entropia e Ganho de Informação\n",
        "\n",
        "Este notebook implementa a análise de entropia e ganho de informação conforme especificado no guião, seguindo todos os requisitos:\n",
        "- Cálculo de entropia sem bibliotecas de algoritmos de AA\n",
        "- Análise com foco na classe Iris-setosa como alvo\n",
        "- Particionamento por features discretizadas\n",
        "- Explicação da construção de árvores de decisão\n",
        "\n",
        "## Fundamentos Teóricos\n",
        "\n",
        "### Entropia\n",
        "A entropia mede a \"impureza\" ou \"desordem\" de um conjunto de dados:\n",
        "\n",
        "**entropy(S) = -p₊ × log₂(p₊) - p₋ × log₂(p₋)**\n",
        "\n",
        "Onde:\n",
        "- p₊ = proporção de exemplos positivos\n",
        "- p₋ = proporção de exemplos negativos\n",
        "\n",
        "### Ganho de Informação\n",
        "O ganho de informação mede quanto uma feature reduz a entropia:\n",
        "\n",
        "**gain(S, a) = entropy(S) - Σ(|Sᵥ|/|S|) × entropy(Sᵥ)**\n",
        "\n",
        "Onde Sᵥ são os subconjuntos criados pelos valores v da feature a.\n",
        "\n",
        "**Objetivo**: Analisar qual feature oferece maior ganho para distinguir Iris-setosa das demais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Configuração\n",
        "np.random.seed(42)\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "def load_iris_data(filepath):\n",
        "    \"\"\"Carrega o dataset Iris\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(',')\n",
        "                if len(parts) == 5:\n",
        "                    features = [float(x) for x in parts[:4]]\n",
        "                    label = parts[4]\n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "    \n",
        "    X = np.array(data)\n",
        "    \n",
        "    # Converter labels para números\n",
        "    unique_labels = list(set(labels))\n",
        "    unique_labels.sort()\n",
        "    \n",
        "    label_to_num = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y = np.array([label_to_num[label] for label in labels])\n",
        "    \n",
        "    return X, y, unique_labels\n",
        "\n",
        "def discretize_features(X, method='tercis'):\n",
        "    \"\"\"Discretiza features contínuas em categorias low/medium/high\"\"\"\n",
        "    X_discretized = np.zeros_like(X, dtype=int)\n",
        "    thresholds = {}\n",
        "    \n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        \n",
        "        if method == 'tercis':\n",
        "            threshold_low = np.percentile(feature_values, 33.33)\n",
        "            threshold_high = np.percentile(feature_values, 66.67)\n",
        "        else:\n",
        "            raise ValueError(f\"Método '{method}' não reconhecido\")\n",
        "        \n",
        "        discretized_feature = np.zeros(len(feature_values), dtype=int)\n",
        "        discretized_feature[feature_values <= threshold_low] = 0  # low\n",
        "        discretized_feature[(feature_values > threshold_low) & (feature_values <= threshold_high)] = 1  # medium  \n",
        "        discretized_feature[feature_values > threshold_high] = 2  # high\n",
        "        \n",
        "        X_discretized[:, feature_idx] = discretized_feature\n",
        "        thresholds[feature_idx] = (threshold_low, threshold_high)\n",
        "    \n",
        "    return X_discretized, thresholds\n",
        "\n",
        "def create_binary_target(y, target_class=0):\n",
        "    \"\"\"\n",
        "    Cria target binário: classe alvo vs. todas as outras\n",
        "    \n",
        "    Args:\n",
        "        y: array de labels originais\n",
        "        target_class: classe a ser considerada positiva (default: 0 = Iris-setosa)\n",
        "    \n",
        "    Returns:\n",
        "        y_binary: array binário (1 = classe alvo, 0 = outras classes)\n",
        "    \"\"\"\n",
        "    y_binary = (y == target_class).astype(int)\n",
        "    return y_binary\n",
        "\n",
        "def calculate_entropy(y_binary):\n",
        "    \"\"\"\n",
        "    Calcula entropia de um conjunto com classes binárias\n",
        "    \n",
        "    Args:\n",
        "        y_binary: array binário de labels\n",
        "    \n",
        "    Returns:\n",
        "        entropy: valor da entropia\n",
        "    \"\"\"\n",
        "    if len(y_binary) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Contar classes positivas e negativas\n",
        "    n_positive = np.sum(y_binary == 1)\n",
        "    n_negative = np.sum(y_binary == 0)\n",
        "    total = len(y_binary)\n",
        "    \n",
        "    # Proporções\n",
        "    p_positive = n_positive / total\n",
        "    p_negative = n_negative / total\n",
        "    \n",
        "    # Calcular entropia (evitar log(0))\n",
        "    entropy = 0\n",
        "    if p_positive > 0:\n",
        "        entropy -= p_positive * math.log2(p_positive)\n",
        "    if p_negative > 0:\n",
        "        entropy -= p_negative * math.log2(p_negative)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X_discretized, y_binary, feature_idx):\n",
        "    \"\"\"\n",
        "    Calcula o ganho de informação para uma feature específica\n",
        "    \n",
        "    Args:\n",
        "        X_discretized: features discretizadas\n",
        "        y_binary: labels binários\n",
        "        feature_idx: índice da feature a analisar\n",
        "    \n",
        "    Returns:\n",
        "        gain: ganho de informação\n",
        "        subsets_info: informações sobre os subconjuntos criados\n",
        "    \"\"\"\n",
        "    # Entropia do conjunto original\n",
        "    original_entropy = calculate_entropy(y_binary)\n",
        "    \n",
        "    # Obter valores únicos da feature\n",
        "    feature_values = X_discretized[:, feature_idx]\n",
        "    unique_values = np.unique(feature_values)\n",
        "    \n",
        "    # Calcular entropia ponderada dos subconjuntos\n",
        "    weighted_entropy = 0\n",
        "    total_samples = len(y_binary)\n",
        "    subsets_info = {}\n",
        "    \n",
        "    for value in unique_values:\n",
        "        # Criar subconjunto para este valor da feature\n",
        "        subset_mask = (feature_values == value)\n",
        "        subset_y = y_binary[subset_mask]\n",
        "        subset_size = len(subset_y)\n",
        "        \n",
        "        # Calcular entropia do subconjunto\n",
        "        subset_entropy = calculate_entropy(subset_y)\n",
        "        \n",
        "        # Adicionar à entropia ponderada\n",
        "        weight = subset_size / total_samples\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        \n",
        "        # Armazenar informações do subconjunto\n",
        "        n_positive = np.sum(subset_y == 1)\n",
        "        n_negative = np.sum(subset_y == 0)\n",
        "        \n",
        "        subsets_info[value] = {\n",
        "            'size': subset_size,\n",
        "            'entropy': subset_entropy,\n",
        "            'weight': weight,\n",
        "            'n_positive': n_positive,\n",
        "            'n_negative': n_negative,\n",
        "            'proportion_positive': n_positive / subset_size if subset_size > 0 else 0\n",
        "        }\n",
        "    \n",
        "    # Calcular ganho de informação\n",
        "    information_gain = original_entropy - weighted_entropy\n",
        "    \n",
        "    return information_gain, subsets_info\n",
        "\n",
        "# Carregar e preparar dados\n",
        "print(\"=== CARREGAMENTO E PREPARAÇÃO DOS DADOS ===\")\n",
        "\n",
        "# Carregar dados\n",
        "X_continuous, y, class_names = load_iris_data('iris/iris.data')\n",
        "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
        "\n",
        "print(f\"Dataset carregado: {X_continuous.shape}\")\n",
        "print(f\"Classes originais: {class_names}\")\n",
        "\n",
        "# Discretizar features (reutilizando do exercício anterior)\n",
        "X_discretized, thresholds = discretize_features(X_continuous, method='tercis')\n",
        "\n",
        "# Criar target binário: Iris-setosa vs. outras classes\n",
        "y_binary = create_binary_target(y, target_class=0)  # 0 = Iris-setosa\n",
        "\n",
        "print(f\"\\n=== DEFINIÇÃO DO PROBLEMA BINÁRIO ===\")\n",
        "print(f\"Classe alvo (positiva): {class_names[0]} (Iris-setosa)\")\n",
        "print(f\"Outras classes (negativa): {class_names[1]}, {class_names[2]}\")\n",
        "\n",
        "# Contar distribuição\n",
        "n_positive = np.sum(y_binary == 1)\n",
        "n_negative = np.sum(y_binary == 0)\n",
        "total = len(y_binary)\n",
        "\n",
        "print(f\"\\nDistribuição:\")\n",
        "print(f\"  Iris-setosa (p+): {n_positive} exemplos ({n_positive/total*100:.1f}%)\")\n",
        "print(f\"  Outras classes (p-): {n_negative} exemplos ({n_negative/total*100:.1f}%)\")\n",
        "\n",
        "# Calcular entropia do conjunto completo\n",
        "print(f\"\\n=== ENTROPIA DO CONJUNTO COMPLETO ===\")\n",
        "original_entropy = calculate_entropy(y_binary)\n",
        "print(f\"Entropia(S) = {original_entropy:.4f}\")\n",
        "\n",
        "# Interpretar o valor da entropia\n",
        "if original_entropy == 0:\n",
        "    interpretation = \"Conjunto puro (todas as amostras da mesma classe)\"\n",
        "elif original_entropy == 1:\n",
        "    interpretation = \"Máxima impureza (distribuição 50/50)\"\n",
        "else:\n",
        "    interpretation = f\"Impureza moderada (mais próximo de {'puro' if original_entropy < 0.5 else 'impuro'})\"\n",
        "\n",
        "print(f\"Interpretação: {interpretation}\")\n",
        "\n",
        "# Mostrar fórmula de cálculo\n",
        "p_pos = n_positive / total\n",
        "p_neg = n_negative / total\n",
        "print(f\"\\nCálculo detalhado:\")\n",
        "print(f\"  p+ = {n_positive}/{total} = {p_pos:.3f}\")\n",
        "print(f\"  p- = {n_negative}/{total} = {p_neg:.3f}\")\n",
        "print(f\"  entropy(S) = -({p_pos:.3f} × log₂({p_pos:.3f})) - ({p_neg:.3f} × log₂({p_neg:.3f}))\")\n",
        "print(f\"  entropy(S) = -({p_pos:.3f} × {math.log2(p_pos):.3f}) - ({p_neg:.3f} × {math.log2(p_neg):.3f})\")\n",
        "print(f\"  entropy(S) = {-p_pos * math.log2(p_pos):.4f} + {-p_neg * math.log2(p_neg):.4f} = {original_entropy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise detalhada: partição pela primeira feature (Sepal Length)\n",
        "print(f\"\\n=== EXEMPLO DETALHADO: PARTIÇÃO POR {feature_names[0].upper()} ===\")\n",
        "\n",
        "feature_idx = 0  # Sepal Length\n",
        "gain, subsets_info = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "\n",
        "print(f\"Analisando partição por {feature_names[feature_idx]}:\")\n",
        "\n",
        "# Mostrar limiares de discretização\n",
        "low_thresh, high_thresh = thresholds[feature_idx]\n",
        "print(f\"Limiares de discretização:\")\n",
        "print(f\"  Low: ≤ {low_thresh:.2f}\")\n",
        "print(f\"  Medium: {low_thresh:.2f} < valor ≤ {high_thresh:.2f}\")\n",
        "print(f\"  High: > {high_thresh:.2f}\")\n",
        "\n",
        "print(f\"\\nSubconjuntos criados:\")\n",
        "value_names = ['Low', 'Medium', 'High']\n",
        "\n",
        "for value in [0, 1, 2]:  # low, medium, high\n",
        "    if value in subsets_info:\n",
        "        info = subsets_info[value]\n",
        "        print(f\"\\n{value_names[value]} Dataset:\")\n",
        "        print(f\"  Tamanho: {info['size']} exemplos ({info['weight']*100:.1f}% do total)\")\n",
        "        print(f\"  Iris-setosa: {info['n_positive']} exemplos\")\n",
        "        print(f\"  Outras: {info['n_negative']} exemplos\")\n",
        "        print(f\"  Proporção Iris-setosa: {info['proportion_positive']:.3f}\")\n",
        "        print(f\"  Entropia: {info['entropy']:.4f}\")\n",
        "\n",
        "# Calcular entropia ponderada\n",
        "weighted_entropy = sum(info['weight'] * info['entropy'] for info in subsets_info.values())\n",
        "print(f\"\\nEntropia ponderada dos subconjuntos:\")\n",
        "print(f\"  Σ (|Sv|/|S|) × entropy(Sv) = {weighted_entropy:.4f}\")\n",
        "\n",
        "print(f\"\\nGanho de informação:\")\n",
        "print(f\"  gain(S, {feature_names[feature_idx]}) = {original_entropy:.4f} - {weighted_entropy:.4f} = {gain:.4f}\")\n",
        "\n",
        "print(f\"\\n=== ANÁLISE COMPLETA: TODAS AS FEATURES ===\")\n",
        "\n",
        "# Calcular ganho para todas as features\n",
        "gains_info = {}\n",
        "\n",
        "for feature_idx in range(len(feature_names)):\n",
        "    gain, subsets_info = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "    gains_info[feature_idx] = {\n",
        "        'gain': gain,\n",
        "        'subsets': subsets_info,\n",
        "        'feature_name': feature_names[feature_idx]\n",
        "    }\n",
        "\n",
        "# Mostrar resultados ordenados por ganho\n",
        "print(\"Ganho de informação por feature:\")\n",
        "print(f\"{'Feature':<15} {'Ganho':<8} {'Ranking'}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Ordenar por ganho (maior primeiro)\n",
        "sorted_features = sorted(gains_info.items(), key=lambda x: x[1]['gain'], reverse=True)\n",
        "\n",
        "for rank, (feature_idx, info) in enumerate(sorted_features, 1):\n",
        "    feature_name = info['feature_name']\n",
        "    gain = info['gain']\n",
        "    print(f\"{feature_name:<15} {gain:<8.4f} {rank}\")\n",
        "\n",
        "# Feature com maior ganho\n",
        "best_feature_idx, best_info = sorted_features[0]\n",
        "best_feature_name = best_info['feature_name']\n",
        "best_gain = best_info['gain']\n",
        "\n",
        "print(f\"\\n**MELHOR FEATURE**: {best_feature_name} (ganho = {best_gain:.4f})\")\n",
        "\n",
        "# Análise detalhada da melhor feature\n",
        "print(f\"\\n=== ANÁLISE DETALHADA: {best_feature_name.upper()} ===\")\n",
        "\n",
        "best_subsets = best_info['subsets']\n",
        "print(f\"Particionamento por {best_feature_name}:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        purity = max(info['proportion_positive'], 1 - info['proportion_positive'])\n",
        "        purity_desc = \"Muito puro\" if purity > 0.9 else \"Puro\" if purity > 0.7 else \"Impuro\"\n",
        "        \n",
        "        print(f\"\\n{value_names[value]}:\")\n",
        "        print(f\"  Tamanho: {info['size']} exemplos\")\n",
        "        print(f\"  Entropia: {info['entropy']:.4f}\")\n",
        "        print(f\"  Pureza: {purity:.3f} ({purity_desc})\")\n",
        "        \n",
        "        if info['n_positive'] == info['size']:\n",
        "            print(f\"  → Todos são Iris-setosa!\")\n",
        "        elif info['n_negative'] == info['size']:\n",
        "            print(f\"  → Nenhum é Iris-setosa!\")\n",
        "        else:\n",
        "            print(f\"  → Misto: {info['n_positive']} Iris-setosa, {info['n_negative']} outras\")\n",
        "\n",
        "# Visualização dos ganhos\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Análise de Ganho de Informação para Árvores de Decisão', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Gráfico 1: Ganho por feature\n",
        "ax1 = axes[0, 0]\n",
        "features = [gains_info[i]['feature_name'] for i in range(4)]\n",
        "gains = [gains_info[i]['gain'] for i in range(4)]\n",
        "colors = ['gold' if i == best_feature_idx else 'lightblue' for i in range(4)]\n",
        "\n",
        "bars = ax1.bar(features, gains, color=colors, edgecolor='black')\n",
        "ax1.set_title('Ganho de Informação por Feature')\n",
        "ax1.set_ylabel('Ganho de Informação')\n",
        "ax1.set_xticklabels(features, rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for bar, gain in zip(bars, gains):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{gain:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gráfico 2: Entropia dos subconjuntos da melhor feature\n",
        "ax2 = axes[0, 1]\n",
        "subset_names = [f'{value_names[v]}' for v in [0, 1, 2] if v in best_subsets]\n",
        "subset_entropies = [best_subsets[v]['entropy'] for v in [0, 1, 2] if v in best_subsets]\n",
        "subset_sizes = [best_subsets[v]['size'] for v in [0, 1, 2] if v in best_subsets]\n",
        "\n",
        "bars2 = ax2.bar(subset_names, subset_entropies, color=['lightcoral', 'lightgreen', 'lightyellow'])\n",
        "ax2.set_title(f'Entropia dos Subconjuntos - {best_feature_name}')\n",
        "ax2.set_ylabel('Entropia')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar tamanhos dos subconjuntos\n",
        "for bar, entropy, size in zip(bars2, subset_entropies, subset_sizes):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "             f'{entropy:.3f}\\n(n={size})', ha='center', va='bottom')\n",
        "\n",
        "# Gráfico 3: Distribuição de classes na melhor feature\n",
        "ax3 = axes[1, 0]\n",
        "x_pos = np.arange(len(subset_names))\n",
        "positives = [best_subsets[v]['n_positive'] for v in [0, 1, 2] if v in best_subsets]\n",
        "negatives = [best_subsets[v]['n_negative'] for v in [0, 1, 2] if v in best_subsets]\n",
        "\n",
        "width = 0.35\n",
        "bars3a = ax3.bar(x_pos - width/2, positives, width, label='Iris-setosa', color='red', alpha=0.7)\n",
        "bars3b = ax3.bar(x_pos + width/2, negatives, width, label='Outras classes', color='blue', alpha=0.7)\n",
        "\n",
        "ax3.set_title(f'Distribuição de Classes - {best_feature_name}')\n",
        "ax3.set_ylabel('Número de Exemplos')\n",
        "ax3.set_xticks(x_pos)\n",
        "ax3.set_xticklabels(subset_names)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 4: Comparação visual das entropias\n",
        "ax4 = axes[1, 1]\n",
        "all_entropies = [original_entropy] + [gains_info[i]['subsets'][v]['entropy'] \n",
        "                                     for i in range(4) for v in [0, 1, 2] \n",
        "                                     if v in gains_info[i]['subsets']]\n",
        "labels = ['Original'] + [f'{gains_info[i][\"feature_name\"][:4]}-{value_names[v]}' \n",
        "                        for i in range(4) for v in [0, 1, 2] \n",
        "                        if v in gains_info[i]['subsets']]\n",
        "\n",
        "# Destacar conjunto original e subconjuntos da melhor feature\n",
        "colors_entropy = ['red'] + ['gold' if label.startswith(best_feature_name[:4]) else 'lightgray' \n",
        "                           for label in labels[1:]]\n",
        "\n",
        "bars4 = ax4.bar(range(len(all_entropies)), all_entropies, color=colors_entropy, alpha=0.7)\n",
        "ax4.set_title('Comparação de Entropias')\n",
        "ax4.set_ylabel('Entropia')\n",
        "ax4.set_xticks(range(len(labels)))\n",
        "ax4.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== INTERPRETAÇÃO DOS RESULTADOS ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**CAPACIDADE DISCRIMINATIVA**:\n",
        "\n",
        "1. **Antes da partição** (conjunto S):\n",
        "   - Entropia = {original_entropy:.4f}\n",
        "   - Indica impureza moderada (mistura de classes)\n",
        "\n",
        "2. **Após partição por {best_feature_name}**:\n",
        "   - Ganho = {best_gain:.4f}\n",
        "   - Redução significativa na entropia média\n",
        "   - Melhora na capacidade de classificação\n",
        "\n",
        "**O QUE ISSO SIGNIFICA**:\n",
        "- {best_feature_name} é a feature mais informativa para distinguir Iris-setosa\n",
        "- Conhecer o valor de {best_feature_name} reduz significativamente a incerteza\n",
        "- Esta seria a melhor escolha para o nó raiz de uma árvore de decisão\n",
        "\n",
        "**RANKING DE IMPORTÂNCIA**:\"\"\")\n",
        "\n",
        "for rank, (feature_idx, info) in enumerate(sorted_features, 1):\n",
        "    effectiveness = \"Excelente\" if info['gain'] > 0.5 else \"Boa\" if info['gain'] > 0.3 else \"Moderada\" if info['gain'] > 0.1 else \"Fraca\"\n",
        "    print(f\"  {rank}º. {info['feature_name']}: {info['gain']:.4f} ({effectiveness})\")\n",
        "\n",
        "print(f\"\\n**POTENCIAL DE CLASSIFICAÇÃO**:\")\n",
        "improvement = (1 - weighted_entropy / original_entropy) * 100\n",
        "print(f\"- Redução da entropia: {improvement:.1f}%\")\n",
        "print(f\"- Melhoria na pureza dos subconjuntos\")\n",
        "print(f\"- Base sólida para construção de árvore de decisão\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construção de árvores de decisão usando esta estratégia\n",
        "print(f\"\\n=== COMO CONSTRUIR UMA ÁRVORE DE DECISÃO ===\")\n",
        "\n",
        "def build_decision_tree_explanation():\n",
        "    \"\"\"Explica o algoritmo de construção de árvores de decisão\"\"\"\n",
        "    \n",
        "    print(\"\"\"\n",
        "**ALGORITMO ID3 (Iterative Dichotomiser 3)**:\n",
        "\n",
        "1. **Nó Raiz**:\n",
        "   - Começar com o conjunto completo S\n",
        "   - Calcular entropia de S\n",
        "   - Se entropia = 0 → nó folha (classe pura)\n",
        "   - Senão → continuar particionamento\n",
        "\n",
        "2. **Seleção da Feature**:\n",
        "   - Calcular ganho de informação para todas as features\n",
        "   - Escolher a feature com MAIOR ganho\n",
        "   - Esta torna-se o critério de divisão do nó\n",
        "\n",
        "3. **Particionamento**:\n",
        "   - Dividir o conjunto pelos valores da feature escolhida\n",
        "   - Criar um ramo para cada valor (Low, Medium, High)\n",
        "   - Cada ramo recebe o subconjunto correspondente\n",
        "\n",
        "4. **Recursão**:\n",
        "   - Para cada subconjunto criado:\n",
        "     * Se puro (entropia = 0) → criar nó folha\n",
        "     * Se impuro → repetir processo (voltar ao passo 1)\n",
        "     * Se não há mais features → classe majoritária\n",
        "\n",
        "5. **Critérios de Parada**:\n",
        "   - Entropia = 0 (conjunto puro)\n",
        "   - Não há mais features para dividir\n",
        "   - Número mínimo de exemplos atingido\n",
        "   - Profundidade máxima alcançada\n",
        "\"\"\")\n",
        "\n",
        "build_decision_tree_explanation()\n",
        "\n",
        "# Demonstração prática: construção da primeira camada\n",
        "print(f\"=== DEMONSTRAÇÃO: PRIMEIRA CAMADA DA ÁRVORE ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**PASSO A PASSO PARA NOSSO CASO**:\n",
        "\n",
        "1. **Nó Raiz**:\n",
        "   - Dataset completo: 150 exemplos\n",
        "   - Entropia inicial: {original_entropy:.4f}\n",
        "   - Objetivo: classificar Iris-setosa vs. outras\n",
        "\n",
        "2. **Escolha da Feature**:\n",
        "   - Testamos todas as 4 features\n",
        "   - {best_feature_name} tem maior ganho: {best_gain:.4f}\n",
        "   - DECISÃO: usar {best_feature_name} como nó raiz\n",
        "\n",
        "3. **Criação dos Ramos**:\"\"\")\n",
        "\n",
        "# Mostrar a estrutura da primeira camada\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        if info['entropy'] == 0:\n",
        "            if info['n_positive'] == info['size']:\n",
        "                decision = \"→ FOLHA: Iris-setosa\"\n",
        "            else:\n",
        "                decision = \"→ FOLHA: Outras classes\"\n",
        "        else:\n",
        "            decision = f\"→ CONTINUAR (entropia={info['entropy']:.4f})\"\n",
        "        \n",
        "        print(f\"   {best_feature_name} = {value_name}: {info['size']} exemplos {decision}\")\n",
        "\n",
        "# Visualizar a árvore textualmente\n",
        "print(f\"\\n=== REPRESENTAÇÃO DA ÁRVORE (PRIMEIRA CAMADA) ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "                    [RAIZ]\n",
        "               {best_feature_name} = ?\n",
        "               Entropia: {original_entropy:.4f}\n",
        "                 /      |      \\\\\n",
        "                /       |       \\\\\n",
        "            Low        Medium     High\n",
        "       ({best_subsets[0]['size']} exemplos)   ({best_subsets[1]['size']} exemplos)    ({best_subsets[2]['size']} exemplos)\n",
        "     Ent: {best_subsets[0]['entropy']:.3f}    Ent: {best_subsets[1]['entropy']:.3f}     Ent: {best_subsets[2]['entropy']:.3f}\n",
        "\"\"\")\n",
        "\n",
        "# Análise dos próximos passos\n",
        "print(f\"**PRÓXIMOS PASSOS**:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        if info['entropy'] == 0:\n",
        "            print(f\"• Ramo {value_name}: PARAR (pureza alcançada)\")\n",
        "        else:\n",
        "            print(f\"• Ramo {value_name}: CONTINUAR particionamento\")\n",
        "            print(f\"  - Calcular ganho das features restantes\")\n",
        "            print(f\"  - Escolher melhor feature para subdividir\")\n",
        "            print(f\"  - Criar novos subnós\")\n",
        "\n",
        "# Vantagens e limitações\n",
        "print(f\"\\n=== VANTAGENS E LIMITAÇÕES ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**VANTAGENS DO MÉTODO**:\n",
        "• Interpretabilidade: regras claras e legíveis\n",
        "• Não assume distribuição específica dos dados\n",
        "• Lida bem com features categóricas\n",
        "• Identifica automaticamente features importantes\n",
        "• Robusto a outliers\n",
        "\n",
        "**LIMITAÇÕES**:\n",
        "• Tendência ao overfitting (árvores muito profundas)\n",
        "• Instabilidade (pequenas mudanças → árvores diferentes)\n",
        "• Bias para features com mais valores\n",
        "• Dificuldade com relações lineares complexas\n",
        "• Pode criar árvores desbalanceadas\n",
        "\n",
        "**MELHORIAS POSSÍVEIS**:\n",
        "• Poda da árvore (pruning)\n",
        "• Critérios alternativos (Gini, gain ratio)\n",
        "• Ensemble methods (Random Forest)\n",
        "• Regularização (profundidade máxima, min samples)\n",
        "\"\"\")\n",
        "\n",
        "# Comparação com outros métodos\n",
        "print(f\"\\n=== COMPARAÇÃO COM OUTROS ALGORITMOS ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**ÁRVORES DE DECISÃO vs. ALGORITMOS ANTERIORES**:\n",
        "\n",
        "| Aspecto              | Árvore Decisão | k-NN      | Naive Bayes |\n",
        "|---------------------|----------------|-----------|-------------|\n",
        "| Interpretabilidade   | Excelente      | Fraca     | Boa         |\n",
        "| Velocidade (predição)| Muito rápida   | Lenta     | Rápida      |\n",
        "| Dados categóricos   | Nativa         | Problemas | Boa         |\n",
        "| Overfitting         | Alto risco     | Médio     | Baixo       |\n",
        "| Preparação dados    | Mínima         | Escala    | Discretização|\n",
        "\n",
        "**QUANDO USAR ÁRVORES**:\n",
        "• Quando interpretabilidade é crucial\n",
        "• Dados com mix de tipos (numérico + categórico)  \n",
        "• Necessidade de regras explicitas\n",
        "• Base para algoritmos ensemble\n",
        "\"\"\")\n",
        "\n",
        "# Exemplo de regras extraídas\n",
        "print(f\"\\n=== REGRAS EXTRAÍDAS DA ANÁLISE ===\")\n",
        "\n",
        "print(\"Regras para classificar Iris-setosa:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        low_thresh, high_thresh = thresholds[best_feature_idx]\n",
        "        \n",
        "        if value == 0:  # Low\n",
        "            condition = f\"{best_feature_name} ≤ {low_thresh:.2f}\"\n",
        "        elif value == 1:  # Medium  \n",
        "            condition = f\"{low_thresh:.2f} < {best_feature_name} ≤ {high_thresh:.2f}\"\n",
        "        else:  # High\n",
        "            condition = f\"{best_feature_name} > {high_thresh:.2f}\"\n",
        "        \n",
        "        probability = info['proportion_positive']\n",
        "        \n",
        "        if probability >= 0.9:\n",
        "            prediction = \"Iris-setosa\"\n",
        "            confidence = \"Alta\"\n",
        "        elif probability >= 0.5:\n",
        "            prediction = \"Provavelmente Iris-setosa\"\n",
        "            confidence = \"Média\"\n",
        "        elif probability >= 0.1:\n",
        "            prediction = \"Possivelmente outras classes\"\n",
        "            confidence = \"Média\"\n",
        "        else:\n",
        "            prediction = \"Outras classes\"\n",
        "            confidence = \"Alta\"\n",
        "        \n",
        "        print(f\"• SE {condition}\")\n",
        "        print(f\"  ENTÃO {prediction} (confiança: {confidence})\")\n",
        "        print(f\"  Base: {info['size']} exemplos, {probability:.1%} Iris-setosa\")\n",
        "        print()\n",
        "\n",
        "print(\"Essas regras capturam o conhecimento extraído dos dados de forma interpretável!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo Final do Exercício 4\n",
        "\n",
        "### ✅ Implementação Completa Conforme Especificações\n",
        "\n",
        "**Todos os requisitos do guião foram cumpridos:**\n",
        "\n",
        "1. **Análise de entropia sem bibliotecas de AA** - Implementação do zero\n",
        "2. **Foco na classe Iris-setosa como alvo** - Problema binário bem definido\n",
        "3. **Particionamento por features discretizadas** - Análise detalhada de subconjuntos\n",
        "4. **Cálculo de ganho de informação** - Para todas as 4 features\n",
        "5. **Explicação da construção de árvores de decisão** - Algoritmo ID3 detalhado\n",
        "\n",
        "### 🎯 Principais Descobertas\n",
        "\n",
        "1. **Entropia do Conjunto Original**:\n",
        "   - Entropia = 0.918 (impureza moderada)\n",
        "   - Distribuição: 33% Iris-setosa, 67% outras classes\n",
        "   - Indica necessidade de particionamento\n",
        "\n",
        "2. **Ranking de Features por Ganho**:\n",
        "   - **1º lugar**: Feature com maior discriminação\n",
        "   - Ganho significativo indica boa separabilidade\n",
        "   - Diferenças claras entre features\n",
        "\n",
        "3. **Análise dos Subconjuntos**:\n",
        "   - Alguns subconjuntos alcançam pureza completa\n",
        "   - Redução significativa da entropia média\n",
        "   - Base sólida para árvore de decisão\n",
        "\n",
        "### 🌳 Construção de Árvores de Decisão\n",
        "\n",
        "**Algoritmo ID3 Demonstrado**:\n",
        "1. **Seleção**: Feature com maior ganho de informação\n",
        "2. **Particionamento**: Divisão pelos valores discretos\n",
        "3. **Recursão**: Repetir processo nos subnós impuros\n",
        "4. **Parada**: Entropia zero ou critérios atingidos\n",
        "\n",
        "**Vantagens Identificadas**:\n",
        "- Interpretabilidade excelente (regras claras)\n",
        "- Identificação automática de features importantes\n",
        "- Funcionamento natural com dados categóricos\n",
        "- Robustez a outliers\n",
        "\n",
        "### 📊 Insights sobre Ganho de Informação\n",
        "\n",
        "**Significado Prático**:\n",
        "- Mede redução da incerteza após particionamento\n",
        "- Quantifica valor discriminativo de cada feature\n",
        "- Orienta decisões de construção da árvore\n",
        "- Base matemática sólida para seleção de features\n",
        "\n",
        "**Fórmula Aplicada**:\n",
        "```\n",
        "gain(S, feature) = entropy(S) - Σ(|Sv|/|S|) × entropy(Sv)\n",
        "```\n",
        "\n",
        "### 🎓 Valor Educacional\n",
        "\n",
        "Este exercício demonstrou:\n",
        "- **Fundamentos teóricos**: Conceitos de entropia e informação\n",
        "- **Aplicação prática**: Construção passo-a-passo de árvores\n",
        "- **Análise crítica**: Vantagens e limitações do método\n",
        "- **Comparação**: Posicionamento versus outros algoritmos\n",
        "\n",
        "### 🔄 Comparação com Exercícios Anteriores\n",
        "\n",
        "| Aspecto | Árvores Decisão | k-NN | Naive Bayes |\n",
        "|---------|-----------------|------|-------------|\n",
        "| **Interpretabilidade** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |\n",
        "| **Performance** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
        "| **Velocidade** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
        "| **Simplicidade** | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
        "\n",
        "### 💡 Regras Extraídas\n",
        "\n",
        "A análise produziu regras interpretáveis do tipo:\n",
        "- **SE** feature ≤ threshold **ENTÃO** classe com probabilidade X\n",
        "- Regras baseadas em evidência estatística\n",
        "- Facilmente aplicáveis por humanos\n",
        "- Transparentes e auditáveis\n",
        "\n",
        "### 🔮 Extensões Possíveis\n",
        "\n",
        "**Melhorias Implementáveis**:\n",
        "- Critérios alternativos (Gini, Gain Ratio)\n",
        "- Poda da árvore para evitar overfitting\n",
        "- Tratamento de valores ausentes\n",
        "- Árvores multiclasse completas\n",
        "\n",
        "**Exercício 4 (Entropia e Ganho de Informação) completado com sucesso! 🏆**\n",
        "\n",
        "### 🏁 Conclusão Geral dos Exercícios\n",
        "\n",
        "**Jornada Completa de Aprendizagem**:\n",
        "1. **Perceptrão**: Fundamentos de redes neurais\n",
        "2. **k-NN**: Métodos baseados em instâncias  \n",
        "3. **Naive Bayes**: Abordagens probabilísticas\n",
        "4. **Árvores de Decisão**: Métodos baseados em regras\n",
        "\n",
        "**Todos implementados do zero, analisados rigorosamente e comparados estatisticamente! 🎯**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
