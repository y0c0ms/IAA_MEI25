{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exerc√≠cio 4: An√°lise de Entropia e Ganho de Informa√ß√£o\n",
        "\n",
        "Este notebook implementa a an√°lise de entropia e ganho de informa√ß√£o conforme especificado no gui√£o, seguindo todos os requisitos:\n",
        "- C√°lculo de entropia sem bibliotecas de algoritmos de AA\n",
        "- An√°lise com foco na classe Iris-setosa como alvo\n",
        "- Particionamento por features discretizadas\n",
        "- Explica√ß√£o da constru√ß√£o de √°rvores de decis√£o\n",
        "\n",
        "## Fundamentos Te√≥ricos\n",
        "\n",
        "### Entropia\n",
        "A entropia mede a \"impureza\" ou \"desordem\" de um conjunto de dados:\n",
        "\n",
        "**entropy(S) = -p‚Çä √ó log‚ÇÇ(p‚Çä) - p‚Çã √ó log‚ÇÇ(p‚Çã)**\n",
        "\n",
        "Onde:\n",
        "- p‚Çä = propor√ß√£o de exemplos positivos\n",
        "- p‚Çã = propor√ß√£o de exemplos negativos\n",
        "\n",
        "### Ganho de Informa√ß√£o\n",
        "O ganho de informa√ß√£o mede quanto uma feature reduz a entropia:\n",
        "\n",
        "**gain(S, a) = entropy(S) - Œ£(|S·µ•|/|S|) √ó entropy(S·µ•)**\n",
        "\n",
        "Onde S·µ• s√£o os subconjuntos criados pelos valores v da feature a.\n",
        "\n",
        "**Objetivo**: Analisar qual feature oferece maior ganho para distinguir Iris-setosa das demais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Configura√ß√£o\n",
        "np.random.seed(42)\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "def load_iris_data(filepath):\n",
        "    \"\"\"Carrega o dataset Iris\"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(',')\n",
        "                if len(parts) == 5:\n",
        "                    features = [float(x) for x in parts[:4]]\n",
        "                    label = parts[4]\n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "    \n",
        "    X = np.array(data)\n",
        "    \n",
        "    # Converter labels para n√∫meros\n",
        "    unique_labels = list(set(labels))\n",
        "    unique_labels.sort()\n",
        "    \n",
        "    label_to_num = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y = np.array([label_to_num[label] for label in labels])\n",
        "    \n",
        "    return X, y, unique_labels\n",
        "\n",
        "def discretize_features(X, method='tercis'):\n",
        "    \"\"\"Discretiza features cont√≠nuas em categorias low/medium/high\"\"\"\n",
        "    X_discretized = np.zeros_like(X, dtype=int)\n",
        "    thresholds = {}\n",
        "    \n",
        "    for feature_idx in range(X.shape[1]):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        \n",
        "        if method == 'tercis':\n",
        "            threshold_low = np.percentile(feature_values, 33.33)\n",
        "            threshold_high = np.percentile(feature_values, 66.67)\n",
        "        else:\n",
        "            raise ValueError(f\"M√©todo '{method}' n√£o reconhecido\")\n",
        "        \n",
        "        discretized_feature = np.zeros(len(feature_values), dtype=int)\n",
        "        discretized_feature[feature_values <= threshold_low] = 0  # low\n",
        "        discretized_feature[(feature_values > threshold_low) & (feature_values <= threshold_high)] = 1  # medium  \n",
        "        discretized_feature[feature_values > threshold_high] = 2  # high\n",
        "        \n",
        "        X_discretized[:, feature_idx] = discretized_feature\n",
        "        thresholds[feature_idx] = (threshold_low, threshold_high)\n",
        "    \n",
        "    return X_discretized, thresholds\n",
        "\n",
        "def create_binary_target(y, target_class=0):\n",
        "    \"\"\"\n",
        "    Cria target bin√°rio: classe alvo vs. todas as outras\n",
        "    \n",
        "    Args:\n",
        "        y: array de labels originais\n",
        "        target_class: classe a ser considerada positiva (default: 0 = Iris-setosa)\n",
        "    \n",
        "    Returns:\n",
        "        y_binary: array bin√°rio (1 = classe alvo, 0 = outras classes)\n",
        "    \"\"\"\n",
        "    y_binary = (y == target_class).astype(int)\n",
        "    return y_binary\n",
        "\n",
        "def calculate_entropy(y_binary):\n",
        "    \"\"\"\n",
        "    Calcula entropia de um conjunto com classes bin√°rias\n",
        "    \n",
        "    Args:\n",
        "        y_binary: array bin√°rio de labels\n",
        "    \n",
        "    Returns:\n",
        "        entropy: valor da entropia\n",
        "    \"\"\"\n",
        "    if len(y_binary) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Contar classes positivas e negativas\n",
        "    n_positive = np.sum(y_binary == 1)\n",
        "    n_negative = np.sum(y_binary == 0)\n",
        "    total = len(y_binary)\n",
        "    \n",
        "    # Propor√ß√µes\n",
        "    p_positive = n_positive / total\n",
        "    p_negative = n_negative / total\n",
        "    \n",
        "    # Calcular entropia (evitar log(0))\n",
        "    entropy = 0\n",
        "    if p_positive > 0:\n",
        "        entropy -= p_positive * math.log2(p_positive)\n",
        "    if p_negative > 0:\n",
        "        entropy -= p_negative * math.log2(p_negative)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X_discretized, y_binary, feature_idx):\n",
        "    \"\"\"\n",
        "    Calcula o ganho de informa√ß√£o para uma feature espec√≠fica\n",
        "    \n",
        "    Args:\n",
        "        X_discretized: features discretizadas\n",
        "        y_binary: labels bin√°rios\n",
        "        feature_idx: √≠ndice da feature a analisar\n",
        "    \n",
        "    Returns:\n",
        "        gain: ganho de informa√ß√£o\n",
        "        subsets_info: informa√ß√µes sobre os subconjuntos criados\n",
        "    \"\"\"\n",
        "    # Entropia do conjunto original\n",
        "    original_entropy = calculate_entropy(y_binary)\n",
        "    \n",
        "    # Obter valores √∫nicos da feature\n",
        "    feature_values = X_discretized[:, feature_idx]\n",
        "    unique_values = np.unique(feature_values)\n",
        "    \n",
        "    # Calcular entropia ponderada dos subconjuntos\n",
        "    weighted_entropy = 0\n",
        "    total_samples = len(y_binary)\n",
        "    subsets_info = {}\n",
        "    \n",
        "    for value in unique_values:\n",
        "        # Criar subconjunto para este valor da feature\n",
        "        subset_mask = (feature_values == value)\n",
        "        subset_y = y_binary[subset_mask]\n",
        "        subset_size = len(subset_y)\n",
        "        \n",
        "        # Calcular entropia do subconjunto\n",
        "        subset_entropy = calculate_entropy(subset_y)\n",
        "        \n",
        "        # Adicionar √† entropia ponderada\n",
        "        weight = subset_size / total_samples\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        \n",
        "        # Armazenar informa√ß√µes do subconjunto\n",
        "        n_positive = np.sum(subset_y == 1)\n",
        "        n_negative = np.sum(subset_y == 0)\n",
        "        \n",
        "        subsets_info[value] = {\n",
        "            'size': subset_size,\n",
        "            'entropy': subset_entropy,\n",
        "            'weight': weight,\n",
        "            'n_positive': n_positive,\n",
        "            'n_negative': n_negative,\n",
        "            'proportion_positive': n_positive / subset_size if subset_size > 0 else 0\n",
        "        }\n",
        "    \n",
        "    # Calcular ganho de informa√ß√£o\n",
        "    information_gain = original_entropy - weighted_entropy\n",
        "    \n",
        "    return information_gain, subsets_info\n",
        "\n",
        "# Carregar e preparar dados\n",
        "print(\"=== CARREGAMENTO E PREPARA√á√ÉO DOS DADOS ===\")\n",
        "\n",
        "# Carregar dados\n",
        "X_continuous, y, class_names = load_iris_data('iris/iris.data')\n",
        "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
        "\n",
        "print(f\"Dataset carregado: {X_continuous.shape}\")\n",
        "print(f\"Classes originais: {class_names}\")\n",
        "\n",
        "# Discretizar features (reutilizando do exerc√≠cio anterior)\n",
        "X_discretized, thresholds = discretize_features(X_continuous, method='tercis')\n",
        "\n",
        "# Criar target bin√°rio: Iris-setosa vs. outras classes\n",
        "y_binary = create_binary_target(y, target_class=0)  # 0 = Iris-setosa\n",
        "\n",
        "print(f\"\\n=== DEFINI√á√ÉO DO PROBLEMA BIN√ÅRIO ===\")\n",
        "print(f\"Classe alvo (positiva): {class_names[0]} (Iris-setosa)\")\n",
        "print(f\"Outras classes (negativa): {class_names[1]}, {class_names[2]}\")\n",
        "\n",
        "# Contar distribui√ß√£o\n",
        "n_positive = np.sum(y_binary == 1)\n",
        "n_negative = np.sum(y_binary == 0)\n",
        "total = len(y_binary)\n",
        "\n",
        "print(f\"\\nDistribui√ß√£o:\")\n",
        "print(f\"  Iris-setosa (p+): {n_positive} exemplos ({n_positive/total*100:.1f}%)\")\n",
        "print(f\"  Outras classes (p-): {n_negative} exemplos ({n_negative/total*100:.1f}%)\")\n",
        "\n",
        "# Calcular entropia do conjunto completo\n",
        "print(f\"\\n=== ENTROPIA DO CONJUNTO COMPLETO ===\")\n",
        "original_entropy = calculate_entropy(y_binary)\n",
        "print(f\"Entropia(S) = {original_entropy:.4f}\")\n",
        "\n",
        "# Interpretar o valor da entropia\n",
        "if original_entropy == 0:\n",
        "    interpretation = \"Conjunto puro (todas as amostras da mesma classe)\"\n",
        "elif original_entropy == 1:\n",
        "    interpretation = \"M√°xima impureza (distribui√ß√£o 50/50)\"\n",
        "else:\n",
        "    interpretation = f\"Impureza moderada (mais pr√≥ximo de {'puro' if original_entropy < 0.5 else 'impuro'})\"\n",
        "\n",
        "print(f\"Interpreta√ß√£o: {interpretation}\")\n",
        "\n",
        "# Mostrar f√≥rmula de c√°lculo\n",
        "p_pos = n_positive / total\n",
        "p_neg = n_negative / total\n",
        "print(f\"\\nC√°lculo detalhado:\")\n",
        "print(f\"  p+ = {n_positive}/{total} = {p_pos:.3f}\")\n",
        "print(f\"  p- = {n_negative}/{total} = {p_neg:.3f}\")\n",
        "print(f\"  entropy(S) = -({p_pos:.3f} √ó log‚ÇÇ({p_pos:.3f})) - ({p_neg:.3f} √ó log‚ÇÇ({p_neg:.3f}))\")\n",
        "print(f\"  entropy(S) = -({p_pos:.3f} √ó {math.log2(p_pos):.3f}) - ({p_neg:.3f} √ó {math.log2(p_neg):.3f})\")\n",
        "print(f\"  entropy(S) = {-p_pos * math.log2(p_pos):.4f} + {-p_neg * math.log2(p_neg):.4f} = {original_entropy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise detalhada: parti√ß√£o pela primeira feature (Sepal Length)\n",
        "print(f\"\\n=== EXEMPLO DETALHADO: PARTI√á√ÉO POR {feature_names[0].upper()} ===\")\n",
        "\n",
        "feature_idx = 0  # Sepal Length\n",
        "gain, subsets_info = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "\n",
        "print(f\"Analisando parti√ß√£o por {feature_names[feature_idx]}:\")\n",
        "\n",
        "# Mostrar limiares de discretiza√ß√£o\n",
        "low_thresh, high_thresh = thresholds[feature_idx]\n",
        "print(f\"Limiares de discretiza√ß√£o:\")\n",
        "print(f\"  Low: ‚â§ {low_thresh:.2f}\")\n",
        "print(f\"  Medium: {low_thresh:.2f} < valor ‚â§ {high_thresh:.2f}\")\n",
        "print(f\"  High: > {high_thresh:.2f}\")\n",
        "\n",
        "print(f\"\\nSubconjuntos criados:\")\n",
        "value_names = ['Low', 'Medium', 'High']\n",
        "\n",
        "for value in [0, 1, 2]:  # low, medium, high\n",
        "    if value in subsets_info:\n",
        "        info = subsets_info[value]\n",
        "        print(f\"\\n{value_names[value]} Dataset:\")\n",
        "        print(f\"  Tamanho: {info['size']} exemplos ({info['weight']*100:.1f}% do total)\")\n",
        "        print(f\"  Iris-setosa: {info['n_positive']} exemplos\")\n",
        "        print(f\"  Outras: {info['n_negative']} exemplos\")\n",
        "        print(f\"  Propor√ß√£o Iris-setosa: {info['proportion_positive']:.3f}\")\n",
        "        print(f\"  Entropia: {info['entropy']:.4f}\")\n",
        "\n",
        "# Calcular entropia ponderada\n",
        "weighted_entropy = sum(info['weight'] * info['entropy'] for info in subsets_info.values())\n",
        "print(f\"\\nEntropia ponderada dos subconjuntos:\")\n",
        "print(f\"  Œ£ (|Sv|/|S|) √ó entropy(Sv) = {weighted_entropy:.4f}\")\n",
        "\n",
        "print(f\"\\nGanho de informa√ß√£o:\")\n",
        "print(f\"  gain(S, {feature_names[feature_idx]}) = {original_entropy:.4f} - {weighted_entropy:.4f} = {gain:.4f}\")\n",
        "\n",
        "print(f\"\\n=== AN√ÅLISE COMPLETA: TODAS AS FEATURES ===\")\n",
        "\n",
        "# Calcular ganho para todas as features\n",
        "gains_info = {}\n",
        "\n",
        "for feature_idx in range(len(feature_names)):\n",
        "    gain, subsets_info = calculate_information_gain(X_discretized, y_binary, feature_idx)\n",
        "    gains_info[feature_idx] = {\n",
        "        'gain': gain,\n",
        "        'subsets': subsets_info,\n",
        "        'feature_name': feature_names[feature_idx]\n",
        "    }\n",
        "\n",
        "# Mostrar resultados ordenados por ganho\n",
        "print(\"Ganho de informa√ß√£o por feature:\")\n",
        "print(f\"{'Feature':<15} {'Ganho':<8} {'Ranking'}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "# Ordenar por ganho (maior primeiro)\n",
        "sorted_features = sorted(gains_info.items(), key=lambda x: x[1]['gain'], reverse=True)\n",
        "\n",
        "for rank, (feature_idx, info) in enumerate(sorted_features, 1):\n",
        "    feature_name = info['feature_name']\n",
        "    gain = info['gain']\n",
        "    print(f\"{feature_name:<15} {gain:<8.4f} {rank}\")\n",
        "\n",
        "# Feature com maior ganho\n",
        "best_feature_idx, best_info = sorted_features[0]\n",
        "best_feature_name = best_info['feature_name']\n",
        "best_gain = best_info['gain']\n",
        "\n",
        "print(f\"\\n**MELHOR FEATURE**: {best_feature_name} (ganho = {best_gain:.4f})\")\n",
        "\n",
        "# An√°lise detalhada da melhor feature\n",
        "print(f\"\\n=== AN√ÅLISE DETALHADA: {best_feature_name.upper()} ===\")\n",
        "\n",
        "best_subsets = best_info['subsets']\n",
        "print(f\"Particionamento por {best_feature_name}:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        purity = max(info['proportion_positive'], 1 - info['proportion_positive'])\n",
        "        purity_desc = \"Muito puro\" if purity > 0.9 else \"Puro\" if purity > 0.7 else \"Impuro\"\n",
        "        \n",
        "        print(f\"\\n{value_names[value]}:\")\n",
        "        print(f\"  Tamanho: {info['size']} exemplos\")\n",
        "        print(f\"  Entropia: {info['entropy']:.4f}\")\n",
        "        print(f\"  Pureza: {purity:.3f} ({purity_desc})\")\n",
        "        \n",
        "        if info['n_positive'] == info['size']:\n",
        "            print(f\"  ‚Üí Todos s√£o Iris-setosa!\")\n",
        "        elif info['n_negative'] == info['size']:\n",
        "            print(f\"  ‚Üí Nenhum √© Iris-setosa!\")\n",
        "        else:\n",
        "            print(f\"  ‚Üí Misto: {info['n_positive']} Iris-setosa, {info['n_negative']} outras\")\n",
        "\n",
        "# Visualiza√ß√£o dos ganhos\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('An√°lise de Ganho de Informa√ß√£o para √Årvores de Decis√£o', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Gr√°fico 1: Ganho por feature\n",
        "ax1 = axes[0, 0]\n",
        "features = [gains_info[i]['feature_name'] for i in range(4)]\n",
        "gains = [gains_info[i]['gain'] for i in range(4)]\n",
        "colors = ['gold' if i == best_feature_idx else 'lightblue' for i in range(4)]\n",
        "\n",
        "bars = ax1.bar(features, gains, color=colors, edgecolor='black')\n",
        "ax1.set_title('Ganho de Informa√ß√£o por Feature')\n",
        "ax1.set_ylabel('Ganho de Informa√ß√£o')\n",
        "ax1.set_xticklabels(features, rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar valores nas barras\n",
        "for bar, gain in zip(bars, gains):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{gain:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Gr√°fico 2: Entropia dos subconjuntos da melhor feature\n",
        "ax2 = axes[0, 1]\n",
        "subset_names = [f'{value_names[v]}' for v in [0, 1, 2] if v in best_subsets]\n",
        "subset_entropies = [best_subsets[v]['entropy'] for v in [0, 1, 2] if v in best_subsets]\n",
        "subset_sizes = [best_subsets[v]['size'] for v in [0, 1, 2] if v in best_subsets]\n",
        "\n",
        "bars2 = ax2.bar(subset_names, subset_entropies, color=['lightcoral', 'lightgreen', 'lightyellow'])\n",
        "ax2.set_title(f'Entropia dos Subconjuntos - {best_feature_name}')\n",
        "ax2.set_ylabel('Entropia')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Adicionar tamanhos dos subconjuntos\n",
        "for bar, entropy, size in zip(bars2, subset_entropies, subset_sizes):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "             f'{entropy:.3f}\\n(n={size})', ha='center', va='bottom')\n",
        "\n",
        "# Gr√°fico 3: Distribui√ß√£o de classes na melhor feature\n",
        "ax3 = axes[1, 0]\n",
        "x_pos = np.arange(len(subset_names))\n",
        "positives = [best_subsets[v]['n_positive'] for v in [0, 1, 2] if v in best_subsets]\n",
        "negatives = [best_subsets[v]['n_negative'] for v in [0, 1, 2] if v in best_subsets]\n",
        "\n",
        "width = 0.35\n",
        "bars3a = ax3.bar(x_pos - width/2, positives, width, label='Iris-setosa', color='red', alpha=0.7)\n",
        "bars3b = ax3.bar(x_pos + width/2, negatives, width, label='Outras classes', color='blue', alpha=0.7)\n",
        "\n",
        "ax3.set_title(f'Distribui√ß√£o de Classes - {best_feature_name}')\n",
        "ax3.set_ylabel('N√∫mero de Exemplos')\n",
        "ax3.set_xticks(x_pos)\n",
        "ax3.set_xticklabels(subset_names)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Gr√°fico 4: Compara√ß√£o visual das entropias\n",
        "ax4 = axes[1, 1]\n",
        "all_entropies = [original_entropy] + [gains_info[i]['subsets'][v]['entropy'] \n",
        "                                     for i in range(4) for v in [0, 1, 2] \n",
        "                                     if v in gains_info[i]['subsets']]\n",
        "labels = ['Original'] + [f'{gains_info[i][\"feature_name\"][:4]}-{value_names[v]}' \n",
        "                        for i in range(4) for v in [0, 1, 2] \n",
        "                        if v in gains_info[i]['subsets']]\n",
        "\n",
        "# Destacar conjunto original e subconjuntos da melhor feature\n",
        "colors_entropy = ['red'] + ['gold' if label.startswith(best_feature_name[:4]) else 'lightgray' \n",
        "                           for label in labels[1:]]\n",
        "\n",
        "bars4 = ax4.bar(range(len(all_entropies)), all_entropies, color=colors_entropy, alpha=0.7)\n",
        "ax4.set_title('Compara√ß√£o de Entropias')\n",
        "ax4.set_ylabel('Entropia')\n",
        "ax4.set_xticks(range(len(labels)))\n",
        "ax4.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== INTERPRETA√á√ÉO DOS RESULTADOS ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**CAPACIDADE DISCRIMINATIVA**:\n",
        "\n",
        "1. **Antes da parti√ß√£o** (conjunto S):\n",
        "   - Entropia = {original_entropy:.4f}\n",
        "   - Indica impureza moderada (mistura de classes)\n",
        "\n",
        "2. **Ap√≥s parti√ß√£o por {best_feature_name}**:\n",
        "   - Ganho = {best_gain:.4f}\n",
        "   - Redu√ß√£o significativa na entropia m√©dia\n",
        "   - Melhora na capacidade de classifica√ß√£o\n",
        "\n",
        "**O QUE ISSO SIGNIFICA**:\n",
        "- {best_feature_name} √© a feature mais informativa para distinguir Iris-setosa\n",
        "- Conhecer o valor de {best_feature_name} reduz significativamente a incerteza\n",
        "- Esta seria a melhor escolha para o n√≥ raiz de uma √°rvore de decis√£o\n",
        "\n",
        "**RANKING DE IMPORT√ÇNCIA**:\"\"\")\n",
        "\n",
        "for rank, (feature_idx, info) in enumerate(sorted_features, 1):\n",
        "    effectiveness = \"Excelente\" if info['gain'] > 0.5 else \"Boa\" if info['gain'] > 0.3 else \"Moderada\" if info['gain'] > 0.1 else \"Fraca\"\n",
        "    print(f\"  {rank}¬∫. {info['feature_name']}: {info['gain']:.4f} ({effectiveness})\")\n",
        "\n",
        "print(f\"\\n**POTENCIAL DE CLASSIFICA√á√ÉO**:\")\n",
        "improvement = (1 - weighted_entropy / original_entropy) * 100\n",
        "print(f\"- Redu√ß√£o da entropia: {improvement:.1f}%\")\n",
        "print(f\"- Melhoria na pureza dos subconjuntos\")\n",
        "print(f\"- Base s√≥lida para constru√ß√£o de √°rvore de decis√£o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constru√ß√£o de √°rvores de decis√£o usando esta estrat√©gia\n",
        "print(f\"\\n=== COMO CONSTRUIR UMA √ÅRVORE DE DECIS√ÉO ===\")\n",
        "\n",
        "def build_decision_tree_explanation():\n",
        "    \"\"\"Explica o algoritmo de constru√ß√£o de √°rvores de decis√£o\"\"\"\n",
        "    \n",
        "    print(\"\"\"\n",
        "**ALGORITMO ID3 (Iterative Dichotomiser 3)**:\n",
        "\n",
        "1. **N√≥ Raiz**:\n",
        "   - Come√ßar com o conjunto completo S\n",
        "   - Calcular entropia de S\n",
        "   - Se entropia = 0 ‚Üí n√≥ folha (classe pura)\n",
        "   - Sen√£o ‚Üí continuar particionamento\n",
        "\n",
        "2. **Sele√ß√£o da Feature**:\n",
        "   - Calcular ganho de informa√ß√£o para todas as features\n",
        "   - Escolher a feature com MAIOR ganho\n",
        "   - Esta torna-se o crit√©rio de divis√£o do n√≥\n",
        "\n",
        "3. **Particionamento**:\n",
        "   - Dividir o conjunto pelos valores da feature escolhida\n",
        "   - Criar um ramo para cada valor (Low, Medium, High)\n",
        "   - Cada ramo recebe o subconjunto correspondente\n",
        "\n",
        "4. **Recurs√£o**:\n",
        "   - Para cada subconjunto criado:\n",
        "     * Se puro (entropia = 0) ‚Üí criar n√≥ folha\n",
        "     * Se impuro ‚Üí repetir processo (voltar ao passo 1)\n",
        "     * Se n√£o h√° mais features ‚Üí classe majorit√°ria\n",
        "\n",
        "5. **Crit√©rios de Parada**:\n",
        "   - Entropia = 0 (conjunto puro)\n",
        "   - N√£o h√° mais features para dividir\n",
        "   - N√∫mero m√≠nimo de exemplos atingido\n",
        "   - Profundidade m√°xima alcan√ßada\n",
        "\"\"\")\n",
        "\n",
        "build_decision_tree_explanation()\n",
        "\n",
        "# Demonstra√ß√£o pr√°tica: constru√ß√£o da primeira camada\n",
        "print(f\"=== DEMONSTRA√á√ÉO: PRIMEIRA CAMADA DA √ÅRVORE ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**PASSO A PASSO PARA NOSSO CASO**:\n",
        "\n",
        "1. **N√≥ Raiz**:\n",
        "   - Dataset completo: 150 exemplos\n",
        "   - Entropia inicial: {original_entropy:.4f}\n",
        "   - Objetivo: classificar Iris-setosa vs. outras\n",
        "\n",
        "2. **Escolha da Feature**:\n",
        "   - Testamos todas as 4 features\n",
        "   - {best_feature_name} tem maior ganho: {best_gain:.4f}\n",
        "   - DECIS√ÉO: usar {best_feature_name} como n√≥ raiz\n",
        "\n",
        "3. **Cria√ß√£o dos Ramos**:\"\"\")\n",
        "\n",
        "# Mostrar a estrutura da primeira camada\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        if info['entropy'] == 0:\n",
        "            if info['n_positive'] == info['size']:\n",
        "                decision = \"‚Üí FOLHA: Iris-setosa\"\n",
        "            else:\n",
        "                decision = \"‚Üí FOLHA: Outras classes\"\n",
        "        else:\n",
        "            decision = f\"‚Üí CONTINUAR (entropia={info['entropy']:.4f})\"\n",
        "        \n",
        "        print(f\"   {best_feature_name} = {value_name}: {info['size']} exemplos {decision}\")\n",
        "\n",
        "# Visualizar a √°rvore textualmente\n",
        "print(f\"\\n=== REPRESENTA√á√ÉO DA √ÅRVORE (PRIMEIRA CAMADA) ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "                    [RAIZ]\n",
        "               {best_feature_name} = ?\n",
        "               Entropia: {original_entropy:.4f}\n",
        "                 /      |      \\\\\n",
        "                /       |       \\\\\n",
        "            Low        Medium     High\n",
        "       ({best_subsets[0]['size']} exemplos)   ({best_subsets[1]['size']} exemplos)    ({best_subsets[2]['size']} exemplos)\n",
        "     Ent: {best_subsets[0]['entropy']:.3f}    Ent: {best_subsets[1]['entropy']:.3f}     Ent: {best_subsets[2]['entropy']:.3f}\n",
        "\"\"\")\n",
        "\n",
        "# An√°lise dos pr√≥ximos passos\n",
        "print(f\"**PR√ìXIMOS PASSOS**:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        if info['entropy'] == 0:\n",
        "            print(f\"‚Ä¢ Ramo {value_name}: PARAR (pureza alcan√ßada)\")\n",
        "        else:\n",
        "            print(f\"‚Ä¢ Ramo {value_name}: CONTINUAR particionamento\")\n",
        "            print(f\"  - Calcular ganho das features restantes\")\n",
        "            print(f\"  - Escolher melhor feature para subdividir\")\n",
        "            print(f\"  - Criar novos subn√≥s\")\n",
        "\n",
        "# Vantagens e limita√ß√µes\n",
        "print(f\"\\n=== VANTAGENS E LIMITA√á√ïES ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**VANTAGENS DO M√âTODO**:\n",
        "‚Ä¢ Interpretabilidade: regras claras e leg√≠veis\n",
        "‚Ä¢ N√£o assume distribui√ß√£o espec√≠fica dos dados\n",
        "‚Ä¢ Lida bem com features categ√≥ricas\n",
        "‚Ä¢ Identifica automaticamente features importantes\n",
        "‚Ä¢ Robusto a outliers\n",
        "\n",
        "**LIMITA√á√ïES**:\n",
        "‚Ä¢ Tend√™ncia ao overfitting (√°rvores muito profundas)\n",
        "‚Ä¢ Instabilidade (pequenas mudan√ßas ‚Üí √°rvores diferentes)\n",
        "‚Ä¢ Bias para features com mais valores\n",
        "‚Ä¢ Dificuldade com rela√ß√µes lineares complexas\n",
        "‚Ä¢ Pode criar √°rvores desbalanceadas\n",
        "\n",
        "**MELHORIAS POSS√çVEIS**:\n",
        "‚Ä¢ Poda da √°rvore (pruning)\n",
        "‚Ä¢ Crit√©rios alternativos (Gini, gain ratio)\n",
        "‚Ä¢ Ensemble methods (Random Forest)\n",
        "‚Ä¢ Regulariza√ß√£o (profundidade m√°xima, min samples)\n",
        "\"\"\")\n",
        "\n",
        "# Compara√ß√£o com outros m√©todos\n",
        "print(f\"\\n=== COMPARA√á√ÉO COM OUTROS ALGORITMOS ===\")\n",
        "\n",
        "print(f\"\"\"\n",
        "**√ÅRVORES DE DECIS√ÉO vs. ALGORITMOS ANTERIORES**:\n",
        "\n",
        "| Aspecto              | √Årvore Decis√£o | k-NN      | Naive Bayes |\n",
        "|---------------------|----------------|-----------|-------------|\n",
        "| Interpretabilidade   | Excelente      | Fraca     | Boa         |\n",
        "| Velocidade (predi√ß√£o)| Muito r√°pida   | Lenta     | R√°pida      |\n",
        "| Dados categ√≥ricos   | Nativa         | Problemas | Boa         |\n",
        "| Overfitting         | Alto risco     | M√©dio     | Baixo       |\n",
        "| Prepara√ß√£o dados    | M√≠nima         | Escala    | Discretiza√ß√£o|\n",
        "\n",
        "**QUANDO USAR √ÅRVORES**:\n",
        "‚Ä¢ Quando interpretabilidade √© crucial\n",
        "‚Ä¢ Dados com mix de tipos (num√©rico + categ√≥rico)  \n",
        "‚Ä¢ Necessidade de regras explicitas\n",
        "‚Ä¢ Base para algoritmos ensemble\n",
        "\"\"\")\n",
        "\n",
        "# Exemplo de regras extra√≠das\n",
        "print(f\"\\n=== REGRAS EXTRA√çDAS DA AN√ÅLISE ===\")\n",
        "\n",
        "print(\"Regras para classificar Iris-setosa:\")\n",
        "\n",
        "for value in [0, 1, 2]:\n",
        "    if value in best_subsets:\n",
        "        info = best_subsets[value]\n",
        "        value_name = value_names[value]\n",
        "        \n",
        "        low_thresh, high_thresh = thresholds[best_feature_idx]\n",
        "        \n",
        "        if value == 0:  # Low\n",
        "            condition = f\"{best_feature_name} ‚â§ {low_thresh:.2f}\"\n",
        "        elif value == 1:  # Medium  \n",
        "            condition = f\"{low_thresh:.2f} < {best_feature_name} ‚â§ {high_thresh:.2f}\"\n",
        "        else:  # High\n",
        "            condition = f\"{best_feature_name} > {high_thresh:.2f}\"\n",
        "        \n",
        "        probability = info['proportion_positive']\n",
        "        \n",
        "        if probability >= 0.9:\n",
        "            prediction = \"Iris-setosa\"\n",
        "            confidence = \"Alta\"\n",
        "        elif probability >= 0.5:\n",
        "            prediction = \"Provavelmente Iris-setosa\"\n",
        "            confidence = \"M√©dia\"\n",
        "        elif probability >= 0.1:\n",
        "            prediction = \"Possivelmente outras classes\"\n",
        "            confidence = \"M√©dia\"\n",
        "        else:\n",
        "            prediction = \"Outras classes\"\n",
        "            confidence = \"Alta\"\n",
        "        \n",
        "        print(f\"‚Ä¢ SE {condition}\")\n",
        "        print(f\"  ENT√ÉO {prediction} (confian√ßa: {confidence})\")\n",
        "        print(f\"  Base: {info['size']} exemplos, {probability:.1%} Iris-setosa\")\n",
        "        print()\n",
        "\n",
        "print(\"Essas regras capturam o conhecimento extra√≠do dos dados de forma interpret√°vel!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo Final do Exerc√≠cio 4\n",
        "\n",
        "### ‚úÖ Implementa√ß√£o Completa Conforme Especifica√ß√µes\n",
        "\n",
        "**Todos os requisitos do gui√£o foram cumpridos:**\n",
        "\n",
        "1. **An√°lise de entropia sem bibliotecas de AA** - Implementa√ß√£o do zero\n",
        "2. **Foco na classe Iris-setosa como alvo** - Problema bin√°rio bem definido\n",
        "3. **Particionamento por features discretizadas** - An√°lise detalhada de subconjuntos\n",
        "4. **C√°lculo de ganho de informa√ß√£o** - Para todas as 4 features\n",
        "5. **Explica√ß√£o da constru√ß√£o de √°rvores de decis√£o** - Algoritmo ID3 detalhado\n",
        "\n",
        "### üéØ Principais Descobertas\n",
        "\n",
        "1. **Entropia do Conjunto Original**:\n",
        "   - Entropia = 0.918 (impureza moderada)\n",
        "   - Distribui√ß√£o: 33% Iris-setosa, 67% outras classes\n",
        "   - Indica necessidade de particionamento\n",
        "\n",
        "2. **Ranking de Features por Ganho**:\n",
        "   - **1¬∫ lugar**: Feature com maior discrimina√ß√£o\n",
        "   - Ganho significativo indica boa separabilidade\n",
        "   - Diferen√ßas claras entre features\n",
        "\n",
        "3. **An√°lise dos Subconjuntos**:\n",
        "   - Alguns subconjuntos alcan√ßam pureza completa\n",
        "   - Redu√ß√£o significativa da entropia m√©dia\n",
        "   - Base s√≥lida para √°rvore de decis√£o\n",
        "\n",
        "### üå≥ Constru√ß√£o de √Årvores de Decis√£o\n",
        "\n",
        "**Algoritmo ID3 Demonstrado**:\n",
        "1. **Sele√ß√£o**: Feature com maior ganho de informa√ß√£o\n",
        "2. **Particionamento**: Divis√£o pelos valores discretos\n",
        "3. **Recurs√£o**: Repetir processo nos subn√≥s impuros\n",
        "4. **Parada**: Entropia zero ou crit√©rios atingidos\n",
        "\n",
        "**Vantagens Identificadas**:\n",
        "- Interpretabilidade excelente (regras claras)\n",
        "- Identifica√ß√£o autom√°tica de features importantes\n",
        "- Funcionamento natural com dados categ√≥ricos\n",
        "- Robustez a outliers\n",
        "\n",
        "### üìä Insights sobre Ganho de Informa√ß√£o\n",
        "\n",
        "**Significado Pr√°tico**:\n",
        "- Mede redu√ß√£o da incerteza ap√≥s particionamento\n",
        "- Quantifica valor discriminativo de cada feature\n",
        "- Orienta decis√µes de constru√ß√£o da √°rvore\n",
        "- Base matem√°tica s√≥lida para sele√ß√£o de features\n",
        "\n",
        "**F√≥rmula Aplicada**:\n",
        "```\n",
        "gain(S, feature) = entropy(S) - Œ£(|Sv|/|S|) √ó entropy(Sv)\n",
        "```\n",
        "\n",
        "### üéì Valor Educacional\n",
        "\n",
        "Este exerc√≠cio demonstrou:\n",
        "- **Fundamentos te√≥ricos**: Conceitos de entropia e informa√ß√£o\n",
        "- **Aplica√ß√£o pr√°tica**: Constru√ß√£o passo-a-passo de √°rvores\n",
        "- **An√°lise cr√≠tica**: Vantagens e limita√ß√µes do m√©todo\n",
        "- **Compara√ß√£o**: Posicionamento versus outros algoritmos\n",
        "\n",
        "### üîÑ Compara√ß√£o com Exerc√≠cios Anteriores\n",
        "\n",
        "| Aspecto | √Årvores Decis√£o | k-NN | Naive Bayes |\n",
        "|---------|-----------------|------|-------------|\n",
        "| **Interpretabilidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Velocidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "| **Simplicidade** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
        "\n",
        "### üí° Regras Extra√≠das\n",
        "\n",
        "A an√°lise produziu regras interpret√°veis do tipo:\n",
        "- **SE** feature ‚â§ threshold **ENT√ÉO** classe com probabilidade X\n",
        "- Regras baseadas em evid√™ncia estat√≠stica\n",
        "- Facilmente aplic√°veis por humanos\n",
        "- Transparentes e audit√°veis\n",
        "\n",
        "### üîÆ Extens√µes Poss√≠veis\n",
        "\n",
        "**Melhorias Implement√°veis**:\n",
        "- Crit√©rios alternativos (Gini, Gain Ratio)\n",
        "- Poda da √°rvore para evitar overfitting\n",
        "- Tratamento de valores ausentes\n",
        "- √Årvores multiclasse completas\n",
        "\n",
        "**Exerc√≠cio 4 (Entropia e Ganho de Informa√ß√£o) completado com sucesso! üèÜ**\n",
        "\n",
        "### üèÅ Conclus√£o Geral dos Exerc√≠cios\n",
        "\n",
        "**Jornada Completa de Aprendizagem**:\n",
        "1. **Perceptr√£o**: Fundamentos de redes neurais\n",
        "2. **k-NN**: M√©todos baseados em inst√¢ncias  \n",
        "3. **Naive Bayes**: Abordagens probabil√≠sticas\n",
        "4. **√Årvores de Decis√£o**: M√©todos baseados em regras\n",
        "\n",
        "**Todos implementados do zero, analisados rigorosamente e comparados estatisticamente! üéØ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
