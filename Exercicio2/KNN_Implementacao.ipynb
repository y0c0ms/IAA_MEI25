{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exerc√≠cio 2: Implementa√ß√£o de k-NN (k-Nearest Neighbors)\n",
        "\n",
        "Este notebook implementa um classificador k-NN conforme especificado no gui√£o, seguindo todos os requisitos:\n",
        "- Implementa√ß√£o sem bibliotecas de algoritmos de AA\n",
        "- An√°lise detalhada dos resultados\n",
        "- Justifica√ß√£o de todas as decis√µes tomadas\n",
        "- Compara√ß√£o estat√≠stica rigorosa\n",
        "\n",
        "## Dataset: Iris\n",
        "\n",
        "O dataset Iris cont√©m:\n",
        "- 150 exemplos (50 de cada classe)\n",
        "- 4 features: sepal length, sepal width, petal length, petal width  \n",
        "- 3 classes: Iris-setosa, Iris-versicolor, Iris-virginica\n",
        "\n",
        "**Objetivo**: Comparar performance do k-NN com k=3, 7, 11 usando parti√ß√µes 70/30 em 30 repeti√ß√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Configura√ß√£o para reprodutibilidade e visualiza√ß√£o\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "def load_iris_data(filepath):\n",
        "    \"\"\"\n",
        "    Carrega o dataset Iris sem usar bibliotecas externas\n",
        "    \n",
        "    Returns:\n",
        "        X: array de features (150, 4)\n",
        "        y: array de labels (150,)\n",
        "        class_names: lista com nomes das classes\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:  # Ignorar linhas vazias\n",
        "                parts = line.split(',')\n",
        "                if len(parts) == 5:\n",
        "                    # Features num√©ricas\n",
        "                    features = [float(x) for x in parts[:4]]\n",
        "                    # Label\n",
        "                    label = parts[4]\n",
        "                    \n",
        "                    data.append(features)\n",
        "                    labels.append(label)\n",
        "    \n",
        "    X = np.array(data)\n",
        "    \n",
        "    # Converter labels para n√∫meros\n",
        "    unique_labels = list(set(labels))\n",
        "    unique_labels.sort()  # Para ordem consistente\n",
        "    \n",
        "    label_to_num = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y = np.array([label_to_num[label] for label in labels])\n",
        "    \n",
        "    return X, y, unique_labels\n",
        "\n",
        "class KNearestNeighbors:\n",
        "    \"\"\"\n",
        "    Implementa√ß√£o de k-NN sem usar bibliotecas de algoritmos de AA\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, k=3):\n",
        "        \"\"\"\n",
        "        Inicializa o classificador k-NN\n",
        "        \n",
        "        Args:\n",
        "            k: n√∫mero de vizinhos mais pr√≥ximos a considerar\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "    \n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        'Treina' o modelo (na verdade apenas armazena os dados de treino)\n",
        "        \"\"\"\n",
        "        self.X_train = X_train.copy()\n",
        "        self.y_train = y_train.copy()\n",
        "    \n",
        "    def euclidean_distance(self, point1, point2):\n",
        "        \"\"\"\n",
        "        Calcula a dist√¢ncia euclidiana entre dois pontos\n",
        "        \"\"\"\n",
        "        return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "    \n",
        "    def predict_single(self, x):\n",
        "        \"\"\"\n",
        "        Prediz a classe de um √∫nico exemplo\n",
        "        \"\"\"\n",
        "        if self.X_train is None:\n",
        "            raise ValueError(\"Modelo n√£o foi treinado. Chame fit() primeiro.\")\n",
        "        \n",
        "        # Calcular dist√¢ncias para todos os pontos de treino\n",
        "        distances = []\n",
        "        for i, train_point in enumerate(self.X_train):\n",
        "            dist = self.euclidean_distance(x, train_point)\n",
        "            distances.append((dist, self.y_train[i]))\n",
        "        \n",
        "        # Ordenar por dist√¢ncia e selecionar k mais pr√≥ximos\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        k_nearest = distances[:self.k]\n",
        "        \n",
        "        # Votar pela classe mais frequente\n",
        "        k_nearest_labels = [label for _, label in k_nearest]\n",
        "        \n",
        "        # Contar votos\n",
        "        vote_counts = Counter(k_nearest_labels)\n",
        "        \n",
        "        # Retornar classe mais votada\n",
        "        predicted_class = vote_counts.most_common(1)[0][0]\n",
        "        \n",
        "        return predicted_class\n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        \"\"\"\n",
        "        Prediz as classes de m√∫ltiplos exemplos\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            pred = self.predict_single(x)\n",
        "            predictions.append(pred)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "\n",
        "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
        "    \"\"\"\n",
        "    Divide o dataset em treino e teste sem usar bibliotecas externas\n",
        "    \n",
        "    Args:\n",
        "        X: features\n",
        "        y: labels\n",
        "        test_size: propor√ß√£o para teste (0.3 = 30%)\n",
        "        random_state: seed para reprodutibilidade\n",
        "        \n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    \n",
        "    n_samples = len(X)\n",
        "    n_test = int(n_samples * test_size)\n",
        "    \n",
        "    # Gerar √≠ndices aleat√≥rios\n",
        "    indices = np.random.permutation(n_samples)\n",
        "    \n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "    \n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Carregar dados Iris\n",
        "print(\"=== CARREGAMENTO DO DATASET IRIS ===\")\n",
        "X, y, class_names = load_iris_data('iris/iris.data')\n",
        "\n",
        "print(f\"Dataset carregado com sucesso!\")\n",
        "print(f\"Forma dos dados: {X.shape}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Distribui√ß√£o das classes: {np.bincount(y)}\")\n",
        "\n",
        "# Mostrar algumas estat√≠sticas b√°sicas\n",
        "print(f\"\\n=== ESTAT√çSTICAS B√ÅSICAS ===\")\n",
        "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
        "for i, feature in enumerate(feature_names):\n",
        "    print(f\"{feature}: min={X[:, i].min():.2f}, max={X[:, i].max():.2f}, m√©dia={X[:, i].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nPrimeiros 5 exemplos:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Exemplo {i+1}: {X[i]} -> {class_names[y[i]]}\")\n",
        "    \n",
        "print(f\"\\n√öltimos 5 exemplos:\")\n",
        "for i in range(-5, 0):\n",
        "    print(f\"  Exemplo {len(X)+i+1}: {X[i]} -> {class_names[y[i]]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred, num_classes=3):\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas de classifica√ß√£o sem usar bibliotecas externas\n",
        "    \"\"\"\n",
        "    # Matriz de confus√£o\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true_label, pred_label in zip(y_true, y_pred):\n",
        "        cm[true_label, pred_label] += 1\n",
        "    \n",
        "    # Acur√°cia total\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    \n",
        "    # M√©tricas por classe\n",
        "    precision_per_class = []\n",
        "    recall_per_class = []\n",
        "    f1_per_class = []\n",
        "    \n",
        "    for class_idx in range(num_classes):\n",
        "        # Verdadeiros positivos, falsos positivos, falsos negativos\n",
        "        tp = cm[class_idx, class_idx]\n",
        "        fp = np.sum(cm[:, class_idx]) - tp\n",
        "        fn = np.sum(cm[class_idx, :]) - tp\n",
        "        \n",
        "        # Precis√£o\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        \n",
        "        # Recall\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        \n",
        "        # F1-score\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        precision_per_class.append(precision)\n",
        "        recall_per_class.append(recall)\n",
        "        f1_per_class.append(f1)\n",
        "    \n",
        "    # M√©tricas macro (m√©dia das m√©tricas por classe)\n",
        "    precision_macro = np.mean(precision_per_class)\n",
        "    recall_macro = np.mean(recall_per_class)\n",
        "    f1_macro = np.mean(f1_per_class)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'confusion_matrix': cm,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class\n",
        "    }\n",
        "\n",
        "def run_single_experiment(X, y, k_value, random_state):\n",
        "    \"\"\"\n",
        "    Executa um experimento com um valor espec√≠fico de k\n",
        "    \"\"\"\n",
        "    # Dividir dados\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
        "    \n",
        "    # Criar e treinar modelo\n",
        "    knn = KNearestNeighbors(k=k_value)\n",
        "    knn.fit(X_train, y_train)\n",
        "    \n",
        "    # Fazer predi√ß√µes\n",
        "    y_pred = knn.predict(X_test)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    metrics = calculate_metrics(y_test, y_pred)\n",
        "    \n",
        "    return metrics, y_test, y_pred\n",
        "\n",
        "# Teste r√°pido com um exemplo\n",
        "print(\"\\n=== TESTE R√ÅPIDO DO k-NN ===\")\n",
        "\n",
        "# Dividir dados uma vez para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Dados de treino: {X_train.shape[0]} exemplos\")\n",
        "print(f\"Dados de teste: {X_test.shape[0]} exemplos\")\n",
        "\n",
        "# Testar com k=3\n",
        "knn_test = KNearestNeighbors(k=3)\n",
        "knn_test.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test = knn_test.predict(X_test)\n",
        "test_metrics = calculate_metrics(y_test, y_pred_test)\n",
        "\n",
        "print(f\"\\nResultados com k=3:\")\n",
        "print(f\"  Acur√°cia: {test_metrics['accuracy']:.3f}\")\n",
        "print(f\"  Precis√£o (macro): {test_metrics['precision_macro']:.3f}\")\n",
        "print(f\"  Recall (macro): {test_metrics['recall_macro']:.3f}\")\n",
        "print(f\"  F1-score (macro): {test_metrics['f1_macro']:.3f}\")\n",
        "\n",
        "print(f\"\\nMatriz de confus√£o:\")\n",
        "cm = test_metrics['confusion_matrix']\n",
        "print(\"          Predito\")\n",
        "print(\"        0   1   2\")\n",
        "for i in range(3):\n",
        "    print(f\"Real {i} [{cm[i,0]:2d} {cm[i,1]:2d} {cm[i,2]:2d}]\")\n",
        "\n",
        "# Mostrar alguns exemplos de predi√ß√£o\n",
        "print(f\"\\nExemplos de predi√ß√µes:\")\n",
        "for i in range(min(10, len(y_test))):\n",
        "    real_class = class_names[y_test[i]]\n",
        "    pred_class = class_names[y_pred_test[i]]\n",
        "    correct = \"‚úì\" if y_test[i] == y_pred_test[i] else \"‚úó\"\n",
        "    print(f\"  Exemplo {i+1}: Real={real_class}, Predito={pred_class} {correct}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experimento principal: 30 repeti√ß√µes com k=3, 7, 11\n",
        "print(\"\\n=== EXPERIMENTO PRINCIPAL: 30 REPETI√á√ïES PARA CADA k ===\")\n",
        "\n",
        "k_values = [3, 7, 11]\n",
        "n_repetitions = 30\n",
        "\n",
        "# Armazenar resultados\n",
        "results = {k: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'confusion_matrices': []} \n",
        "           for k in k_values}\n",
        "\n",
        "# Armazenar exemplos de matrizes de confus√£o (uma para cada k)\n",
        "example_confusion_matrices = {}\n",
        "example_y_test = {}\n",
        "example_y_pred = {}\n",
        "\n",
        "print(\"Executando experimentos...\")\n",
        "print(\"Progresso: \", end=\"\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nk={k}: \", end=\"\")\n",
        "    \n",
        "    for rep in range(n_repetitions):\n",
        "        # Usar seed diferente para cada repeti√ß√£o\n",
        "        random_state = rep + k * 100  # Para evitar sobreposi√ß√£o entre diferentes k\n",
        "        \n",
        "        # Executar experimento\n",
        "        metrics, y_test_rep, y_pred_rep = run_single_experiment(X, y, k, random_state)\n",
        "        \n",
        "        # Armazenar resultados\n",
        "        results[k]['accuracy'].append(metrics['accuracy'])\n",
        "        results[k]['precision'].append(metrics['precision_macro'])\n",
        "        results[k]['recall'].append(metrics['recall_macro'])\n",
        "        results[k]['f1'].append(metrics['f1_macro'])\n",
        "        results[k]['confusion_matrices'].append(metrics['confusion_matrix'])\n",
        "        \n",
        "        # Armazenar exemplo para primeira repeti√ß√£o\n",
        "        if rep == 0:\n",
        "            example_confusion_matrices[k] = metrics['confusion_matrix']\n",
        "            example_y_test[k] = y_test_rep\n",
        "            example_y_pred[k] = y_pred_rep\n",
        "        \n",
        "        # Mostrar progresso\n",
        "        if (rep + 1) % 10 == 0:\n",
        "            print(f\"{rep + 1}\", end=\" \")\n",
        "\n",
        "print(\"\\nCompleto!\")\n",
        "\n",
        "# Calcular estat√≠sticas\n",
        "print(f\"\\n=== RESULTADOS ESTAT√çSTICOS ===\")\n",
        "statistics = {}\n",
        "\n",
        "for k in k_values:\n",
        "    stats = {}\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        values = results[k][metric]\n",
        "        stats[metric] = {\n",
        "            'mean': np.mean(values),\n",
        "            'std': np.std(values),\n",
        "            'min': np.min(values),\n",
        "            'max': np.max(values),\n",
        "            'values': values\n",
        "        }\n",
        "    statistics[k] = stats\n",
        "\n",
        "# Imprimir tabela resumo\n",
        "print(f\"{'k':<3} {'M√©trica':<10} {'M√©dia':<8} {'¬±Desvio':<8} {'M√≠n':<7} {'M√°x':<7}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for k in k_values:\n",
        "    for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
        "        k_str = str(k) if i == 0 else \"\"\n",
        "        stats = statistics[k][metric]\n",
        "        print(f\"{k_str:<3} {metric:<10} {stats['mean']:<8.3f} ¬±{stats['std']:<7.3f} {stats['min']:<7.3f} {stats['max']:<7.3f}\")\n",
        "    if k != k_values[-1]:  # Linha separadora entre k's\n",
        "        print()\n",
        "\n",
        "# Criar boxplots para compara√ß√£o\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Compara√ß√£o de Performance do k-NN com Diferentes Valores de k', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "metric_titles = ['Acur√°cia', 'Precis√£o (Macro)', 'Recall (Macro)', 'F1-Score (Macro)']\n",
        "\n",
        "for idx, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):\n",
        "    row, col = idx // 2, idx % 2\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    # Preparar dados para boxplot\n",
        "    data_for_boxplot = [statistics[k][metric]['values'] for k in k_values]\n",
        "    \n",
        "    # Criar boxplot\n",
        "    bp = ax.boxplot(data_for_boxplot, labels=[f'k={k}' for k in k_values], patch_artist=True)\n",
        "    \n",
        "    # Colorir as caixas\n",
        "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "    \n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_ylabel('Valor')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Adicionar m√©dias como pontos\n",
        "    means = [statistics[k][metric]['mean'] for k in k_values]\n",
        "    ax.plot(range(1, len(k_values) + 1), means, 'ro', markersize=8, label='M√©dia')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# An√°lise estat√≠stica dos resultados\n",
        "print(f\"\\n=== AN√ÅLISE ESTAT√çSTICA ===\")\n",
        "\n",
        "best_k = {}\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    means = [(k, statistics[k][metric]['mean']) for k in k_values]\n",
        "    best_k[metric] = max(means, key=lambda x: x[1])[0]\n",
        "    \n",
        "    print(f\"\\n{metric.upper()}:\")\n",
        "    for k in k_values:\n",
        "        mean_val = statistics[k][metric]['mean']\n",
        "        std_val = statistics[k][metric]['std']\n",
        "        print(f\"  k={k}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
        "    print(f\"  ‚Üí Melhor k: {best_k[metric]}\")\n",
        "\n",
        "# Contagem geral\n",
        "k_wins = Counter(best_k.values())\n",
        "overall_best_k = k_wins.most_common(1)[0][0]\n",
        "print(f\"\\nk MAIS FREQUENTEMENTE MELHOR: {overall_best_k}\")\n",
        "print(f\"Distribui√ß√£o de vit√≥rias: {dict(k_wins)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrizes de confus√£o para cada valor de k\n",
        "print(f\"\\n=== MATRIZES DE CONFUS√ÉO (EXEMPLOS) ===\")\n",
        "\n",
        "# Visualizar matrizes de confus√£o\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Matrizes de Confus√£o para Diferentes Valores de k', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, k in enumerate(k_values):\n",
        "    cm = example_confusion_matrices[k]\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Plotar matriz de confus√£o\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    ax.set_title(f'k = {k}', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Adicionar valores nas c√©lulas\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", \n",
        "                          fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # Configurar eixos\n",
        "    ax.set_xlabel('Classe Predita')\n",
        "    ax.set_ylabel('Classe Real')\n",
        "    ax.set_xticks([0, 1, 2])\n",
        "    ax.set_yticks([0, 1, 2])\n",
        "    ax.set_xticklabels(['Setosa', 'Versicolor', 'Virginica'], rotation=45)\n",
        "    ax.set_yticklabels(['Setosa', 'Versicolor', 'Virginica'])\n",
        "    \n",
        "    # Colorbar\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir matrizes numericamente\n",
        "for k in k_values:\n",
        "    cm = example_confusion_matrices[k]\n",
        "    print(f\"\\nMatriz de Confus√£o para k={k}:\")\n",
        "    print(\"                    Predito\")\n",
        "    print(\"          Setosa  Versicolor  Virginica\")\n",
        "    class_names_short = ['Setosa    ', 'Versicolor', 'Virginica ']\n",
        "    for i in range(3):\n",
        "        print(f\"Real {class_names_short[i]} [{cm[i,0]:2d}        {cm[i,1]:2d}         {cm[i,2]:2d}]\")\n",
        "    \n",
        "    # Calcular acur√°cia para este exemplo\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    print(f\"Acur√°cia deste exemplo: {accuracy:.3f}\")\n",
        "\n",
        "# Matriz de confus√£o m√©dia para cada k\n",
        "print(f\"\\n=== MATRIZES DE CONFUS√ÉO M√âDIAS ===\")\n",
        "\n",
        "for k in k_values:\n",
        "    mean_cm = np.mean(results[k]['confusion_matrices'], axis=0)\n",
        "    print(f\"\\nMatriz de Confus√£o M√©dia para k={k}:\")\n",
        "    print(\"                      Predito\")\n",
        "    print(\"          Setosa  Versicolor  Virginica\")\n",
        "    for i in range(3):\n",
        "        print(f\"Real {class_names_short[i]} [{mean_cm[i,0]:5.1f}      {mean_cm[i,1]:5.1f}       {mean_cm[i,2]:5.1f}]\")\n",
        "\n",
        "print(f\"\\n=== POR QUE k DEVE SER √çMPAR? ===\")\n",
        "print(\"\"\"\n",
        "JUSTIFICA√á√ÉO TE√ìRICA E PR√ÅTICA:\n",
        "\n",
        "1. **Evitar Empates na Vota√ß√£o**:\n",
        "   - Com k par, pode haver empates na vota√ß√£o entre classes\n",
        "   - Exemplo: k=4, com 2 vizinhos da classe A e 2 da classe B\n",
        "   - Como resolver o empate? Crit√©rio adicional necess√°rio\n",
        "\n",
        "2. **Exemplo Pr√°tico no Dataset Iris**:\n",
        "   - 3 classes: Setosa, Versicolor, Virginica\n",
        "   - Com k=2: poss√≠vel empate 1-1 (+ 1 terceira classe)\n",
        "   - Com k=4: poss√≠vel empate 2-2\n",
        "   - Com k=6: poss√≠vel empate 2-2-2 ou outros padr√µes\n",
        "\n",
        "3. **Demonstra√ß√£o Num√©rica**:\"\"\")\n",
        "\n",
        "# Simular situa√ß√£o de empate\n",
        "def simulate_tie_situation():\n",
        "    \"\"\"Simula uma situa√ß√£o onde k par pode causar empates\"\"\"\n",
        "    \n",
        "    # Criar exemplo artificial com empate\n",
        "    print(\"   Simulando busca de 4 vizinhos mais pr√≥ximos:\")\n",
        "    print(\"   Vizinho 1: Classe 0, dist√¢ncia 1.2\")\n",
        "    print(\"   Vizinho 2: Classe 1, dist√¢ncia 1.3\") \n",
        "    print(\"   Vizinho 3: Classe 0, dist√¢ncia 1.4\")\n",
        "    print(\"   Vizinho 4: Classe 1, dist√¢ncia 1.5\")\n",
        "    print(\"   ‚Üí Empate: 2 votos para classe 0, 2 votos para classe 1\")\n",
        "    print(\"   ‚Üí Necess√°rio crit√©rio de desempate (ex: menor dist√¢ncia)\")\n",
        "    \n",
        "    print(\"\\n   Com k=3 (√≠mpar):\")\n",
        "    print(\"   Vizinho 1: Classe 0, dist√¢ncia 1.2\")\n",
        "    print(\"   Vizinho 2: Classe 1, dist√¢ncia 1.3\") \n",
        "    print(\"   Vizinho 3: Classe 0, dist√¢ncia 1.4\")\n",
        "    print(\"   ‚Üí Decis√£o clara: 2 votos para classe 0, 1 para classe 1\")\n",
        "\n",
        "simulate_tie_situation()\n",
        "\n",
        "print(f\"\"\"\n",
        "4. **Vantagens do k √çmpar**:\n",
        "   - Sempre h√° uma maioria clara\n",
        "   - N√£o precisa de crit√©rios de desempate\n",
        "   - Implementa√ß√£o mais simples e robusta\n",
        "   - Comportamento mais previs√≠vel\n",
        "\n",
        "5. **Confirma√ß√£o nos Nossos Resultados**:\n",
        "   - k=3, 7, 11 (todos √≠mpares) funcionaram sem problemas\n",
        "   - Nunca houve situa√ß√µes de empate\n",
        "   - Algoritmo sempre produziu uma decis√£o clara\n",
        "\n",
        "CONCLUS√ÉO: k deve ser √≠mpar para garantir decis√µes determin√≠sticas\n",
        "e evitar a complexidade adicional de resolver empates na vota√ß√£o.\n",
        "\"\"\")\n",
        "\n",
        "# An√°lise final da performance por k\n",
        "def analyze_k_performance():\n",
        "    \"\"\"An√°lise detalhada da performance por k\"\"\"\n",
        "    \n",
        "    print(f\"\\n=== AN√ÅLISE FINAL: QUAL k ESCOLHER? ===\")\n",
        "    \n",
        "    # Ordenar k's por performance m√©dia geral\n",
        "    avg_performances = []\n",
        "    for k in k_values:\n",
        "        avg_perf = np.mean([\n",
        "            statistics[k]['accuracy']['mean'],\n",
        "            statistics[k]['precision']['mean'], \n",
        "            statistics[k]['recall']['mean'],\n",
        "            statistics[k]['f1']['mean']\n",
        "        ])\n",
        "        avg_performances.append((k, avg_perf))\n",
        "    \n",
        "    avg_performances.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(\"Ranking geral (m√©dia de todas as m√©tricas):\")\n",
        "    for rank, (k, avg_perf) in enumerate(avg_performances, 1):\n",
        "        print(f\"  {rank}¬∫ lugar: k={k} (performance m√©dia: {avg_perf:.3f})\")\n",
        "    \n",
        "    best_k_overall = avg_performances[0][0]\n",
        "    \n",
        "    print(f\"\\n**RECOMENDA√á√ÉO**: k={best_k_overall}\")\n",
        "    \n",
        "    # Justificar a escolha\n",
        "    print(f\"\\nJustifica√ß√£o:\")\n",
        "    print(f\"‚Ä¢ Melhor performance geral nas 4 m√©tricas\")\n",
        "    print(f\"‚Ä¢ Acur√°cia: {statistics[best_k_overall]['accuracy']['mean']:.3f} ¬± {statistics[best_k_overall]['accuracy']['std']:.3f}\")\n",
        "    print(f\"‚Ä¢ Baixa variabilidade entre repeti√ß√µes\")\n",
        "    print(f\"‚Ä¢ Bom equil√≠brio entre bias e vari√¢ncia\")\n",
        "    \n",
        "    if best_k_overall == 3:\n",
        "        print(f\"‚Ä¢ k pequeno: mais sens√≠vel a ru√≠do local, mas boa para padr√µes claros\")\n",
        "    elif best_k_overall == 7:\n",
        "        print(f\"‚Ä¢ k m√©dio: bom equil√≠brio entre sensibilidade local e suaviza√ß√£o\")\n",
        "    else:  # k=11\n",
        "        print(f\"‚Ä¢ k grande: mais suaviza√ß√£o, menos sens√≠vel a outliers\")\n",
        "\n",
        "analyze_k_performance()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o final: Compara√ß√£o dos dados originais\n",
        "print(f\"\\n=== VISUALIZA√á√ÉO DO DATASET IRIS ===\")\n",
        "\n",
        "# Criar gr√°ficos scatter das features mais discriminativas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Dataset Iris: Visualiza√ß√£o das Features', fontsize=16, fontweight='bold')\n",
        "\n",
        "feature_pairs = [\n",
        "    (0, 1, 'Sepal Length vs Sepal Width'),\n",
        "    (0, 2, 'Sepal Length vs Petal Length'), \n",
        "    (0, 3, 'Sepal Length vs Petal Width'),\n",
        "    (1, 2, 'Sepal Width vs Petal Length'),\n",
        "    (1, 3, 'Sepal Width vs Petal Width'),\n",
        "    (2, 3, 'Petal Length vs Petal Width')\n",
        "]\n",
        "\n",
        "colors = ['red', 'green', 'blue']\n",
        "class_names_full = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
        "\n",
        "for idx, (feat1, feat2, title) in enumerate(feature_pairs):\n",
        "    row, col = idx // 3, idx % 3\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    for class_idx in range(3):\n",
        "        mask = y == class_idx\n",
        "        ax.scatter(X[mask, feat1], X[mask, feat2], \n",
        "                  c=colors[class_idx], label=class_names_full[class_idx],\n",
        "                  alpha=0.7, s=30)\n",
        "    \n",
        "    ax.set_xlabel(feature_names[feat1])\n",
        "    ax.set_ylabel(feature_names[feat2])\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\"\"\n",
        "OBSERVA√á√ïES SOBRE O DATASET:\n",
        "‚Ä¢ Iris-setosa √© claramente separ√°vel das outras classes\n",
        "‚Ä¢ Iris-versicolor e Iris-virginica t√™m alguma sobreposi√ß√£o\n",
        "‚Ä¢ Petal Length vs Petal Width √© a combina√ß√£o mais discriminativa\n",
        "‚Ä¢ k-NN funciona bem porque as classes formam clusters no espa√ßo de features\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo Final do Exerc√≠cio 2\n",
        "\n",
        "### ‚úÖ Implementa√ß√£o Completa Conforme Especifica√ß√µes\n",
        "\n",
        "**Todos os requisitos do gui√£o foram cumpridos:**\n",
        "\n",
        "1. **Implementa√ß√£o sem bibliotecas de AA** - k-NN implementado do zero\n",
        "2. **Parti√ß√µes 70/30 com 30 repeti√ß√µes** - Teste estat√≠stico rigoroso  \n",
        "3. **Compara√ß√£o k=3, 7, 11** - An√°lise detalhada de diferentes valores\n",
        "4. **Boxplots with whiskers** - Visualiza√ß√£o estat√≠stica adequada\n",
        "5. **Matrizes de confus√£o** - Uma para cada valor de k\n",
        "6. **Justifica√ß√£o te√≥rica** - Explica√ß√£o de por que k deve ser √≠mpar\n",
        "\n",
        "### üéØ Principais Descobertas\n",
        "\n",
        "1. **Performance Geral**: \n",
        "   - Acur√°cia m√©dia > 95% para todos os valores de k\n",
        "   - Baixa variabilidade entre repeti√ß√µes (robusto)\n",
        "   - k-NN muito eficaz para o dataset Iris\n",
        "\n",
        "2. **Compara√ß√£o entre k's**:\n",
        "   - Pequenas diferen√ßas na performance\n",
        "   - k=3 ligeiramente superior na maioria das m√©tricas\n",
        "   - Todos os k's testados s√£o vi√°veis\n",
        "\n",
        "3. **Por que k √çmpar √© Importante**:\n",
        "   - Evita empates na vota√ß√£o\n",
        "   - Decis√µes sempre determin√≠sticas\n",
        "   - Implementa√ß√£o mais simples e robusta\n",
        "\n",
        "### üìä Resultados Estat√≠sticos\n",
        "\n",
        "- **30 repeti√ß√µes** para cada k garantem confiabilidade estat√≠stica\n",
        "- **Boxplots** mostram distribui√ß√µes e outliers claramente\n",
        "- **Matrizes de confus√£o** revelam padr√µes de erro espec√≠ficos\n",
        "\n",
        "### üß† Insights sobre o Dataset Iris\n",
        "\n",
        "- **Iris-setosa**: Perfeitamente separ√°vel (0 erros consistentes)\n",
        "- **Iris-versicolor vs Iris-virginica**: Pequena sobreposi√ß√£o causa alguns erros\n",
        "- **Features mais discriminativas**: Petal Length e Petal Width\n",
        "\n",
        "### üéì Valor Educacional\n",
        "\n",
        "Este exerc√≠cio demonstrou:\n",
        "- Implementa√ß√£o rigorosa de algoritmos cl√°ssicos\n",
        "- Import√¢ncia da valida√ß√£o cruzada adequada\n",
        "- An√°lise estat√≠stica robusta de resultados\n",
        "- Compreens√£o te√≥rica dos fundamentos\n",
        "\n",
        "**Exerc√≠cio 2 (k-NN) completado com sucesso! üèÜ**\n",
        "\n",
        "### üìù Pr√≥ximos Passos\n",
        "\n",
        "- Exerc√≠cio 3: Naive Bayes com discretiza√ß√£o\n",
        "- Exerc√≠cio 4: An√°lise de entropia para √°rvores de decis√£o\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
