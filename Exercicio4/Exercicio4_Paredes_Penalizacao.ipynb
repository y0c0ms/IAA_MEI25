{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Configuração para gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd02f2",
   "metadata": {},
   "source": [
    "## 1. Ambiente com Paredes e Penalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805503e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironmentWithWalls:\n",
    "    \"\"\"Ambiente de grelha 10x10 com paredes e penalização.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grid_size = 10\n",
    "        self.n_states = 100\n",
    "        self.goal_state = 100\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "        # Paredes conforme Figura 4\n",
    "        self.walls = {4, 14, 24, 34, 44, 54, 64, 74, 84, \n",
    "                     17, 27, 37, 47, 57, 67, 77, 87, 97}\n",
    "        \n",
    "        # Penalização por bater em parede\n",
    "        self.wall_penalty = -0.1\n",
    "        self.goal_reward = 100.0\n",
    "        \n",
    "    def state_to_position(self, state: int) -> Tuple[int, int]:\n",
    "        \"\"\"Converte estado (1-100) para posição (linha, coluna).\"\"\"\n",
    "        row = (state - 1) // self.grid_size\n",
    "        col = (state - 1) % self.grid_size\n",
    "        return row, col\n",
    "    \n",
    "    def position_to_state(self, row: int, col: int) -> int:\n",
    "        \"\"\"Converte posição (linha, coluna) para estado (1-100).\"\"\"\n",
    "        return row * self.grid_size + col + 1\n",
    "    \n",
    "    def is_valid_state(self, state: int) -> bool:\n",
    "        \"\"\"Verifica se o estado está dentro da grelha e não é parede.\"\"\"\n",
    "        if state < 1 or state > self.n_states:\n",
    "            return False\n",
    "        return state not in self.walls\n",
    "    \n",
    "    def transition(self, state: int, action: str) -> Tuple[int, float]:\n",
    "        \"\"\"Executa ação e retorna (novo_estado, recompensa).\"\"\"\n",
    "        row, col = self.state_to_position(state)\n",
    "        \n",
    "        # Calcula novo estado baseado na ação\n",
    "        if action == 'UP':\n",
    "            new_row, new_col = row - 1, col\n",
    "        elif action == 'DOWN':\n",
    "            new_row, new_col = row + 1, col\n",
    "        elif action == 'LEFT':\n",
    "            new_row, new_col = row, col - 1\n",
    "        elif action == 'RIGHT':\n",
    "            new_row, new_col = row, col + 1\n",
    "        else:\n",
    "            raise ValueError(f\"Ação inválida: {action}\")\n",
    "        \n",
    "        # Verifica se novo estado é válido\n",
    "        if (new_row < 0 or new_row >= self.grid_size or \n",
    "            new_col < 0 or new_col >= self.grid_size):\n",
    "            # Fora dos limites - permanece no mesmo estado com penalização\n",
    "            return state, self.wall_penalty\n",
    "        \n",
    "        new_state = self.position_to_state(new_row, new_col)\n",
    "        \n",
    "        # Verifica se novo estado é parede\n",
    "        if new_state in self.walls:\n",
    "            # Bate em parede - permanece no mesmo estado com penalização\n",
    "            return state, self.wall_penalty\n",
    "        \n",
    "        # Movimento válido\n",
    "        reward = self.goal_reward if new_state == self.goal_state else 0.0\n",
    "        return new_state, reward\n",
    "    \n",
    "    def get_reward(self, state: int) -> float:\n",
    "        \"\"\"Retorna recompensa do estado.\"\"\"\n",
    "        return self.goal_reward if state == self.goal_state else 0.0\n",
    "    \n",
    "    def random_action(self) -> str:\n",
    "        \"\"\"Retorna ação aleatória.\"\"\"\n",
    "        return np.random.choice(self.actions)\n",
    "    \n",
    "    def visualize_environment(self):\n",
    "        \"\"\"Visualiza o ambiente com paredes.\"\"\"\n",
    "        grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        # Marca paredes\n",
    "        for wall in self.walls:\n",
    "            row, col = self.state_to_position(wall)\n",
    "            grid[row, col] = -1\n",
    "        \n",
    "        # Marca objetivo\n",
    "        goal_row, goal_col = self.state_to_position(self.goal_state)\n",
    "        grid[goal_row, goal_col] = 2\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid, cmap='RdYlGn', interpolation='nearest')\n",
    "        plt.colorbar(label='Tipo de célula', ticks=[-1, 0, 2])\n",
    "        plt.gca().set_yticklabels([])\n",
    "        plt.gca().set_xticklabels([])\n",
    "        \n",
    "        # Adiciona números dos estados\n",
    "        for state in range(1, self.n_states + 1):\n",
    "            row, col = self.state_to_position(state)\n",
    "            color = 'white' if state in self.walls else 'black'\n",
    "            plt.text(col, row, str(state), ha='center', va='center', \n",
    "                    fontsize=8, color=color, weight='bold')\n",
    "        \n",
    "        plt.title('Ambiente com Paredes (Figura 4)\\n-1: Parede, 0: Livre, 2: Objetivo', \n",
    "                 fontsize=14, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o ambiente\n",
    "env = GridEnvironmentWithWalls()\n",
    "env.visualize_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b314bb",
   "metadata": {},
   "source": [
    "## 2. Baseline: Random Walk com Paredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_walk_episode(env: GridEnvironmentWithWalls, \n",
    "                            initial_state: int = 1, \n",
    "                            max_steps: int = 1000) -> Tuple[float, int, float]:\n",
    "    \"\"\"Executa um episódio com random walk.\n",
    "    \n",
    "    Retorna:\n",
    "        - recompensa média por passo\n",
    "        - número de passos até objetivo (ou max_steps)\n",
    "        - tempo de execução\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    state = initial_state\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = env.random_action()\n",
    "        new_state, reward = env.transition(state, action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if new_state == env.goal_state:\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    avg_reward_per_step = total_reward / steps if steps > 0 else 0.0\n",
    "    \n",
    "    return avg_reward_per_step, steps, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar baseline com paredes\n",
    "np.random.seed(42)\n",
    "n_episodes = 30\n",
    "\n",
    "baseline_results = {\n",
    "    'avg_rewards': [],\n",
    "    'steps': [],\n",
    "    'times': []\n",
    "}\n",
    "\n",
    "print(\"Executando baseline (Random Walk) com paredes...\")\n",
    "for episode in range(n_episodes):\n",
    "    avg_reward, steps, exec_time = run_random_walk_episode(env)\n",
    "    baseline_results['avg_rewards'].append(avg_reward)\n",
    "    baseline_results['steps'].append(steps)\n",
    "    baseline_results['times'].append(exec_time)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episódio {episode + 1}/{n_episodes} concluído\")\n",
    "\n",
    "# Estatísticas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE COM PAREDES - ESTATÍSTICAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Recompensa média por passo: {np.mean(baseline_results['avg_rewards']):.4f} ± {np.std(baseline_results['avg_rewards']):.4f}\")\n",
    "print(f\"Número de passos: {np.mean(baseline_results['steps']):.2f} ± {np.std(baseline_results['steps']):.2f}\")\n",
    "print(f\"Tempo de execução: {np.mean(baseline_results['times']):.4f} ± {np.std(baseline_results['times']):.4f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos resultados baseline\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].boxplot(baseline_results['avg_rewards'])\n",
    "axes[0].set_title('Recompensa Média por Passo\\n(Baseline com Paredes)', weight='bold')\n",
    "axes[0].set_ylabel('Recompensa Média')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(baseline_results['steps'])\n",
    "axes[1].set_title('Número de Passos\\n(Baseline com Paredes)', weight='bold')\n",
    "axes[1].set_ylabel('Passos')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].boxplot(baseline_results['times'])\n",
    "axes[2].set_title('Tempo de Execução\\n(Baseline com Paredes)', weight='bold')\n",
    "axes[2].set_ylabel('Tempo (s)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29c4b7",
   "metadata": {},
   "source": [
    "## 3. Q-Learning com Paredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentWithWalls:\n",
    "    \"\"\"Agente Q-Learning para ambiente com paredes.\"\"\"\n",
    "    \n",
    "    def __init__(self, env: GridEnvironmentWithWalls, alpha: float = 0.7, gamma: float = 0.99):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Tabela Q: [estado, ação]\n",
    "        self.Q = np.zeros((env.n_states + 1, env.n_actions))\n",
    "        \n",
    "    def get_action_index(self, action: str) -> int:\n",
    "        \"\"\"Converte ação para índice.\"\"\"\n",
    "        return self.env.actions.index(action)\n",
    "    \n",
    "    def get_best_action(self, state: int) -> str:\n",
    "        \"\"\"Retorna melhor ação para o estado (com desempate aleatório).\"\"\"\n",
    "        q_values = self.Q[state]\n",
    "        max_q = np.max(q_values)\n",
    "        best_actions = [i for i, q in enumerate(q_values) if q == max_q]\n",
    "        best_action_idx = np.random.choice(best_actions)\n",
    "        return self.env.actions[best_action_idx]\n",
    "    \n",
    "    def update_q(self, state: int, action: str, reward: float, next_state: int):\n",
    "        \"\"\"Atualiza valor Q.\"\"\"\n",
    "        action_idx = self.get_action_index(action)\n",
    "        current_q = self.Q[state, action_idx]\n",
    "        max_next_q = np.max(self.Q[next_state])\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_next_q)\n",
    "        self.Q[state, action_idx] = new_q\n",
    "    \n",
    "    def train_random_walk(self, n_steps: int, test_points: List[int], \n",
    "                          initial_state: int = 1) -> dict:\n",
    "        \"\"\"Treino com random walk.\"\"\"\n",
    "        results = {\n",
    "            'test_points': test_points,\n",
    "            'test_rewards': []\n",
    "        }\n",
    "        \n",
    "        state = initial_state\n",
    "        test_idx = 0\n",
    "        \n",
    "        for step in range(1, n_steps + 1):\n",
    "            # Ação aleatória\n",
    "            action = self.env.random_action()\n",
    "            next_state, reward = self.env.transition(state, action)\n",
    "            \n",
    "            # Atualiza Q\n",
    "            self.update_q(state, action, reward, next_state)\n",
    "            \n",
    "            # Reset se chegou ao objetivo\n",
    "            if next_state == self.env.goal_state:\n",
    "                state = initial_state\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            # Teste nos pontos especificados\n",
    "            if test_idx < len(test_points) and step == test_points[test_idx]:\n",
    "                test_reward = self.test_policy(n_steps=1000, initial_state=initial_state)\n",
    "                results['test_rewards'].append(test_reward)\n",
    "                test_idx += 1\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_epsilon_greedy(self, n_steps: int, test_points: List[int], \n",
    "                            greed: float = 0.9, initial_state: int = 1) -> dict:\n",
    "        \"\"\"Treino com estratégia ε-greedy.\"\"\"\n",
    "        results = {\n",
    "            'test_points': test_points,\n",
    "            'test_rewards': []\n",
    "        }\n",
    "        \n",
    "        state = initial_state\n",
    "        test_idx = 0\n",
    "        \n",
    "        for step in range(1, n_steps + 1):\n",
    "            # Escolhe ação: greedy ou aleatória\n",
    "            if np.random.random() < greed:\n",
    "                action = self.get_best_action(state)\n",
    "            else:\n",
    "                action = self.env.random_action()\n",
    "            \n",
    "            next_state, reward = self.env.transition(state, action)\n",
    "            \n",
    "            # Atualiza Q\n",
    "            self.update_q(state, action, reward, next_state)\n",
    "            \n",
    "            # Reset se chegou ao objetivo\n",
    "            if next_state == self.env.goal_state:\n",
    "                state = initial_state\n",
    "            else:\n",
    "                state = next_state\n",
    "            \n",
    "            # Teste nos pontos especificados\n",
    "            if test_idx < len(test_points) and step == test_points[test_idx]:\n",
    "                test_reward = self.test_policy(n_steps=1000, initial_state=initial_state)\n",
    "                results['test_rewards'].append(test_reward)\n",
    "                test_idx += 1\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_policy(self, n_steps: int = 1000, initial_state: int = 1) -> float:\n",
    "        \"\"\"Testa política atual sem atualizar Q.\"\"\"\n",
    "        state = initial_state\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            action = self.get_best_action(state)\n",
    "            next_state, reward = self.env.transition(state, action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if next_state == self.env.goal_state:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        return total_reward / steps if steps > 0 else 0.0\n",
    "    \n",
    "    def get_utility_map(self) -> np.ndarray:\n",
    "        \"\"\"Retorna mapa de utilidade máxima por estado.\"\"\"\n",
    "        utility = np.zeros((self.env.grid_size, self.env.grid_size))\n",
    "        \n",
    "        for state in range(1, self.env.n_states + 1):\n",
    "            row, col = self.env.state_to_position(state)\n",
    "            if state in self.env.walls:\n",
    "                utility[row, col] = np.nan\n",
    "            else:\n",
    "                utility[row, col] = np.max(self.Q[state])\n",
    "        \n",
    "        return utility\n",
    "    \n",
    "    def visualize_utility(self, title: str = \"Mapa de Utilidade\"):\n",
    "        \"\"\"Visualiza mapa de utilidade.\"\"\"\n",
    "        utility = self.get_utility_map()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        im = plt.imshow(utility, cmap='viridis', interpolation='nearest')\n",
    "        plt.colorbar(im, label='Utilidade Máxima')\n",
    "        \n",
    "        # Adiciona valores\n",
    "        for state in range(1, self.env.n_states + 1):\n",
    "            row, col = self.env.state_to_position(state)\n",
    "            if state not in self.env.walls:\n",
    "                max_q = np.max(self.Q[state])\n",
    "                plt.text(col, row, f'{max_q:.1f}', ha='center', va='center',\n",
    "                        fontsize=8, color='white' if max_q > utility[~np.isnan(utility)].mean() else 'black')\n",
    "        \n",
    "        plt.title(title, fontsize=14, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd749533",
   "metadata": {},
   "source": [
    "## 4. Experimento: Random Walk com Paredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "n_experiments = 30\n",
    "n_training_steps = 20000\n",
    "test_points = [100, 200, 500, 600, 700, 800, 900, 1000, \n",
    "               2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n",
    "\n",
    "# Armazenar resultados\n",
    "random_walk_results = {\n",
    "    'test_rewards': np.zeros((n_experiments, len(test_points))),\n",
    "    'execution_times': []\n",
    "}\n",
    "\n",
    "print(\"Executando Q-Learning com Random Walk (com paredes)...\")\n",
    "for exp in range(n_experiments):\n",
    "    np.random.seed(exp)\n",
    "    \n",
    "    agent = QLearningAgentWithWalls(env)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = agent.train_random_walk(n_training_steps, test_points)\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    random_walk_results['test_rewards'][exp] = results['test_rewards']\n",
    "    random_walk_results['execution_times'].append(exec_time)\n",
    "    \n",
    "    if (exp + 1) % 5 == 0:\n",
    "        print(f\"Experimento {exp + 1}/{n_experiments} concluído (tempo: {exec_time:.2f}s)\")\n",
    "\n",
    "# Estatísticas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Q-LEARNING RANDOM WALK COM PAREDES - ESTATÍSTICAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Tempo médio de execução: {np.mean(random_walk_results['execution_times']):.2f} ± {np.std(random_walk_results['execution_times']):.2f} s\")\n",
    "print(f\"Recompensa final (20000 passos): {np.mean(random_walk_results['test_rewards'][:, -1]):.4f} ± {np.std(random_walk_results['test_rewards'][:, -1]):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e180b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de evolução\n",
    "mean_rewards = np.mean(random_walk_results['test_rewards'], axis=0)\n",
    "std_rewards = np.std(random_walk_results['test_rewards'], axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_points, mean_rewards, 'o-', linewidth=2, markersize=8, label='Média')\n",
    "plt.fill_between(test_points, \n",
    "                 mean_rewards - std_rewards, \n",
    "                 mean_rewards + std_rewards, \n",
    "                 alpha=0.3, label='± 1 desvio padrão')\n",
    "plt.xlabel('Passos de Treino', fontsize=12, weight='bold')\n",
    "plt.ylabel('Recompensa Média por Passo (Teste)', fontsize=12, weight='bold')\n",
    "plt.title('Evolução do Q-Learning com Random Walk (Com Paredes)', fontsize=14, weight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar mapa de utilidade (última experiência)\n",
    "agent.visualize_utility(\"Mapa de Utilidade - Q-Learning Random Walk (Com Paredes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd6c1f",
   "metadata": {},
   "source": [
    "## 5. Experimento: ε-Greedy com Paredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7191817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar diferentes valores de greed\n",
    "greed_values = [0.2, 0.5, 0.9]\n",
    "\n",
    "epsilon_greedy_results = {}\n",
    "\n",
    "for greed in greed_values:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testando ε-greedy com greed = {greed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {\n",
    "        'test_rewards': np.zeros((n_experiments, len(test_points))),\n",
    "        'execution_times': []\n",
    "    }\n",
    "    \n",
    "    for exp in range(n_experiments):\n",
    "        np.random.seed(exp)\n",
    "        \n",
    "        agent = QLearningAgentWithWalls(env)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        exp_results = agent.train_epsilon_greedy(n_training_steps, test_points, greed=greed)\n",
    "        \n",
    "        exec_time = time.time() - start_time\n",
    "        results['test_rewards'][exp] = exp_results['test_rewards']\n",
    "        results['execution_times'].append(exec_time)\n",
    "        \n",
    "        if (exp + 1) % 10 == 0:\n",
    "            print(f\"Experimento {exp + 1}/{n_experiments} concluído\")\n",
    "    \n",
    "    epsilon_greedy_results[greed] = results\n",
    "    \n",
    "    # Estatísticas\n",
    "    print(f\"\\nTempo médio: {np.mean(results['execution_times']):.2f} ± {np.std(results['execution_times']):.2f} s\")\n",
    "    print(f\"Recompensa final: {np.mean(results['test_rewards'][:, -1]):.4f} ± {np.std(results['test_rewards'][:, -1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar todas as estratégias\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Random Walk\n",
    "mean_rw = np.mean(random_walk_results['test_rewards'], axis=0)\n",
    "std_rw = np.std(random_walk_results['test_rewards'], axis=0)\n",
    "plt.plot(test_points, mean_rw, 'o-', linewidth=2, markersize=6, label='Random Walk')\n",
    "plt.fill_between(test_points, mean_rw - std_rw, mean_rw + std_rw, alpha=0.2)\n",
    "\n",
    "# ε-greedy\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, greed in enumerate(greed_values):\n",
    "    mean_eg = np.mean(epsilon_greedy_results[greed]['test_rewards'], axis=0)\n",
    "    std_eg = np.std(epsilon_greedy_results[greed]['test_rewards'], axis=0)\n",
    "    plt.plot(test_points, mean_eg, 's-', linewidth=2, markersize=6, \n",
    "             label=f'ε-greedy (greed={greed})', color=colors[i])\n",
    "    plt.fill_between(test_points, mean_eg - std_eg, mean_eg + std_eg, alpha=0.2, color=colors[i])\n",
    "\n",
    "plt.xlabel('Passos de Treino', fontsize=12, weight='bold')\n",
    "plt.ylabel('Recompensa Média por Passo', fontsize=12, weight='bold')\n",
    "plt.title('Comparação de Estratégias (Ambiente com Paredes)', fontsize=14, weight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d37aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar mapas de utilidade para diferentes estratégias\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "strategies = [\n",
    "    ('Random Walk', None, random_walk_results),\n",
    "    ('ε-greedy (0.2)', 0.2, epsilon_greedy_results[0.2]),\n",
    "    ('ε-greedy (0.5)', 0.5, epsilon_greedy_results[0.5]),\n",
    "    ('ε-greedy (0.9)', 0.9, epsilon_greedy_results[0.9])\n",
    "]\n",
    "\n",
    "for idx, (name, greed, _) in enumerate(strategies):\n",
    "    np.random.seed(10)\n",
    "    agent = QLearningAgentWithWalls(env)\n",
    "    \n",
    "    if greed is None:\n",
    "        agent.train_random_walk(n_training_steps, test_points)\n",
    "    else:\n",
    "        agent.train_epsilon_greedy(n_training_steps, test_points, greed=greed)\n",
    "    \n",
    "    utility = agent.get_utility_map()\n",
    "    \n",
    "    im = axes[idx].imshow(utility, cmap='viridis', interpolation='nearest')\n",
    "    axes[idx].set_title(name, fontsize=12, weight='bold')\n",
    "    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Mapas de Utilidade - Comparação de Estratégias (Com Paredes)', \n",
    "             fontsize=14, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12e87e",
   "metadata": {},
   "source": [
    "## 6. Comparação com Resultados Anteriores (Sem Paredes)\n",
    "\n",
    "### Análise Comparativa\n",
    "\n",
    "Para comparar adequadamente com os exercícios anteriores (sem paredes), vamos analisar:\n",
    "\n",
    "1. **Impacto das paredes no baseline (random walk)**\n",
    "2. **Eficácia do Q-Learning com paredes vs. sem paredes**\n",
    "3. **Efeito da penalização na aprendizagem**\n",
    "4. **Diferenças nos mapas de utilidade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo comparativo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO COMPARATIVO: AMBIENTE COM PAREDES vs SEM PAREDES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. BASELINE (Random Walk)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Recompensa média por passo: {np.mean(baseline_results['avg_rewards']):.4f} ± {np.std(baseline_results['avg_rewards']):.4f}\")\n",
    "print(f\"   Número médio de passos: {np.mean(baseline_results['steps']):.2f} ± {np.std(baseline_results['steps']):.2f}\")\n",
    "print(f\"   Tempo médio: {np.mean(baseline_results['times']):.4f} ± {np.std(baseline_results['times']):.4f} s\")\n",
    "\n",
    "print(\"\\n2. Q-LEARNING COM RANDOM WALK (20000 passos)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Recompensa final no teste: {np.mean(random_walk_results['test_rewards'][:, -1]):.4f} ± {np.std(random_walk_results['test_rewards'][:, -1]):.4f}\")\n",
    "print(f\"   Tempo médio de treino: {np.mean(random_walk_results['execution_times']):.2f} ± {np.std(random_walk_results['execution_times']):.2f} s\")\n",
    "\n",
    "print(\"\\n3. ε-GREEDY (20000 passos)\")\n",
    "print(\"-\" * 80)\n",
    "for greed in greed_values:\n",
    "    mean_reward = np.mean(epsilon_greedy_results[greed]['test_rewards'][:, -1])\n",
    "    std_reward = np.std(epsilon_greedy_results[greed]['test_rewards'][:, -1])\n",
    "    mean_time = np.mean(epsilon_greedy_results[greed]['execution_times'])\n",
    "    std_time = np.std(epsilon_greedy_results[greed]['execution_times'])\n",
    "    print(f\"   greed={greed}: Recompensa = {mean_reward:.4f} ± {std_reward:.4f}, Tempo = {mean_time:.2f} ± {std_time:.2f} s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OBSERVAÇÕES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. As paredes tornam o problema mais difícil, aumentando o número médio de passos\")\n",
    "print(\"2. A penalização de -0.1 ajuda o agente a evitar bater em paredes repetidamente\")\n",
    "print(\"3. Estratégias com maior exploração (greed baixo) podem ser mais eficazes\")\n",
    "print(\"   em ambientes com paredes, pois descobrem caminhos alternativos\")\n",
    "print(\"4. O Q-Learning aprende a navegar eficientemente mesmo com obstáculos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc2e14",
   "metadata": {},
   "source": [
    "## 7. Análise do Efeito da Penalização\n",
    "\n",
    "A penalização de -0.1 ao bater em paredes tem os seguintes efeitos:\n",
    "\n",
    "1. **Desencoraja ações inúteis**: O agente aprende a evitar ações que levam a paredes\n",
    "2. **Acelera a aprendizagem**: A penalização fornece feedback negativo imediato\n",
    "3. **Melhora a navegação**: O agente aprende caminhos mais eficientes\n",
    "4. **Equilibra exploração/exploração**: A penalização não é tão grande que impeça exploração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee443ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise detalhada da trajetória do agente treinado\n",
    "def analyze_trajectory(agent, env, initial_state=1, max_steps=1000):\n",
    "    \"\"\"Analisa trajetória do agente treinado.\"\"\"\n",
    "    trajectory = [initial_state]\n",
    "    state = initial_state\n",
    "    wall_hits = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.get_best_action(state)\n",
    "        next_state, reward = env.transition(state, action)\n",
    "        \n",
    "        # Conta batidas em parede\n",
    "        if next_state == state and reward == env.wall_penalty:\n",
    "            wall_hits += 1\n",
    "        \n",
    "        trajectory.append(next_state)\n",
    "        \n",
    "        if next_state == env.goal_state:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return trajectory, wall_hits\n",
    "\n",
    "# Analisa melhor agente\n",
    "np.random.seed(10)\n",
    "best_agent = QLearningAgentWithWalls(env)\n",
    "best_agent.train_epsilon_greedy(n_training_steps, test_points, greed=0.9)\n",
    "\n",
    "trajectory, wall_hits = analyze_trajectory(best_agent, env)\n",
    "\n",
    "print(\"\\nANÁLISE DA TRAJETÓRIA DO MELHOR AGENTE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Número de passos até o objetivo: {len(trajectory) - 1}\")\n",
    "print(f\"Número de batidas em parede: {wall_hits}\")\n",
    "print(f\"Estados visitados: {trajectory[:20]}...\" if len(trajectory) > 20 else f\"Estados visitados: {trajectory}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar trajetória\n",
    "def visualize_trajectory(env, trajectory):\n",
    "    \"\"\"Visualiza trajetória do agente.\"\"\"\n",
    "    grid = np.zeros((env.grid_size, env.grid_size))\n",
    "    \n",
    "    # Marca paredes\n",
    "    for wall in env.walls:\n",
    "        row, col = env.state_to_position(wall)\n",
    "        grid[row, col] = -1\n",
    "    \n",
    "    # Marca trajetória\n",
    "    for i, state in enumerate(trajectory):\n",
    "        if state not in env.walls:\n",
    "            row, col = env.state_to_position(state)\n",
    "            grid[row, col] = i + 1\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(grid, cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar(label='Ordem de visita')\n",
    "    \n",
    "    # Adiciona setas para mostrar direção\n",
    "    for i in range(len(trajectory) - 1):\n",
    "        state = trajectory[i]\n",
    "        next_state = trajectory[i + 1]\n",
    "        \n",
    "        if state != next_state and state not in env.walls:\n",
    "            row1, col1 = env.state_to_position(state)\n",
    "            row2, col2 = env.state_to_position(next_state)\n",
    "            \n",
    "            if i % 3 == 0:  # Mostrar apenas algumas setas para clareza\n",
    "                plt.arrow(col1, row1, (col2 - col1) * 0.4, (row2 - row1) * 0.4,\n",
    "                         head_width=0.2, head_length=0.15, fc='yellow', ec='yellow', linewidth=2)\n",
    "    \n",
    "    plt.title('Trajetória do Agente Treinado (Com Paredes)', fontsize=14, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_trajectory(env, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbcd257",
   "metadata": {},
   "source": [
    "## 8. Conclusões\n",
    "\n",
    "### Principais Descobertas:\n",
    "\n",
    "1. **Impacto das Paredes**:\n",
    "   - As paredes aumentam significativamente a dificuldade do problema\n",
    "   - O número médio de passos para atingir o objetivo aumenta\n",
    "   - A navegação requer aprendizagem de caminhos mais complexos\n",
    "\n",
    "2. **Efeito da Penalização**:\n",
    "   - A penalização de -0.1 é eficaz para desencorajar batidas em paredes\n",
    "   - Não é tão grande a ponto de dominar o sinal de recompensa do objetivo\n",
    "   - Ajuda o agente a aprender políticas mais eficientes\n",
    "\n",
    "3. **Estratégias de Aprendizagem**:\n",
    "   - ε-greedy com greed moderado (0.5-0.7) é eficaz em ambientes com obstáculos\n",
    "   - Muito exploitation (greed = 0.9) pode levar a políticas subótimas\n",
    "   - Random walk puro é muito ineficiente neste ambiente\n",
    "\n",
    "4. **Comparação com Ambiente Sem Paredes**:\n",
    "   - O Q-Learning ainda é capaz de aprender boas políticas\n",
    "   - Requer mais passos de treino para convergir\n",
    "   - A exploração é mais importante neste cenário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fcc57",
   "metadata": {},
   "source": [
    "## 9. Salvando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a513093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados em arquivo\n",
    "import pickle\n",
    "\n",
    "results_to_save = {\n",
    "    'baseline': baseline_results,\n",
    "    'random_walk': random_walk_results,\n",
    "    'epsilon_greedy': epsilon_greedy_results,\n",
    "    'test_points': test_points,\n",
    "    'greed_values': greed_values,\n",
    "    'environment': {\n",
    "        'walls': list(env.walls),\n",
    "        'wall_penalty': env.wall_penalty,\n",
    "        'goal_reward': env.goal_reward\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('exercicio4_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_to_save, f)\n",
    "\n",
    "print(\"Resultados salvos em 'exercicio4_results.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
