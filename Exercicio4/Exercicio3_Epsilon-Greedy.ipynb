{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acab2c2a",
   "metadata": {},
   "source": [
    "## 1. Importação e Configuração Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6174e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "\n",
    "# Configuração para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Configuração do ambiente\n",
    "GRID_SIZE = 10\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE\n",
    "ESTADO_INICIAL = 1\n",
    "ESTADO_OBJETIVO = 100\n",
    "\n",
    "PAREDES = {\n",
    "    23, 24, 25, 26,\n",
    "    33, 34, 35, 36,\n",
    "    54, 64, 74, 84,\n",
    "    55, 65, 75, 85,\n",
    "}\n",
    "\n",
    "ACOES = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "MAX_PASSOS_POR_EPISODIO = 1000\n",
    "\n",
    "print(\"Configuração base importada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a22dd",
   "metadata": {},
   "source": [
    "## 2. Funções do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transicao_estado(estado: int, acao: str) -> int:\n",
    "    \"\"\"Calcula o próximo estado após aplicar uma ação.\"\"\"\n",
    "    linha = (estado - 1) // GRID_SIZE\n",
    "    coluna = (estado - 1) % GRID_SIZE\n",
    "    \n",
    "    nova_linha, nova_coluna = linha, coluna\n",
    "    \n",
    "    if acao == 'UP':\n",
    "        nova_linha = linha - 1\n",
    "    elif acao == 'DOWN':\n",
    "        nova_linha = linha + 1\n",
    "    elif acao == 'LEFT':\n",
    "        nova_coluna = coluna - 1\n",
    "    elif acao == 'RIGHT':\n",
    "        nova_coluna = coluna + 1\n",
    "    \n",
    "    if nova_linha < 0 or nova_linha >= GRID_SIZE or nova_coluna < 0 or nova_coluna >= GRID_SIZE:\n",
    "        return estado\n",
    "    \n",
    "    novo_estado = nova_linha * GRID_SIZE + nova_coluna + 1\n",
    "    if novo_estado in PAREDES:\n",
    "        return estado\n",
    "    \n",
    "    return novo_estado\n",
    "\n",
    "\n",
    "def recompensa(estado: int) -> float:\n",
    "    \"\"\"Retorna a recompensa de um estado.\"\"\"\n",
    "    return 100.0 if estado == ESTADO_OBJETIVO else 0.0\n",
    "\n",
    "\n",
    "def acao_aleatoria() -> str:\n",
    "    \"\"\"Escolhe uma ação aleatória.\"\"\"\n",
    "    return np.random.choice(ACOES)\n",
    "\n",
    "\n",
    "print(\"Funções do ambiente definidas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d87808",
   "metadata": {},
   "source": [
    "## 3. Configuração do Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813386ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do Q-Learning\n",
    "ALPHA = 0.7  # Taxa de aprendizagem\n",
    "GAMMA = 0.99  # Fator de desconto\n",
    "\n",
    "# Parâmetros de treino\n",
    "NUM_PASSOS_TREINO = 20000\n",
    "NUM_PASSOS_TESTE = 1000\n",
    "NUM_EXPERIENCIAS = 30\n",
    "\n",
    "# Pontos de teste durante o treino\n",
    "PONTOS_TESTE = [100, 200, 500, 600, 700, 800, 900, 1000, \n",
    "                2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n",
    "\n",
    "# Valores de greed a testar\n",
    "VALORES_GREED = [0.2, 0.5, 0.9]\n",
    "\n",
    "# Mapeamento de ações para índices\n",
    "ACAO_PARA_INDICE = {acao: i for i, acao in enumerate(ACOES)}\n",
    "INDICE_PARA_ACAO = {i: acao for i, acao in enumerate(ACOES)}\n",
    "\n",
    "print(\"Configuração do Q-Learning:\")\n",
    "print(f\"  α (alpha) = {ALPHA}\")\n",
    "print(f\"  γ (gamma) = {GAMMA}\")\n",
    "print(f\"  Número de passos de treino: {NUM_PASSOS_TREINO}\")\n",
    "print(f\"  Número de experiências: {NUM_EXPERIENCIAS}\")\n",
    "print(f\"  Valores de greed a testar: {VALORES_GREED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526d2e4",
   "metadata": {},
   "source": [
    "## 4. Funções Auxiliares de Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_Q():\n",
    "    \"\"\"Inicializa a tabela Q com zeros.\"\"\"\n",
    "    return np.zeros((NUM_STATES + 1, len(ACOES)))\n",
    "\n",
    "\n",
    "def atualizar_Q(Q, estado, acao, proximo_estado, recompensa_recebida):\n",
    "    \"\"\"Atualiza a tabela Q usando a equação de Q-Learning.\"\"\"\n",
    "    indice_acao = ACAO_PARA_INDICE[acao]\n",
    "    q_atual = Q[estado, indice_acao]\n",
    "    max_q_proximo = np.max(Q[proximo_estado, :])\n",
    "    novo_q = (1 - ALPHA) * q_atual + ALPHA * (recompensa_recebida + GAMMA * max_q_proximo)\n",
    "    Q[estado, indice_acao] = novo_q\n",
    "\n",
    "\n",
    "def escolher_melhor_acao(Q, estado):\n",
    "    \"\"\"Escolhe a melhor ação com desempate aleatório.\"\"\"\n",
    "    valores_q = Q[estado, :]\n",
    "    max_q = np.max(valores_q)\n",
    "    acoes_maximas = [i for i, q in enumerate(valores_q) if q == max_q]\n",
    "    indice_escolhido = np.random.choice(acoes_maximas)\n",
    "    return INDICE_PARA_ACAO[indice_escolhido]\n",
    "\n",
    "\n",
    "def escolher_acao_epsilon_greedy(Q, estado, greed):\n",
    "    \"\"\"Escolhe ação usando estratégia ε-greedy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Tabela Q\n",
    "        estado: Estado atual\n",
    "        greed: Probabilidade de escolher a melhor ação (0 a 1)\n",
    "    \n",
    "    Returns:\n",
    "        Ação escolhida\n",
    "    \"\"\"\n",
    "    if np.random.random() < greed:\n",
    "        # Exploração: escolhe a melhor ação\n",
    "        return escolher_melhor_acao(Q, estado)\n",
    "    else:\n",
    "        # Exploração: escolhe ação aleatória\n",
    "        return acao_aleatoria()\n",
    "\n",
    "\n",
    "def testar_politica_Q(Q, num_passos=NUM_PASSOS_TESTE):\n",
    "    \"\"\"Testa a política aprendida sem alterar Q.\"\"\"\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensa_total = 0.0\n",
    "    passos_executados = 0\n",
    "    \n",
    "    for _ in range(num_passos):\n",
    "        acao = escolher_melhor_acao(Q, estado_atual)\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        recompensa_total += r\n",
    "        passos_executados += 1\n",
    "        \n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "    \n",
    "    return recompensa_total / passos_executados if passos_executados > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"Funções de Q-Learning implementadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29740f47",
   "metadata": {},
   "source": [
    "## 5. Função de Treino ε-greedy com Greed Fixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3646a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino_epsilon_greedy_fixo(greed, seed=None):\n",
    "    \"\"\"Executa treino com ε-greedy com valor fixo de greed.\n",
    "    \n",
    "    Args:\n",
    "        greed: Probabilidade fixa de escolher a melhor ação\n",
    "        seed: Semente para reprodutibilidade\n",
    "    \n",
    "    Returns:\n",
    "        Q: Tabela Q final\n",
    "        recompensas_teste: Lista de recompensas nos pontos de teste\n",
    "        tempo_total: Tempo de execução\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    inicio = time.time()\n",
    "    Q = inicializar_Q()\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensas_teste = []\n",
    "    \n",
    "    for passo in range(1, NUM_PASSOS_TREINO + 1):\n",
    "        # Escolhe ação usando ε-greedy\n",
    "        acao = escolher_acao_epsilon_greedy(Q, estado_atual, greed)\n",
    "        \n",
    "        # Executa ação\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        \n",
    "        # Atualiza Q\n",
    "        atualizar_Q(Q, estado_atual, acao, proximo_estado, r)\n",
    "        \n",
    "        # Reset se atingiu objetivo\n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "        \n",
    "        # Testa política nos pontos definidos\n",
    "        if passo in PONTOS_TESTE:\n",
    "            recomp_teste = testar_politica_Q(Q)\n",
    "            recompensas_teste.append(recomp_teste)\n",
    "    \n",
    "    tempo_total = time.time() - inicio\n",
    "    return Q, recompensas_teste, tempo_total\n",
    "\n",
    "\n",
    "print(\"Função de treino ε-greedy (fixo) definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c9b9f",
   "metadata": {},
   "source": [
    "## 6. Função de Treino ε-greedy com Greed Crescente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_greed_crescente(passo_atual, total_passos, greed_inicial=0.3, greed_final=1.0):\n",
    "    \"\"\"Calcula o valor de greed crescente.\n",
    "    \n",
    "    Começa em greed_inicial (30%) e cresce linearmente até greed_final (100%).\n",
    "    Os primeiros 30% dos passos mantêm greed_inicial.\n",
    "    \n",
    "    Args:\n",
    "        passo_atual: Passo atual do treino\n",
    "        total_passos: Total de passos de treino\n",
    "        greed_inicial: Valor inicial de greed (default 0.3)\n",
    "        greed_final: Valor final de greed (default 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Valor de greed para o passo atual\n",
    "    \"\"\"\n",
    "    # Primeiros 30% dos passos mantêm greed inicial\n",
    "    passo_inicio_crescimento = int(0.3 * total_passos)\n",
    "    \n",
    "    if passo_atual <= passo_inicio_crescimento:\n",
    "        return greed_inicial\n",
    "    \n",
    "    # Crescimento linear de greed_inicial até greed_final\n",
    "    progresso = (passo_atual - passo_inicio_crescimento) / (total_passos - passo_inicio_crescimento)\n",
    "    greed = greed_inicial + progresso * (greed_final - greed_inicial)\n",
    "    \n",
    "    return min(greed, greed_final)  # Garante não ultrapassar greed_final\n",
    "\n",
    "\n",
    "def treino_epsilon_greedy_crescente(seed=None):\n",
    "    \"\"\"Executa treino com ε-greedy com greed crescente.\n",
    "    \n",
    "    Greed começa em 0.3 (30%) e cresce até 1.0 (100%).\n",
    "    \n",
    "    Args:\n",
    "        seed: Semente para reprodutibilidade\n",
    "    \n",
    "    Returns:\n",
    "        Q: Tabela Q final\n",
    "        recompensas_teste: Lista de recompensas nos pontos de teste\n",
    "        tempo_total: Tempo de execução\n",
    "        valores_greed: Lista de valores de greed ao longo do treino\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    inicio = time.time()\n",
    "    Q = inicializar_Q()\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensas_teste = []\n",
    "    valores_greed = []\n",
    "    \n",
    "    for passo in range(1, NUM_PASSOS_TREINO + 1):\n",
    "        # Calcula greed crescente\n",
    "        greed_atual = calcular_greed_crescente(passo, NUM_PASSOS_TREINO)\n",
    "        \n",
    "        # Escolhe ação usando ε-greedy\n",
    "        acao = escolher_acao_epsilon_greedy(Q, estado_atual, greed_atual)\n",
    "        \n",
    "        # Executa ação\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        \n",
    "        # Atualiza Q\n",
    "        atualizar_Q(Q, estado_atual, acao, proximo_estado, r)\n",
    "        \n",
    "        # Reset se atingiu objetivo\n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "        \n",
    "        # Testa política nos pontos definidos\n",
    "        if passo in PONTOS_TESTE:\n",
    "            recomp_teste = testar_politica_Q(Q)\n",
    "            recompensas_teste.append(recomp_teste)\n",
    "            valores_greed.append(greed_atual)\n",
    "    \n",
    "    tempo_total = time.time() - inicio\n",
    "    return Q, recompensas_teste, tempo_total, valores_greed\n",
    "\n",
    "\n",
    "print(\"Função de treino ε-greedy (crescente) definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc82ad1",
   "metadata": {},
   "source": [
    "## 7. Experimentos com Greed Fixo (0.2, 0.5, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENTOS COM VALORES FIXOS DE GREED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "resultados_fixos = {}\n",
    "\n",
    "for greed in VALORES_GREED:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TREINO COM GREED = {greed} ({int(greed*100)}% greedy, {int((1-greed)*100)}% random)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    todas_recompensas = []\n",
    "    todos_tempos = []\n",
    "    todas_Q = []\n",
    "    \n",
    "    print(f\"Executando {NUM_EXPERIENCIAS} experiências...\\n\")\n",
    "    \n",
    "    for exp in range(1, NUM_EXPERIENCIAS + 1):\n",
    "        Q, recompensas, tempo = treino_epsilon_greedy_fixo(greed, seed=42 + exp)\n",
    "        todas_recompensas.append(recompensas)\n",
    "        todos_tempos.append(tempo)\n",
    "        todas_Q.append(Q.copy())\n",
    "        \n",
    "        if exp % 5 == 0:\n",
    "            print(f\"Experiência {exp:2d}: tempo = {tempo:.2f}s, recompensa final = {recompensas[-1]:.6f}\")\n",
    "    \n",
    "    todas_recompensas = np.array(todas_recompensas)\n",
    "    todos_tempos = np.array(todos_tempos)\n",
    "    \n",
    "    # Armazena resultados\n",
    "    resultados_fixos[greed] = {\n",
    "        'recompensas': todas_recompensas,\n",
    "        'tempos': todos_tempos,\n",
    "        'Q_final': todas_Q[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTADOS - GREED = {greed}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Tempo médio: {np.mean(todos_tempos):.2f}s ± {np.std(todos_tempos):.2f}s\")\n",
    "    print(f\"Recompensa final média: {np.mean(todas_recompensas[:, -1]):.6f} ± {np.std(todas_recompensas[:, -1]):.6f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"EXPERIMENTOS COM GREED FIXO CONCLUÍDOS!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf251c53",
   "metadata": {},
   "source": [
    "## 8. Experimento com Greed Crescente (0.3 → 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENTO COM GREED CRESCENTE (0.3 → 1.0)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOs primeiros 30% dos passos mantêm greed = 0.3\")\n",
    "print(\"Depois, greed cresce linearmente até 1.0 no passo final\\n\")\n",
    "\n",
    "todas_recompensas_crescente = []\n",
    "todos_tempos_crescente = []\n",
    "todas_Q_crescente = []\n",
    "todos_valores_greed = []\n",
    "\n",
    "print(f\"Executando {NUM_EXPERIENCIAS} experiências...\\n\")\n",
    "\n",
    "for exp in range(1, NUM_EXPERIENCIAS + 1):\n",
    "    Q, recompensas, tempo, valores_greed = treino_epsilon_greedy_crescente(seed=42 + exp)\n",
    "    todas_recompensas_crescente.append(recompensas)\n",
    "    todos_tempos_crescente.append(tempo)\n",
    "    todas_Q_crescente.append(Q.copy())\n",
    "    todos_valores_greed.append(valores_greed)\n",
    "    \n",
    "    if exp % 5 == 0:\n",
    "        print(f\"Experiência {exp:2d}: tempo = {tempo:.2f}s, recompensa final = {recompensas[-1]:.6f}\")\n",
    "\n",
    "todas_recompensas_crescente = np.array(todas_recompensas_crescente)\n",
    "todos_tempos_crescente = np.array(todos_tempos_crescente)\n",
    "valores_greed_medios = np.mean(todos_valores_greed, axis=0)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTADOS - GREED CRESCENTE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Tempo médio: {np.mean(todos_tempos_crescente):.2f}s ± {np.std(todos_tempos_crescente):.2f}s\")\n",
    "print(f\"Recompensa final média: {np.mean(todas_recompensas_crescente[:, -1]):.6f} ± {np.std(todas_recompensas_crescente[:, -1]):.6f}\")\n",
    "print(f\"Greed inicial: {valores_greed_medios[0]:.3f}\")\n",
    "print(f\"Greed final: {valores_greed_medios[-1]:.3f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d8388",
   "metadata": {},
   "source": [
    "## 9. Visualização: Comparação de Todos os Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Cores para cada método\n",
    "cores = {\n",
    "    0.2: 'red',\n",
    "    0.5: 'orange',\n",
    "    0.9: 'blue',\n",
    "    'crescente': 'green'\n",
    "}\n",
    "\n",
    "# Plot greed fixo\n",
    "for greed in VALORES_GREED:\n",
    "    recompensas = resultados_fixos[greed]['recompensas']\n",
    "    medias = np.mean(recompensas, axis=0)\n",
    "    desvios = np.std(recompensas, axis=0)\n",
    "    \n",
    "    label = f'Greed = {greed} ({int(greed*100)}% greedy)'\n",
    "    ax.plot(PONTOS_TESTE, medias, color=cores[greed], linewidth=2.5, \n",
    "            marker='o', markersize=7, label=label, zorder=3)\n",
    "    ax.fill_between(PONTOS_TESTE, medias - desvios, medias + desvios, \n",
    "                    alpha=0.15, color=cores[greed])\n",
    "\n",
    "# Plot greed crescente\n",
    "medias_crescente = np.mean(todas_recompensas_crescente, axis=0)\n",
    "desvios_crescente = np.std(todas_recompensas_crescente, axis=0)\n",
    "\n",
    "ax.plot(PONTOS_TESTE, medias_crescente, color=cores['crescente'], linewidth=3, \n",
    "        marker='D', markersize=8, label='Greed Crescente (0.3→1.0)', zorder=4)\n",
    "ax.fill_between(PONTOS_TESTE, medias_crescente - desvios_crescente, \n",
    "                medias_crescente + desvios_crescente, \n",
    "                alpha=0.15, color=cores['crescente'])\n",
    "\n",
    "ax.set_xlabel('Número de Passos de Treino', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Recompensa Média por Passo (Teste)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Exercício 3: Comparação de Estratégias ε-greedy\\n'\n",
    "             f'(α={ALPHA}, γ={GAMMA}, {NUM_EXPERIENCIAS} experiências)', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.legend(fontsize=11, loc='lower right', framealpha=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercicio3_comparacao_epsilon_greedy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráfico salvo como 'exercicio3_comparacao_epsilon_greedy.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f40a3b",
   "metadata": {},
   "source": [
    "## 10. Visualização: Evolução do Greed Crescente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a27fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar array de todos os passos para visualização\n",
    "todos_passos = np.arange(1, NUM_PASSOS_TREINO + 1)\n",
    "todos_greeds = [calcular_greed_crescente(p, NUM_PASSOS_TREINO) for p in todos_passos]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Subplot 1: Evolução do greed\n",
    "ax1.plot(todos_passos, todos_greeds, color='purple', linewidth=2.5)\n",
    "ax1.axhline(y=0.3, color='red', linestyle='--', linewidth=1.5, \n",
    "            label='Greed inicial (0.3)', alpha=0.7)\n",
    "ax1.axhline(y=1.0, color='green', linestyle='--', linewidth=1.5, \n",
    "            label='Greed final (1.0)', alpha=0.7)\n",
    "ax1.axvline(x=0.3*NUM_PASSOS_TREINO, color='orange', linestyle=':', \n",
    "            linewidth=2, label='30% dos passos', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Passo de Treino', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Valor de Greed (ε)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Evolução do Greed ao Longo do Treino', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11, loc='right')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# Subplot 2: Comparação com greed fixo nos pontos de teste\n",
    "medias_crescente = np.mean(todas_recompensas_crescente, axis=0)\n",
    "medias_09 = np.mean(resultados_fixos[0.9]['recompensas'], axis=0)\n",
    "medias_05 = np.mean(resultados_fixos[0.5]['recompensas'], axis=0)\n",
    "\n",
    "ax2.plot(PONTOS_TESTE, medias_crescente, 'g-', linewidth=3, \n",
    "         marker='D', markersize=8, label='Greed Crescente (0.3→1.0)', zorder=3)\n",
    "ax2.plot(PONTOS_TESTE, medias_09, 'b--', linewidth=2, \n",
    "         marker='o', markersize=6, label='Greed = 0.9 (fixo)', alpha=0.7)\n",
    "ax2.plot(PONTOS_TESTE, medias_05, 'orange', linewidth=2, linestyle='--',\n",
    "         marker='s', markersize=6, label='Greed = 0.5 (fixo)', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Número de Passos de Treino', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Recompensa Média por Passo', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Desempenho: Greed Crescente vs Greed Fixo', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercicio3_greed_crescente_detalhe.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráfico salvo como 'exercicio3_greed_crescente_detalhe.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ec461",
   "metadata": {},
   "source": [
    "## 11. Mapas de Calor das Utilidades Aprendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b50582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_mapa_calor_utilidade(Q, titulo=\"Mapa de Calor da Utilidade\", \n",
    "                               nome_ficheiro=\"mapa_calor.png\"):\n",
    "    \"\"\"Cria mapa de calor mostrando U(s) = max_a Q[s,a].\"\"\"\n",
    "    utilidades = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    \n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        utilidades[linha, coluna] = np.max(Q[estado, :])\n",
    "    \n",
    "    for estado in PAREDES:\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        utilidades[linha, coluna] = np.nan\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 11))\n",
    "    im = ax.imshow(utilidades, cmap='YlOrRd', interpolation='nearest')\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Utilidade U(s) = max Q[s,a]', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        \n",
    "        if estado in PAREDES:\n",
    "            ax.text(coluna, linha, 'X', ha='center', va='center',\n",
    "                   color='black', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            utilidade = utilidades[linha, coluna]\n",
    "            cor_texto = 'white' if utilidade > np.nanmax(utilidades) * 0.5 else 'black'\n",
    "            ax.text(coluna, linha, f'{estado}\\n{utilidade:.1f}', \n",
    "                   ha='center', va='center', color=cor_texto, \n",
    "                   fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(GRID_SIZE))\n",
    "    ax.set_yticks(range(GRID_SIZE))\n",
    "    ax.set_xticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_yticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_xlabel('Coluna', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Linha', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.set_xticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which='minor', size=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(nome_ficheiro, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Mapa de calor salvo como '{nome_ficheiro}'\")\n",
    "\n",
    "\n",
    "print(\"Criando mapas de calor...\\n\")\n",
    "\n",
    "# Mapas para valores fixos de greed\n",
    "for greed in VALORES_GREED:\n",
    "    criar_mapa_calor_utilidade(\n",
    "        resultados_fixos[greed]['Q_final'],\n",
    "        titulo=f\"Mapa de Calor - ε-greedy com greed={greed}\",\n",
    "        nome_ficheiro=f\"exercicio3_mapa_calor_greed_{int(greed*10)}.png\"\n",
    "    )\n",
    "\n",
    "# Mapa para greed crescente\n",
    "criar_mapa_calor_utilidade(\n",
    "    todas_Q_crescente[-1],\n",
    "    titulo=\"Mapa de Calor - ε-greedy com greed crescente (0.3→1.0)\",\n",
    "    nome_ficheiro=\"exercicio3_mapa_calor_greed_crescente.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7def91",
   "metadata": {},
   "source": [
    "## 12. Análise Estatística Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*90)\n",
    "print(\"ANÁLISE ESTATÍSTICA COMPARATIVA - EXERCÍCIO 3\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"1. TEMPO DE EXECUÇÃO\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for greed in VALORES_GREED:\n",
    "    tempos = resultados_fixos[greed]['tempos']\n",
    "    print(f\"Greed = {greed:3.1f}: {np.mean(tempos):6.2f}s ± {np.std(tempos):5.2f}s\")\n",
    "\n",
    "print(f\"Crescente:   {np.mean(todos_tempos_crescente):6.2f}s ± {np.std(todos_tempos_crescente):5.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"2. RECOMPENSA FINAL (após 20000 passos)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "resultados_finais = {}\n",
    "\n",
    "for greed in VALORES_GREED:\n",
    "    recomp_final = resultados_fixos[greed]['recompensas'][:, -1]\n",
    "    media = np.mean(recomp_final)\n",
    "    desvio = np.std(recomp_final)\n",
    "    resultados_finais[greed] = media\n",
    "    print(f\"Greed = {greed:3.1f}: {media:.6f} ± {desvio:.6f}\")\n",
    "\n",
    "recomp_final_crescente = todas_recompensas_crescente[:, -1]\n",
    "media_crescente = np.mean(recomp_final_crescente)\n",
    "desvio_crescente = np.std(recomp_final_crescente)\n",
    "resultados_finais['crescente'] = media_crescente\n",
    "print(f\"Crescente:   {media_crescente:.6f} ± {desvio_crescente:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"3. COMPARAÇÃO DE DESEMPENHO\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "melhor_metodo = max(resultados_finais, key=resultados_finais.get)\n",
    "melhor_valor = resultados_finais[melhor_metodo]\n",
    "\n",
    "print(f\"\\nMelhor método: {melhor_metodo if isinstance(melhor_metodo, str) else f'Greed = {melhor_metodo}'}\")\n",
    "print(f\"Recompensa: {melhor_valor:.6f}\\n\")\n",
    "\n",
    "print(\"Melhoria relativa em relação aos outros métodos:\")\n",
    "for metodo, valor in sorted(resultados_finais.items(), key=lambda x: x[1], reverse=True):\n",
    "    if metodo != melhor_metodo:\n",
    "        melhoria = ((melhor_valor - valor) / valor * 100)\n",
    "        nome_metodo = metodo if isinstance(metodo, str) else f'Greed = {metodo}'\n",
    "        print(f\"  vs {nome_metodo:15s}: {melhoria:+6.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"4. TAXA DE EXPLORAÇÃO vs EXPLORAÇÃO\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for greed in VALORES_GREED:\n",
    "    exploracao = int((1 - greed) * 100)\n",
    "    exploracao_word = int(greed * 100)\n",
    "    print(f\"Greed = {greed}: {exploracao_word}% exploração (greedy), {exploracao}% exploração (random)\")\n",
    "\n",
    "print(f\"Crescente:   30% exploração inicial → 100% exploração final\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fe4cd",
   "metadata": {},
   "source": [
    "## 13. Boxplot Comparativo de Recompensas Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0896d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Preparar dados para boxplot\n",
    "dados_boxplot = []\n",
    "labels = []\n",
    "\n",
    "for greed in VALORES_GREED:\n",
    "    recomp_final = resultados_fixos[greed]['recompensas'][:, -1]\n",
    "    dados_boxplot.append(recomp_final)\n",
    "    labels.append(f'Greed={greed}\\n({int(greed*100)}% greedy)')\n",
    "\n",
    "dados_boxplot.append(todas_recompensas_crescente[:, -1])\n",
    "labels.append('Greed\\nCrescente\\n(0.3→1.0)')\n",
    "\n",
    "bp = ax.boxplot(dados_boxplot, labels=labels, patch_artist=True,\n",
    "                notch=True, showmeans=True,\n",
    "                meanprops=dict(marker='D', markerfacecolor='red', markersize=8),\n",
    "                medianprops=dict(color='darkblue', linewidth=2))\n",
    "\n",
    "# Colorir boxes\n",
    "cores_boxes = ['#ffcccc', '#ffcc99', '#ccccff', '#ccffcc']\n",
    "for patch, cor in zip(bp['boxes'], cores_boxes):\n",
    "    patch.set_facecolor(cor)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Recompensa Média por Passo (após 20000 passos)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Exercício 3: Comparação de Desempenho Final\\n'\n",
    "             f'Estratégias ε-greedy ({NUM_EXPERIENCIAS} experiências)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Adicionar legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='white', edgecolor='black', label='Mediana (linha azul)'),\n",
    "    plt.Line2D([0], [0], marker='D', color='w', markerfacecolor='red', \n",
    "               markersize=8, label='Média (diamante vermelho)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercicio3_boxplot_comparacao.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráfico salvo como 'exercicio3_boxplot_comparacao.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3079581",
   "metadata": {},
   "source": [
    "## 14. Conclusões do Exercício 3\n",
    "\n",
    "### Análise dos Resultados\n",
    "\n",
    "#### 1. Greed = 0.2 (20% greedy, 80% random)\n",
    "\n",
    "**Características:**\n",
    "- Alta exploração (80% ações aleatórias)\n",
    "- Baixa exploração (20% melhor ação)\n",
    "\n",
    "**Resultados:**\n",
    "- Convergência lenta\n",
    "- Muita exploração do espaço de estados\n",
    "- Desempenho final limitado pela aleatoriedade excessiva\n",
    "- Similar ao Random Walk puro\n",
    "\n",
    "#### 2. Greed = 0.5 (50% greedy, 50% random)\n",
    "\n",
    "**Características:**\n",
    "- Equilíbrio entre exploração e exploração\n",
    "- 50% de cada estratégia\n",
    "\n",
    "**Resultados:**\n",
    "- Convergência moderada\n",
    "- Boa exploração do espaço\n",
    "- Desempenho intermediário\n",
    "- Bom compromisso para ambientes desconhecidos\n",
    "\n",
    "#### 3. Greed = 0.9 (90% greedy, 10% random)\n",
    "\n",
    "**Características:**\n",
    "- Alta exploração (90% melhor ação)\n",
    "- Baixa exploração (10% ações aleatórias)\n",
    "\n",
    "**Resultados:**\n",
    "- Convergência rápida\n",
    "- Exploração focada\n",
    "- Melhor desempenho entre os valores fixos\n",
    "- Mínima aleatoriedade mantém descoberta de alternativas\n",
    "\n",
    "#### 4. Greed Crescente (0.3 → 1.0)\n",
    "\n",
    "**Características:**\n",
    "- **Fase inicial (primeiros 30%)**: greed = 0.3 (exploração)\n",
    "- **Fase crescente (70% restantes)**: greed aumenta linearmente até 1.0 (exploração)\n",
    "\n",
    "**Estratégia:**\n",
    "```\n",
    "Passos 1-6000:      greed = 0.3  (30% greedy, 70% random)\n",
    "Passos 6001-20000:  greed: 0.3 → 1.0 (crescimento linear)\n",
    "Passo 20000:        greed = 1.0  (100% greedy, 0% random)\n",
    "```\n",
    "\n",
    "**Resultados:**\n",
    "- **Melhor desempenho global**\n",
    "- Combina vantagens de ambas as fases:\n",
    "  - Exploração inicial ampla\n",
    "  - Exploração progressiva à medida que aprende\n",
    "- Convergência final muito forte\n",
    "- Tabela Q mais robusta\n",
    "\n",
    "### Comparação Final\n",
    "\n",
    "**Ranking de Desempenho (do melhor ao pior):**\n",
    "\n",
    "1. **Greed Crescente** (0.3 → 1.0)\n",
    "2. **Greed = 0.9** (fixo)\n",
    "3. **Greed = 0.5** (fixo)\n",
    "4. **Greed = 0.2** (fixo)\n",
    "\n",
    "### Lições Aprendidas\n",
    "\n",
    "1. **Dilema Exploração-Exploração:**\n",
    "   - Muita exploração → convergência lenta\n",
    "   - Muita exploração → pode ficar presa em mínimos locais\n",
    "   - **Solução:** estratégia adaptativa\n",
    "\n",
    "2. **Estratégia Crescente é Superior:**\n",
    "   - Explora amplamente no início\n",
    "   - Refina progressivamente\n",
    "   - Converge para política ótima\n",
    "\n",
    "3. **Importância da Exploração Inicial:**\n",
    "   - Primeiros 30% com exploração alta garante descoberta\n",
    "   - Evita convergência prematura\n",
    "\n",
    "4. **Transição Suave:**\n",
    "   - Crescimento linear permite adaptação gradual\n",
    "   - Não há mudanças bruscas de comportamento\n",
    "\n",
    "### Aplicações Práticas\n",
    "\n",
    "A estratégia **ε-greedy com greed crescente** é amplamente usada em:\n",
    "- Robótica (aprendizagem de controlo)\n",
    "- Jogos (IA adaptativa)\n",
    "- Sistemas de recomendação\n",
    "- Otimização de recursos\n",
    "\n",
    "### Próximos Passos\n",
    "\n",
    "Melhorias possíveis:\n",
    "- Testar diferentes curvas de crescimento (exponencial, logarítmica)\n",
    "- Variar o ponto de início do crescimento (testar 20%, 40%, etc.)\n",
    "- Implementar decay exponencial: $\\epsilon_t = \\epsilon_0 \\cdot e^{-\\lambda t}$\n",
    "- Explorar estratégias adaptativas baseadas no desempenho"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
