{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afc1d56",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "\n",
    "# Configuração para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"Seed utilizada: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57006cfc",
   "metadata": {},
   "source": [
    "## 2. Definição do Ambiente\n",
    "\n",
    "### 2.a) Configuração da Grelha e Paredes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensões da grelha\n",
    "GRID_SIZE = 10\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE  # 100 estados\n",
    "\n",
    "# Estados especiais\n",
    "ESTADO_INICIAL = 1\n",
    "ESTADO_OBJETIVO = 100\n",
    "\n",
    "# Definição das paredes (estados inacessíveis)\n",
    "# Com base na Figura 1 do enunciado, definimos algumas paredes\n",
    "PAREDES = {\n",
    "    23, 24, 25, 26,  # Parede horizontal superior\n",
    "    33, 34, 35, 36,  # Continuação\n",
    "    54, 64, 74, 84,  # Parede vertical à direita\n",
    "    55, 65, 75, 85,  # Continuação\n",
    "}\n",
    "\n",
    "# Ações possíveis\n",
    "ACOES = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "# Parâmetros de simulação\n",
    "MAX_PASSOS_POR_EPISODIO = 1000\n",
    "NUM_EPISODIOS = 30\n",
    "\n",
    "print(f\"Ambiente configurado:\")\n",
    "print(f\"  - Grelha: {GRID_SIZE}×{GRID_SIZE} = {NUM_STATES} estados\")\n",
    "print(f\"  - Estado inicial: {ESTADO_INICIAL}\")\n",
    "print(f\"  - Estado objetivo: {ESTADO_OBJETIVO}\")\n",
    "print(f\"  - Número de paredes: {len(PAREDES)}\")\n",
    "print(f\"  - Ações disponíveis: {ACOES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbb7f2",
   "metadata": {},
   "source": [
    "### 2.b) Função de Transição de Estado\n",
    "\n",
    "Implementação de $s' = f(s, a)$ que calcula o próximo estado após aplicar uma ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transicao_estado(estado: int, acao: str) -> int:\n",
    "    \"\"\"\n",
    "    Calcula o próximo estado após aplicar uma ação.\n",
    "    \n",
    "    Args:\n",
    "        estado: Estado atual (1 a 100)\n",
    "        acao: Ação a executar ('UP', 'DOWN', 'LEFT', 'RIGHT')\n",
    "    \n",
    "    Returns:\n",
    "        Próximo estado (1 a 100)\n",
    "    \"\"\"\n",
    "    # Converter estado para coordenadas (linha, coluna)\n",
    "    # Estado 1 = (0, 0), Estado 10 = (0, 9), Estado 11 = (1, 0), etc.\n",
    "    linha = (estado - 1) // GRID_SIZE\n",
    "    coluna = (estado - 1) % GRID_SIZE\n",
    "    \n",
    "    # Calcular novo estado baseado na ação\n",
    "    nova_linha, nova_coluna = linha, coluna\n",
    "    \n",
    "    if acao == 'UP':\n",
    "        nova_linha = linha - 1\n",
    "    elif acao == 'DOWN':\n",
    "        nova_linha = linha + 1\n",
    "    elif acao == 'LEFT':\n",
    "        nova_coluna = coluna - 1\n",
    "    elif acao == 'RIGHT':\n",
    "        nova_coluna = coluna + 1\n",
    "    \n",
    "    # Verificar se o movimento é válido\n",
    "    # 1. Dentro dos limites da grelha\n",
    "    if nova_linha < 0 or nova_linha >= GRID_SIZE or nova_coluna < 0 or nova_coluna >= GRID_SIZE:\n",
    "        return estado  # Permanece no mesmo estado\n",
    "    \n",
    "    # 2. Não é uma parede\n",
    "    novo_estado = nova_linha * GRID_SIZE + nova_coluna + 1\n",
    "    if novo_estado in PAREDES:\n",
    "        return estado  # Permanece no mesmo estado\n",
    "    \n",
    "    return novo_estado\n",
    "\n",
    "\n",
    "# Testes da função de transição\n",
    "print(\"Testes da função de transição:\")\n",
    "print(f\"  f(1, 'RIGHT') = {transicao_estado(1, 'RIGHT')} (esperado: 2)\")\n",
    "print(f\"  f(1, 'DOWN') = {transicao_estado(1, 'DOWN')} (esperado: 11)\")\n",
    "print(f\"  f(1, 'UP') = {transicao_estado(1, 'UP')} (esperado: 1 - limite superior)\")\n",
    "print(f\"  f(1, 'LEFT') = {transicao_estado(1, 'LEFT')} (esperado: 1 - limite esquerdo)\")\n",
    "print(f\"  f(100, 'RIGHT') = {transicao_estado(100, 'RIGHT')} (esperado: 100 - limite direito)\")\n",
    "print(f\"  f(100, 'DOWN') = {transicao_estado(100, 'DOWN')} (esperado: 100 - limite inferior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84160cc6",
   "metadata": {},
   "source": [
    "### 2.c) Função de Recompensa\n",
    "\n",
    "Implementação de $r(s)$ que retorna a recompensa associada a um estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd259f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompensa(estado: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a recompensa de um estado.\n",
    "    \n",
    "    Args:\n",
    "        estado: Estado atual (1 a 100)\n",
    "    \n",
    "    Returns:\n",
    "        Recompensa: 100 se estado objetivo, 0 caso contrário\n",
    "    \"\"\"\n",
    "    if estado == ESTADO_OBJETIVO:\n",
    "        return 100.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# Testes da função de recompensa\n",
    "print(\"Testes da função de recompensa:\")\n",
    "print(f\"  r(1) = {recompensa(1)} (esperado: 0)\")\n",
    "print(f\"  r(50) = {recompensa(50)} (esperado: 0)\")\n",
    "print(f\"  r(100) = {recompensa(100)} (esperado: 100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8464fa1",
   "metadata": {},
   "source": [
    "### 2.d) Função de Ação Aleatória\n",
    "\n",
    "Seleciona uma ação aleatória com distribuição uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc78f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acao_aleatoria() -> str:\n",
    "    \"\"\"\n",
    "    Escolhe uma ação aleatória com distribuição uniforme.\n",
    "    \n",
    "    Returns:\n",
    "        Uma das ações: 'UP', 'DOWN', 'LEFT', 'RIGHT'\n",
    "    \"\"\"\n",
    "    return np.random.choice(ACOES)\n",
    "\n",
    "\n",
    "# Teste da função de ação aleatória\n",
    "print(\"Teste da função de ação aleatória (10 amostras):\")\n",
    "acoes_teste = [acao_aleatoria() for _ in range(10)]\n",
    "print(f\"  Ações geradas: {acoes_teste}\")\n",
    "\n",
    "# Verificar distribuição\n",
    "print(\"\\nDistribuição de 1000 ações aleatórias:\")\n",
    "acoes_distribuicao = [acao_aleatoria() for _ in range(1000)]\n",
    "for acao in ACOES:\n",
    "    contagem = acoes_distribuicao.count(acao)\n",
    "    percentagem = (contagem / 1000) * 100\n",
    "    print(f\"  {acao}: {contagem} vezes ({percentagem:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1104a",
   "metadata": {},
   "source": [
    "### 2.e) Função de Simulação de Episódio\n",
    "\n",
    "Executa um episódio completo com ações aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simular_episodio() -> Tuple[float, int, float]:\n",
    "    \"\"\"\n",
    "    Simula um episódio completo com ações aleatórias.\n",
    "    \n",
    "    Returns:\n",
    "        Tuplo com:\n",
    "        - recompensa_media_por_passo: Recompensa média obtida por passo\n",
    "        - num_passos: Número de passos até atingir o objetivo (ou MAX_PASSOS)\n",
    "        - tempo_execucao: Tempo de execução em segundos\n",
    "    \"\"\"\n",
    "    inicio = time.time()\n",
    "    \n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensa_total = 0.0\n",
    "    num_passos = 0\n",
    "    \n",
    "    for passo in range(MAX_PASSOS_POR_EPISODIO):\n",
    "        # Escolher ação aleatória\n",
    "        acao = acao_aleatoria()\n",
    "        \n",
    "        # Aplicar ação e obter novo estado\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        \n",
    "        # Obter recompensa\n",
    "        r = recompensa(proximo_estado)\n",
    "        recompensa_total += r\n",
    "        \n",
    "        num_passos += 1\n",
    "        \n",
    "        # Verificar se atingiu o objetivo\n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            break\n",
    "        \n",
    "        # Atualizar estado\n",
    "        estado_atual = proximo_estado\n",
    "    \n",
    "    tempo_execucao = time.time() - inicio\n",
    "    recompensa_media_por_passo = recompensa_total / num_passos if num_passos > 0 else 0.0\n",
    "    \n",
    "    return recompensa_media_por_passo, num_passos, tempo_execucao\n",
    "\n",
    "\n",
    "# Teste de um episódio individual\n",
    "print(\"Teste de um episódio individual:\")\n",
    "recomp_media, n_passos, tempo = simular_episodio()\n",
    "print(f\"  Recompensa média por passo: {recomp_media:.4f}\")\n",
    "print(f\"  Número de passos: {n_passos}\")\n",
    "print(f\"  Tempo de execução: {tempo:.6f} segundos\")\n",
    "if n_passos < MAX_PASSOS_POR_EPISODIO:\n",
    "    print(f\"  ✓ Objetivo atingido!\")\n",
    "else:\n",
    "    print(f\"  ✗ Objetivo não atingido (máximo de passos alcançado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c6dba",
   "metadata": {},
   "source": [
    "## 3. Execução de 30 Episódios e Recolha de Dados\n",
    "\n",
    "### 3.a) Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar resultados\n",
    "recompensas_medias = []\n",
    "numeros_passos = []\n",
    "tempos_execucao = []\n",
    "\n",
    "print(f\"Executando {NUM_EPISODIOS} episódios...\\n\")\n",
    "\n",
    "for episodio in range(1, NUM_EPISODIOS + 1):\n",
    "    recomp_media, n_passos, tempo = simular_episodio()\n",
    "    \n",
    "    recompensas_medias.append(recomp_media)\n",
    "    numeros_passos.append(n_passos)\n",
    "    tempos_execucao.append(tempo)\n",
    "    \n",
    "    # Mostrar progresso a cada 5 episódios\n",
    "    if episodio % 5 == 0:\n",
    "        print(f\"Episódio {episodio:2d}: {n_passos:4d} passos, \"\n",
    "              f\"recompensa média = {recomp_media:.4f}, \"\n",
    "              f\"tempo = {tempo:.6f}s\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Simulação concluída!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32debb",
   "metadata": {},
   "source": [
    "### 3.b) Cálculo de Estatísticas (Baselines)\n",
    "\n",
    "Estas estatísticas representam o desempenho do sistema com ações puramente aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d455bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para arrays numpy para cálculos\n",
    "recompensas_medias = np.array(recompensas_medias)\n",
    "numeros_passos = np.array(numeros_passos)\n",
    "tempos_execucao = np.array(tempos_execucao)\n",
    "\n",
    "# Calcular estatísticas\n",
    "stats = {\n",
    "    'recompensa_media': {\n",
    "        'media': np.mean(recompensas_medias),\n",
    "        'desvio': np.std(recompensas_medias),\n",
    "        'mediana': np.median(recompensas_medias),\n",
    "        'min': np.min(recompensas_medias),\n",
    "        'max': np.max(recompensas_medias)\n",
    "    },\n",
    "    'num_passos': {\n",
    "        'media': np.mean(numeros_passos),\n",
    "        'desvio': np.std(numeros_passos),\n",
    "        'mediana': np.median(numeros_passos),\n",
    "        'min': np.min(numeros_passos),\n",
    "        'max': np.max(numeros_passos)\n",
    "    },\n",
    "    'tempo_execucao': {\n",
    "        'media': np.mean(tempos_execucao),\n",
    "        'desvio': np.std(tempos_execucao),\n",
    "        'mediana': np.median(tempos_execucao),\n",
    "        'min': np.min(tempos_execucao),\n",
    "        'max': np.max(tempos_execucao)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apresentar resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ESTATÍSTICAS BASELINE (Política Aleatória)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. RECOMPENSA MÉDIA POR PASSO:\")\n",
    "print(f\"   Média:          {stats['recompensa_media']['media']:.6f}\")\n",
    "print(f\"   Desvio padrão:  {stats['recompensa_media']['desvio']:.6f}\")\n",
    "print(f\"   Mediana:        {stats['recompensa_media']['mediana']:.6f}\")\n",
    "print(f\"   Mínimo:         {stats['recompensa_media']['min']:.6f}\")\n",
    "print(f\"   Máximo:         {stats['recompensa_media']['max']:.6f}\")\n",
    "\n",
    "print(\"\\n2. NÚMERO DE PASSOS ATÉ ATINGIR O OBJETIVO:\")\n",
    "print(f\"   Média:          {stats['num_passos']['media']:.2f}\")\n",
    "print(f\"   Desvio padrão:  {stats['num_passos']['desvio']:.2f}\")\n",
    "print(f\"   Mediana:        {stats['num_passos']['mediana']:.0f}\")\n",
    "print(f\"   Mínimo:         {stats['num_passos']['min']:.0f}\")\n",
    "print(f\"   Máximo:         {stats['num_passos']['max']:.0f}\")\n",
    "\n",
    "print(\"\\n3. TEMPO MÉDIO DE EXECUÇÃO:\")\n",
    "print(f\"   Média:          {stats['tempo_execucao']['media']:.6f} segundos\")\n",
    "print(f\"   Desvio padrão:  {stats['tempo_execucao']['desvio']:.6f} segundos\")\n",
    "print(f\"   Mediana:        {stats['tempo_execucao']['mediana']:.6f} segundos\")\n",
    "print(f\"   Mínimo:         {stats['tempo_execucao']['min']:.6f} segundos\")\n",
    "print(f\"   Máximo:         {stats['tempo_execucao']['max']:.6f} segundos\")\n",
    "\n",
    "# Contar episódios bem-sucedidos\n",
    "episodios_sucesso = np.sum(numeros_passos < MAX_PASSOS_POR_EPISODIO)\n",
    "taxa_sucesso = (episodios_sucesso / NUM_EPISODIOS) * 100\n",
    "\n",
    "print(\"\\n4. TAXA DE SUCESSO:\")\n",
    "print(f\"   Episódios bem-sucedidos: {episodios_sucesso}/{NUM_EPISODIOS}\")\n",
    "print(f\"   Taxa de sucesso:         {taxa_sucesso:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f8ab4",
   "metadata": {},
   "source": [
    "## 4. Visualização Gráfica\n",
    "\n",
    "### 4.a) Boxplots das Métricas de Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar estilo dos gráficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Cores para os boxplots\n",
    "cores = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# 1. Recompensa Média por Passo\n",
    "bp1 = axes[0].boxplot([recompensas_medias], \n",
    "                       widths=0.6,\n",
    "                       patch_artist=True,\n",
    "                       medianprops=dict(color='red', linewidth=2),\n",
    "                       boxprops=dict(facecolor=cores[0], alpha=0.7),\n",
    "                       whiskerprops=dict(linewidth=1.5),\n",
    "                       capprops=dict(linewidth=1.5))\n",
    "axes[0].set_ylabel('Recompensa Média', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Política Aleatória', fontsize=11)\n",
    "axes[0].set_title('Recompensa Média por Passo\\n(30 Episódios)', \n",
    "                  fontsize=13, fontweight='bold', pad=15)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks([1])\n",
    "axes[0].set_xticklabels(['Random Walk'])\n",
    "\n",
    "# Adicionar estatísticas no gráfico\n",
    "axes[0].text(1.3, stats['recompensa_media']['media'], \n",
    "             f\"μ = {stats['recompensa_media']['media']:.4f}\\nσ = {stats['recompensa_media']['desvio']:.4f}\",\n",
    "             fontsize=9, verticalalignment='center')\n",
    "\n",
    "# 2. Número de Passos\n",
    "bp2 = axes[1].boxplot([numeros_passos], \n",
    "                       widths=0.6,\n",
    "                       patch_artist=True,\n",
    "                       medianprops=dict(color='red', linewidth=2),\n",
    "                       boxprops=dict(facecolor=cores[1], alpha=0.7),\n",
    "                       whiskerprops=dict(linewidth=1.5),\n",
    "                       capprops=dict(linewidth=1.5))\n",
    "axes[1].set_ylabel('Número de Passos', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Política Aleatória', fontsize=11)\n",
    "axes[1].set_title('Número de Passos até ao Objetivo\\n(30 Episódios, máx. 1000)', \n",
    "                  fontsize=13, fontweight='bold', pad=15)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks([1])\n",
    "axes[1].set_xticklabels(['Random Walk'])\n",
    "axes[1].axhline(y=MAX_PASSOS_POR_EPISODIO, color='red', linestyle='--', \n",
    "                linewidth=1, alpha=0.5, label='Máximo (1000)')\n",
    "axes[1].legend(loc='upper right', fontsize=9)\n",
    "\n",
    "# Adicionar estatísticas no gráfico\n",
    "axes[1].text(1.3, stats['num_passos']['media'], \n",
    "             f\"μ = {stats['num_passos']['media']:.1f}\\nσ = {stats['num_passos']['desvio']:.1f}\",\n",
    "             fontsize=9, verticalalignment='center')\n",
    "\n",
    "# 3. Tempo de Execução\n",
    "bp3 = axes[2].boxplot([tempos_execucao * 1000],  # Converter para milissegundos\n",
    "                       widths=0.6,\n",
    "                       patch_artist=True,\n",
    "                       medianprops=dict(color='red', linewidth=2),\n",
    "                       boxprops=dict(facecolor=cores[2], alpha=0.7),\n",
    "                       whiskerprops=dict(linewidth=1.5),\n",
    "                       capprops=dict(linewidth=1.5))\n",
    "axes[2].set_ylabel('Tempo de Execução (ms)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Política Aleatória', fontsize=11)\n",
    "axes[2].set_title('Tempo Médio de Execução\\n(30 Episódios)', \n",
    "                  fontsize=13, fontweight='bold', pad=15)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xticks([1])\n",
    "axes[2].set_xticklabels(['Random Walk'])\n",
    "\n",
    "# Adicionar estatísticas no gráfico\n",
    "axes[2].text(1.3, stats['tempo_execucao']['media'] * 1000, \n",
    "             f\"μ = {stats['tempo_execucao']['media']*1000:.2f} ms\\nσ = {stats['tempo_execucao']['desvio']*1000:.2f} ms\",\n",
    "             fontsize=9, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercicio1_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráficos salvos como 'exercicio1_boxplots.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603781e",
   "metadata": {},
   "source": [
    "### 4.b) Visualização do Ambiente (Grelha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fa244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_ambiente():\n",
    "    \"\"\"\n",
    "    Cria uma visualização da grelha do ambiente, mostrando:\n",
    "    - Estado inicial (verde)\n",
    "    - Estado objetivo (vermelho)\n",
    "    - Paredes (azul escuro)\n",
    "    - Estados livres (branco)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Criar matriz da grelha\n",
    "    grelha = np.ones((GRID_SIZE, GRID_SIZE, 3))  # RGB branco por defeito\n",
    "    \n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        \n",
    "        if estado in PAREDES:\n",
    "            grelha[linha, coluna] = [0.1, 0.2, 0.5]  # Azul escuro\n",
    "        elif estado == ESTADO_INICIAL:\n",
    "            grelha[linha, coluna] = [0.2, 0.8, 0.2]  # Verde\n",
    "        elif estado == ESTADO_OBJETIVO:\n",
    "            grelha[linha, coluna] = [0.9, 0.2, 0.2]  # Vermelho\n",
    "    \n",
    "    # Mostrar grelha\n",
    "    ax.imshow(grelha, interpolation='nearest')\n",
    "    \n",
    "    # Adicionar números dos estados\n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        \n",
    "        if estado in PAREDES:\n",
    "            cor_texto = 'white'\n",
    "        elif estado == ESTADO_INICIAL or estado == ESTADO_OBJETIVO:\n",
    "            cor_texto = 'white'\n",
    "        else:\n",
    "            cor_texto = 'black'\n",
    "        \n",
    "        ax.text(coluna, linha, str(estado), \n",
    "               ha='center', va='center', \n",
    "               color=cor_texto, fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # Configurar eixos\n",
    "    ax.set_xticks(range(GRID_SIZE))\n",
    "    ax.set_yticks(range(GRID_SIZE))\n",
    "    ax.set_xticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_yticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_xlabel('Coluna', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Linha', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Ambiente de Navegação do Robot\\nGrelha 10×10 com Paredes', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Adicionar grelha\n",
    "    ax.set_xticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which='minor', size=0)\n",
    "    \n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=[0.2, 0.8, 0.2], label='Estado Inicial (1)'),\n",
    "        Patch(facecolor=[0.9, 0.2, 0.2], label='Estado Objetivo (100)'),\n",
    "        Patch(facecolor=[0.1, 0.2, 0.5], label='Paredes'),\n",
    "        Patch(facecolor='white', edgecolor='black', label='Estados Livres')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', \n",
    "             bbox_to_anchor=(1.05, 1), fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('exercicio1_ambiente.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualização do ambiente salva como 'exercicio1_ambiente.png'\")\n",
    "\n",
    "visualizar_ambiente()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b802b8",
   "metadata": {},
   "source": [
    "## 5. Discussão dos Resultados\n",
    "\n",
    "### Análise do Baseline (Política Aleatória)\n",
    "\n",
    "Os resultados obtidos com a política aleatória (Random Walk) servem como **baseline** para comparação com algoritmos de aprendizagem por reforço que serão implementados nos próximos exercícios.\n",
    "\n",
    "#### Observações principais:\n",
    "\n",
    "1. **Recompensa Média por Passo:**\n",
    "   - Com ações completamente aleatórias, a recompensa média por passo é extremamente baixa\n",
    "   - Isto reflete a dificuldade de alcançar o objetivo (estado 100) através de exploração puramente aleatória\n",
    "   - O desvio padrão elevado indica grande variabilidade entre episódios\n",
    "\n",
    "2. **Número de Passos:**\n",
    "   - A maioria dos episódios provavelmente atinge o limite máximo de 1000 passos\n",
    "   - Episódios bem-sucedidos (quando existem) ocorrem por pura sorte\n",
    "   - A taxa de sucesso baixa confirma a ineficiência desta abordagem\n",
    "\n",
    "3. **Tempo de Execução:**\n",
    "   - O tempo de execução por episódio é relativamente consistente\n",
    "   - Não há overhead computacional significativo nesta fase (sem aprendizagem)\n",
    "\n",
    "#### Próximos passos:\n",
    "\n",
    "Nos exercícios seguintes, implementaremos algoritmos de Q-Learning que deverão:\n",
    "- Aumentar significativamente a recompensa média por passo\n",
    "- Reduzir o número de passos necessários para atingir o objetivo\n",
    "- Alcançar taxas de sucesso próximas de 100%\n",
    "- Aprender uma política ótima através da interação com o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907077a",
   "metadata": {},
   "source": [
    "## 6. Conclusões do Exercício 1\n",
    "\n",
    "### Objetivos Cumpridos:\n",
    "\n",
    "✓ **a)** Implementação da função de transição de estado $s' = f(s, a)$  \n",
    "✓ **b)** Implementação da função de recompensa $r(s)$  \n",
    "✓ **c)** Implementação da função de ação aleatória  \n",
    "✓ **d)** Definição das condições de término de episódio  \n",
    "✓ **e)** Simulação de 30 episódios e recolha de estatísticas  \n",
    "✓ **f)** Criação de boxplots para visualização dos resultados  \n",
    "\n",
    "### Ambiente Criado:\n",
    "\n",
    "- Grelha 10×10 (100 estados)\n",
    "- Sistema de coordenadas e navegação funcional\n",
    "- Deteção de paredes e limites\n",
    "- Sistema de recompensas\n",
    "- Simulação de episódios completa\n",
    "\n",
    "### Baselines Estabelecidos:\n",
    "\n",
    "As estatísticas obtidas com a política aleatória fornecem valores de referência fundamentais para avaliar o desempenho dos algoritmos de aprendizagem por reforço (Q-Learning) que serão implementados nos exercícios subsequentes.\n",
    "\n",
    "---\n",
    "\n",
    "**Próximo passo:** Exercício 2 - Implementação do algoritmo Q-Learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
