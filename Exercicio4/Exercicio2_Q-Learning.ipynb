{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117cbb64",
   "metadata": {},
   "source": [
    "## 1. Importação e Configuração Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "\n",
    "# Configuração para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Configuração do ambiente (do Exercício 1)\n",
    "GRID_SIZE = 10\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE\n",
    "ESTADO_INICIAL = 1\n",
    "ESTADO_OBJETIVO = 100\n",
    "\n",
    "PAREDES = {\n",
    "    23, 24, 25, 26,\n",
    "    33, 34, 35, 36,\n",
    "    54, 64, 74, 84,\n",
    "    55, 65, 75, 85,\n",
    "}\n",
    "\n",
    "ACOES = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "MAX_PASSOS_POR_EPISODIO = 1000\n",
    "\n",
    "print(\"Configuração base importada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c7347",
   "metadata": {},
   "source": [
    "## 2. Funções do Ambiente (do Exercício 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75154d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transicao_estado(estado: int, acao: str) -> int:\n",
    "    \"\"\"Calcula o próximo estado após aplicar uma ação.\"\"\"\n",
    "    linha = (estado - 1) // GRID_SIZE\n",
    "    coluna = (estado - 1) % GRID_SIZE\n",
    "    \n",
    "    nova_linha, nova_coluna = linha, coluna\n",
    "    \n",
    "    if acao == 'UP':\n",
    "        nova_linha = linha - 1\n",
    "    elif acao == 'DOWN':\n",
    "        nova_linha = linha + 1\n",
    "    elif acao == 'LEFT':\n",
    "        nova_coluna = coluna - 1\n",
    "    elif acao == 'RIGHT':\n",
    "        nova_coluna = coluna + 1\n",
    "    \n",
    "    if nova_linha < 0 or nova_linha >= GRID_SIZE or nova_coluna < 0 or nova_coluna >= GRID_SIZE:\n",
    "        return estado\n",
    "    \n",
    "    novo_estado = nova_linha * GRID_SIZE + nova_coluna + 1\n",
    "    if novo_estado in PAREDES:\n",
    "        return estado\n",
    "    \n",
    "    return novo_estado\n",
    "\n",
    "\n",
    "def recompensa(estado: int) -> float:\n",
    "    \"\"\"Retorna a recompensa de um estado.\"\"\"\n",
    "    return 100.0 if estado == ESTADO_OBJETIVO else 0.0\n",
    "\n",
    "\n",
    "def acao_aleatoria() -> str:\n",
    "    \"\"\"Escolhe uma ação aleatória.\"\"\"\n",
    "    return np.random.choice(ACOES)\n",
    "\n",
    "\n",
    "print(\"Funções do ambiente definidas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530db68",
   "metadata": {},
   "source": [
    "## 3. Configuração do Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do Q-Learning\n",
    "ALPHA = 0.7  # Taxa de aprendizagem\n",
    "GAMMA = 0.99  # Fator de desconto\n",
    "\n",
    "# Parâmetros de treino\n",
    "NUM_PASSOS_TREINO = 20000\n",
    "NUM_PASSOS_TESTE = 1000\n",
    "NUM_EXPERIENCIAS = 30\n",
    "\n",
    "# Pontos de teste durante o treino\n",
    "PONTOS_TESTE = [100, 200, 500, 600, 700, 800, 900, 1000, \n",
    "                2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n",
    "\n",
    "# Mapeamento de ações para índices\n",
    "ACAO_PARA_INDICE = {acao: i for i, acao in enumerate(ACOES)}\n",
    "INDICE_PARA_ACAO = {i: acao for i, acao in enumerate(ACOES)}\n",
    "\n",
    "print(\"Configuração do Q-Learning:\")\n",
    "print(f\"  α (alpha) = {ALPHA}\")\n",
    "print(f\"  γ (gamma) = {GAMMA}\")\n",
    "print(f\"  Número de passos de treino: {NUM_PASSOS_TREINO}\")\n",
    "print(f\"  Número de experiências: {NUM_EXPERIENCIAS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7ab36",
   "metadata": {},
   "source": [
    "## 4. Funções Auxiliares de Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_Q():\n",
    "    \"\"\"Inicializa a tabela Q com zeros.\"\"\"\n",
    "    return np.zeros((NUM_STATES + 1, len(ACOES)))\n",
    "\n",
    "\n",
    "def atualizar_Q(Q, estado, acao, proximo_estado, recompensa_recebida):\n",
    "    \"\"\"Atualiza a tabela Q usando a equação de Q-Learning.\"\"\"\n",
    "    indice_acao = ACAO_PARA_INDICE[acao]\n",
    "    q_atual = Q[estado, indice_acao]\n",
    "    max_q_proximo = np.max(Q[proximo_estado, :])\n",
    "    novo_q = (1 - ALPHA) * q_atual + ALPHA * (recompensa_recebida + GAMMA * max_q_proximo)\n",
    "    Q[estado, indice_acao] = novo_q\n",
    "\n",
    "\n",
    "def escolher_melhor_acao(Q, estado):\n",
    "    \"\"\"Escolhe a melhor ação com desempate aleatório.\"\"\"\n",
    "    valores_q = Q[estado, :]\n",
    "    max_q = np.max(valores_q)\n",
    "    acoes_maximas = [i for i, q in enumerate(valores_q) if q == max_q]\n",
    "    indice_escolhido = np.random.choice(acoes_maximas)\n",
    "    return INDICE_PARA_ACAO[indice_escolhido]\n",
    "\n",
    "\n",
    "def testar_politica_Q(Q, num_passos=NUM_PASSOS_TESTE):\n",
    "    \"\"\"Testa a política aprendida sem alterar Q.\"\"\"\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensa_total = 0.0\n",
    "    passos_executados = 0\n",
    "    \n",
    "    for _ in range(num_passos):\n",
    "        acao = escolher_melhor_acao(Q, estado_atual)\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        recompensa_total += r\n",
    "        passos_executados += 1\n",
    "        \n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "    \n",
    "    return recompensa_total / passos_executados if passos_executados > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"Funções de Q-Learning implementadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f6094",
   "metadata": {},
   "source": [
    "## 5. Exercício 2.a) Treino com Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino_random_walk(seed=None):\n",
    "    \"\"\"Executa treino com Random Walk.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    inicio = time.time()\n",
    "    Q = inicializar_Q()\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensas_teste = []\n",
    "    \n",
    "    for passo in range(1, NUM_PASSOS_TREINO + 1):\n",
    "        acao = acao_aleatoria()\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        atualizar_Q(Q, estado_atual, acao, proximo_estado, r)\n",
    "        \n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "        \n",
    "        if passo in PONTOS_TESTE:\n",
    "            recomp_teste = testar_politica_Q(Q)\n",
    "            recompensas_teste.append(recomp_teste)\n",
    "    \n",
    "    tempo_total = time.time() - inicio\n",
    "    return Q, recompensas_teste, tempo_total\n",
    "\n",
    "\n",
    "print(\"Função de treino Random Walk definida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TREINO COM RANDOM WALK - 30 EXPERIÊNCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "todas_recompensas_random = []\n",
    "todos_tempos_random = []\n",
    "todas_Q_random = []\n",
    "\n",
    "print(f\"\\nExecutando {NUM_EXPERIENCIAS} experiências...\\n\")\n",
    "\n",
    "for exp in range(1, NUM_EXPERIENCIAS + 1):\n",
    "    Q, recompensas, tempo = treino_random_walk(seed=42 + exp)\n",
    "    todas_recompensas_random.append(recompensas)\n",
    "    todos_tempos_random.append(tempo)\n",
    "    todas_Q_random.append(Q.copy())\n",
    "    \n",
    "    if exp % 5 == 0:\n",
    "        print(f\"Experiência {exp:2d}: tempo = {tempo:.2f}s, recompensa final = {recompensas[-1]:.6f}\")\n",
    "\n",
    "todas_recompensas_random = np.array(todas_recompensas_random)\n",
    "todos_tempos_random = np.array(todos_tempos_random)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTADOS - RANDOM WALK\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTempo médio: {np.mean(todos_tempos_random):.2f}s ± {np.std(todos_tempos_random):.2f}s\")\n",
    "print(f\"Recompensa final média: {np.mean(todas_recompensas_random[:, -1]):.6f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37ee7e",
   "metadata": {},
   "source": [
    "## 6. Exercício 2.b) Treino Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino_greedy(seed=None):\n",
    "    \"\"\"Executa treino Greedy (sempre melhor ação).\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    inicio = time.time()\n",
    "    Q = inicializar_Q()\n",
    "    estado_atual = ESTADO_INICIAL\n",
    "    recompensas_teste = []\n",
    "    \n",
    "    for passo in range(1, NUM_PASSOS_TREINO + 1):\n",
    "        acao = escolher_melhor_acao(Q, estado_atual)\n",
    "        proximo_estado = transicao_estado(estado_atual, acao)\n",
    "        r = recompensa(proximo_estado)\n",
    "        atualizar_Q(Q, estado_atual, acao, proximo_estado, r)\n",
    "        \n",
    "        if proximo_estado == ESTADO_OBJETIVO:\n",
    "            proximo_estado = ESTADO_INICIAL\n",
    "        \n",
    "        estado_atual = proximo_estado\n",
    "        \n",
    "        if passo in PONTOS_TESTE:\n",
    "            recomp_teste = testar_politica_Q(Q)\n",
    "            recompensas_teste.append(recomp_teste)\n",
    "    \n",
    "    tempo_total = time.time() - inicio\n",
    "    return Q, recompensas_teste, tempo_total\n",
    "\n",
    "\n",
    "print(\"Função de treino Greedy definida!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TREINO GREEDY - 30 EXPERIÊNCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "todas_recompensas_greedy = []\n",
    "todos_tempos_greedy = []\n",
    "todas_Q_greedy = []\n",
    "\n",
    "print(f\"\\nExecutando {NUM_EXPERIENCIAS} experiências...\\n\")\n",
    "\n",
    "for exp in range(1, NUM_EXPERIENCIAS + 1):\n",
    "    Q, recompensas, tempo = treino_greedy(seed=42 + exp)\n",
    "    todas_recompensas_greedy.append(recompensas)\n",
    "    todos_tempos_greedy.append(tempo)\n",
    "    todas_Q_greedy.append(Q.copy())\n",
    "    \n",
    "    if exp % 5 == 0:\n",
    "        print(f\"Experiência {exp:2d}: tempo = {tempo:.2f}s, recompensa final = {recompensas[-1]:.6f}\")\n",
    "\n",
    "todas_recompensas_greedy = np.array(todas_recompensas_greedy)\n",
    "todos_tempos_greedy = np.array(todos_tempos_greedy)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTADOS - GREEDY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTempo médio: {np.mean(todos_tempos_greedy):.2f}s ± {np.std(todos_tempos_greedy):.2f}s\")\n",
    "print(f\"Recompensa final média: {np.mean(todas_recompensas_greedy[:, -1]):.6f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0193de",
   "metadata": {},
   "source": [
    "## 7. Visualização: Comparação Random Walk vs Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f74f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "medias_random = np.mean(todas_recompensas_random, axis=0)\n",
    "desvios_random = np.std(todas_recompensas_random, axis=0)\n",
    "medias_greedy = np.mean(todas_recompensas_greedy, axis=0)\n",
    "desvios_greedy = np.std(todas_recompensas_greedy, axis=0)\n",
    "\n",
    "ax.plot(PONTOS_TESTE, medias_random, 'b-', linewidth=2.5, marker='o', \n",
    "        markersize=7, label='Random Walk', zorder=3)\n",
    "ax.fill_between(PONTOS_TESTE, medias_random - desvios_random, \n",
    "                medias_random + desvios_random, alpha=0.2, color='blue')\n",
    "\n",
    "ax.plot(PONTOS_TESTE, medias_greedy, 'r-', linewidth=2.5, marker='s', \n",
    "        markersize=7, label='Greedy', zorder=3)\n",
    "ax.fill_between(PONTOS_TESTE, medias_greedy - desvios_greedy, \n",
    "                medias_greedy + desvios_greedy, alpha=0.2, color='red')\n",
    "\n",
    "ax.set_xlabel('Número de Passos de Treino', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Recompensa Média por Passo (Teste)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Comparação: Random Walk vs Greedy - Q-Learning\\n'\n",
    "             f'(α={ALPHA}, γ={GAMMA}, {NUM_EXPERIENCIAS} experiências)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=12, loc='lower right', framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('exercicio2_comparacao_random_greedy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráfico salvo como 'exercicio2_comparacao_random_greedy.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ba2c6",
   "metadata": {},
   "source": [
    "## 8. Mapa de Calor da Utilidade Aprendida (Figura 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_mapa_calor_utilidade(Q, titulo=\"Mapa de Calor da Utilidade Aprendida\", \n",
    "                               nome_ficheiro=\"mapa_calor.png\"):\n",
    "    \"\"\"Cria mapa de calor mostrando U(s) = max_a Q[s,a].\"\"\"\n",
    "    utilidades = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    \n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        utilidades[linha, coluna] = np.max(Q[estado, :])\n",
    "    \n",
    "    for estado in PAREDES:\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        utilidades[linha, coluna] = np.nan\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 11))\n",
    "    im = ax.imshow(utilidades, cmap='YlOrRd', interpolation='nearest')\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Utilidade Máxima U(s) = max Q[s,a]', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    for estado in range(1, NUM_STATES + 1):\n",
    "        linha = (estado - 1) // GRID_SIZE\n",
    "        coluna = (estado - 1) % GRID_SIZE\n",
    "        \n",
    "        if estado in PAREDES:\n",
    "            ax.text(coluna, linha, 'X', ha='center', va='center',\n",
    "                   color='black', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            utilidade = utilidades[linha, coluna]\n",
    "            cor_texto = 'white' if utilidade > np.nanmax(utilidades) * 0.5 else 'black'\n",
    "            ax.text(coluna, linha, f'{estado}\\n{utilidade:.1f}', \n",
    "                   ha='center', va='center', color=cor_texto, \n",
    "                   fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(GRID_SIZE))\n",
    "    ax.set_yticks(range(GRID_SIZE))\n",
    "    ax.set_xticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_yticklabels(range(1, GRID_SIZE + 1))\n",
    "    ax.set_xlabel('Coluna', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Linha', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.set_xticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, GRID_SIZE, 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which='minor', size=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(nome_ficheiro, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Mapa de calor salvo como '{nome_ficheiro}'\")\n",
    "\n",
    "\n",
    "# Mapas de calor para ambas as estratégias\n",
    "print(\"Criando mapas de calor...\\n\")\n",
    "\n",
    "criar_mapa_calor_utilidade(\n",
    "    todas_Q_random[-1], \n",
    "    titulo=\"Mapa de Calor - Random Walk com Q-Learning\",\n",
    "    nome_ficheiro=\"exercicio2a_mapa_calor_random_walk.png\"\n",
    ")\n",
    "\n",
    "criar_mapa_calor_utilidade(\n",
    "    todas_Q_greedy[-1], \n",
    "    titulo=\"Mapa de Calor - Greedy com Q-Learning\",\n",
    "    nome_ficheiro=\"exercicio2b_mapa_calor_greedy.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352e4b3",
   "metadata": {},
   "source": [
    "## 9. Análise Comparativa Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ANÁLISE COMPARATIVA: RANDOM WALK vs GREEDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TEMPO DE EXECUÇÃO:\")\n",
    "print(f\"   Random Walk: {np.mean(todos_tempos_random):.2f}s ± {np.std(todos_tempos_random):.2f}s\")\n",
    "print(f\"   Greedy:      {np.mean(todos_tempos_greedy):.2f}s ± {np.std(todos_tempos_greedy):.2f}s\")\n",
    "\n",
    "recomp_final_random = todas_recompensas_random[:, -1]\n",
    "recomp_final_greedy = todas_recompensas_greedy[:, -1]\n",
    "\n",
    "print(\"\\n2. RECOMPENSA FINAL (após 20000 passos):\")\n",
    "print(f\"   Random Walk: {np.mean(recomp_final_random):.6f} ± {np.std(recomp_final_random):.6f}\")\n",
    "print(f\"   Greedy:      {np.mean(recomp_final_greedy):.6f} ± {np.std(recomp_final_greedy):.6f}\")\n",
    "\n",
    "melhoria = ((np.mean(recomp_final_greedy) - np.mean(recomp_final_random)) / \n",
    "            np.mean(recomp_final_random) * 100)\n",
    "print(f\"\\n   Melhoria do Greedy: {melhoria:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184072e",
   "metadata": {},
   "source": [
    "## 10. Conclusões do Exercício 2\n",
    "\n",
    "### Random Walk com Q-Learning\n",
    "\n",
    "**Vantagens:**\n",
    "- Exploração ampla do espaço de estados\n",
    "- Descoberta de múltiplos caminhos\n",
    "- Menos suscetível a mínimos locais\n",
    "\n",
    "**Desvantagens:**\n",
    "- Convergência lenta\n",
    "- Muitas ações desperdiçadas\n",
    "- Variabilidade alta\n",
    "\n",
    "### Greedy com Q-Learning\n",
    "\n",
    "**Vantagens:**\n",
    "- Convergência rápida\n",
    "- Exploração focada\n",
    "- Resultados consistentes\n",
    "\n",
    "**Desvantagens:**\n",
    "- Risco de mínimos locais\n",
    "- Exploração limitada\n",
    "- Dependência da inicialização\n",
    "\n",
    "### Melhor Ação por Estado\n",
    "\n",
    "A tabela Q final permite identificar a **melhor ação para qualquer estado**:\n",
    "\n",
    "```python\n",
    "melhor_acao = argmax_a Q[s, a]\n",
    "```\n",
    "\n",
    "### Próximos Passos\n",
    "\n",
    "O **Exercício 3** implementará a estratégia **ε-greedy**, combinando:\n",
    "- Exploração (ações aleatórias com probabilidade ε)\n",
    "- Exploração (melhor ação com probabilidade 1-ε)\n",
    "\n",
    "Esta abordagem híbrida tende a superar ambas as estratégias puras."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
